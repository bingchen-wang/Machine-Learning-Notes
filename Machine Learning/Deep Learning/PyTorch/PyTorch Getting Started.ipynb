{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0daef5d6-6f82-44e4-9977-7803caa9cd0d",
   "metadata": {},
   "source": [
    "# PyTorch - Getting Started\n",
    "\n",
    "Author: Bingchen Wang\n",
    "\n",
    "Last Updated: 21 Nov, 2022\n",
    "\n",
    "This lab is adapted from <a href = \"https://pytorch.org/tutorials/beginner/basics/intro.html\">PyTorch Tutorials</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce16fc-4dce-4f90-a0dd-770c5d5cd066",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Tensors](#Tensors)\n",
    "- [Datasets & DataLoaders](#D&D)\n",
    "- [Transforms](#Transforms)\n",
    "- [Build Model](#BM)\n",
    "- [Autograd](#Autograd)\n",
    "- Optimization\n",
    "- Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c76e668-dbc6-4f15-83db-6952c5fc89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Grayscale\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e270b9c6-4e57-4ec7-a612-feac8ef782f6",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "In PyTorch, the main objects used to encode the inputs and outputs of a model as well as the parameters are called **tensors**, which are similar to NumPy's ndarrays. An important *distinction* is that tensors can run on GPUs or other hardware accelerators.\n",
    "### Initializing a Tensor\n",
    "From other objects:\n",
    "- `torch.tensor(list/array)`\n",
    "- `torch.from_numpy(array)`\n",
    "- `torch.ones_like(Tensor)` The new tensor retains the properties (shape, datatype) as the argument tensor, unless explicitly overrriden.\n",
    "- `torch.zeros_like(Tensor)` The new tensor retains the properties (shape, datatype) as the argument tensor, unless explicitly overrriden.\n",
    "- `torch.rand_like(Tensor)` The new tensor retains the properties (shape, datatype) as the argument tensor, unless explicitly overrriden.\n",
    "\n",
    "By specifying a shape:\n",
    "- `torch.ones(shape)`\n",
    "- `torch.zeros(shape)`\n",
    "- `torch.rand(shape)`\n",
    "- `torch.randn(shape)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ea9b17-94ee-44e1-a2f9-1df295a7bbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# 1. Directly from data (list)\n",
    "data = [[1, 2],[3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eefe70a-c847-434b-9372-23cbac542a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# 2. From a NumPy array\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np2 = torch.tensor(np_array)\n",
    "print(x_np)\n",
    "print(x_np2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521d28da-2e90-4afb-901a-11891f10d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "tensor([[0.8127, 0.0742],\n",
      "        [0.4468, 0.9417]])\n"
     ]
    }
   ],
   "source": [
    "# 3. From anothr tensor\n",
    "x_ones = torch.ones_like(x_data)\n",
    "print(x_ones)\n",
    "x_rand = torch.rand_like(x_data, dtype = torch.float)\n",
    "print(x_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cb737db-0e4b-47a5-b0f3-7067100cf66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_tensor: \n",
      " tensor([[0.9750, 0.8552],\n",
      "        [0.8981, 0.7508],\n",
      "        [0.5534, 0.4455]])\n",
      "ones_tensor: \n",
      " tensor([1., 1., 1., 1., 1.])\n",
      "zeros_tensor: \n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "randn_tensor: \n",
      " tensor([[ 1.1711, -0.1999, -0.0799],\n",
      "        [ 0.8810,  0.2021,  0.2873],\n",
      "        [-1.2668, -2.1923, -0.2464]])\n"
     ]
    }
   ],
   "source": [
    "# 4. By specifying a shape\n",
    "shape = (3,2)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(5)\n",
    "zeros_tensor = torch.zeros(2,5)\n",
    "randn_tensor = torch.randn((3,3))\n",
    "print(f'rand_tensor: \\n {rand_tensor}')\n",
    "print(f'ones_tensor: \\n {ones_tensor}')\n",
    "print(f'zeros_tensor: \\n {zeros_tensor}')\n",
    "print(f'randn_tensor: \\n {randn_tensor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8d7f6-f8e8-40fa-8075-7e18838ebbe9",
   "metadata": {},
   "source": [
    "### Attributes of a Tensor\n",
    "- `.shape`\n",
    "- `.dtype`\n",
    "- `.device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02105d3-375a-4c36-a789-33be893e9a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: \n",
      " tensor([[ 1.1711, -0.1999, -0.0799],\n",
      "        [ 0.8810,  0.2021,  0.2873],\n",
      "        [-1.2668, -2.1923, -0.2464]]), \n",
      " shape: torch.Size([3, 3]) | dtype: torch.float32 | device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f'Tensor: \\n {randn_tensor}, \\n shape: {randn_tensor.shape} | dtype: {randn_tensor.dtype} | device: {randn_tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a2fe5-42e6-45ae-b890-9bcc0225e6f8",
   "metadata": {},
   "source": [
    "### Operations on Tensor\n",
    "- Moving a tensor to the GPU if available\n",
    "- Standard numpy-like indexing and slicing\n",
    "- Joining two tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3050d0-884e-4bbb-a478-3f4c79c2b043",
   "metadata": {},
   "source": [
    "#### Moving to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ed0a34-1c73-45a7-80d9-2af64440f65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving to the GPU, which does not work with MacBook Pro\n",
    "if torch.cuda.is_available():\n",
    "    ones_tensor = ones_tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad0594-9bb0-431d-94d5-de21622246ef",
   "metadata": {},
   "source": [
    "#### Numpy-like indexing, slicing and assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334686b5-4956-44f5-b006-0f9015983a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randn_tensor[0]: tensor([ 1.1711, -0.1999, -0.0799])\n",
      "randn_tensor[:,0]: tensor([ 1.1711,  0.8810, -1.2668])\n",
      "randn_tensor[...,-1] (last column): tensor([-0.0799,  0.2873, -0.2464])\n",
      "tensor([[ 1.1711, -0.1999, -0.0799],\n",
      "        [ 0.8810,  0.2021,  0.2873],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Numpy-like indexing and slicing\n",
    "print(f'randn_tensor[0]: {randn_tensor[0]}')\n",
    "print(f'randn_tensor[:,0]: {randn_tensor[:,0]}')\n",
    "print(f'randn_tensor[...,-1] (last column): {randn_tensor[...,-1]}')\n",
    "\n",
    "# Numpy-like assignment\n",
    "randn_tensor[2] = 0\n",
    "print(randn_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184cc044-728a-4223-8a3b-ad8e7fa03ff8",
   "metadata": {},
   "source": [
    "#### Joining multiple tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef825e4f-ea1d-4cab-aa8c-e9ccb3ee5989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1:\n",
      "tensor([[ 1.0000,  1.0000,  1.0000, -0.0685, -1.1642,  1.3889, -1.4192],\n",
      "        [ 1.0000,  1.0000,  1.0000, -1.9115, -0.6084,  1.2976, -1.6579],\n",
      "        [ 1.0000,  1.0000,  1.0000, -2.2209,  0.1816, -0.6686,  0.3302]])\n",
      "t2:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Join horizontally\n",
    "tensor1 = torch.ones(3,3)\n",
    "tensor2 = torch.randn(3,4)\n",
    "tensor3 = torch.zeros(2,3)\n",
    "t1 = torch.cat([tensor1, tensor2], dim = 1)\n",
    "\n",
    "# Join vertically\n",
    "t2 = torch.cat([tensor1, tensor3], dim = 0)\n",
    "print(f't1:\\n{t1}\\nt2:\\n{t2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801db128-4252-48cc-8117-a0ced23a4fa6",
   "metadata": {},
   "source": [
    "#### Arithmetic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fcd7165-56c7-4882-8577-0878dcdfc88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [4, 5]])\n",
      "tensor([2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [0, 5, 6],\n",
      "        [0, 8, 9]])\n",
      "tensor([[ 1,  6,  6],\n",
      "        [ 4, 15, 15],\n",
      "        [ 7, 24, 24]])\n",
      "tensor([[ 1,  6,  6],\n",
      "        [ 4, 15, 15],\n",
      "        [ 7, 24, 24]])\n",
      "tensor([[1, 4, 7],\n",
      "        [2, 5, 8],\n",
      "        [3, 6, 9]])\n",
      "tensor([12, 15, 18])\n",
      "tensor(45)\n",
      "Convert to a Python numerical value: 45\n",
      "Operation that is not in-place: t1.add(1) \n",
      "tensor([[ 2,  3,  4],\n",
      "        [ 5,  6,  7],\n",
      "        [ 8,  9, 10]])\n",
      "t1: tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Operation that is in-place: t1.add_(1) \n",
      "tensor([[ 2,  3,  4],\n",
      "        [ 5,  6,  7],\n",
      "        [ 8,  9, 10]])\n",
      "t1: tensor([[ 2,  3,  4],\n",
      "        [ 5,  6,  7],\n",
      "        [ 8,  9, 10]])\n",
      "Operation that is in-place: t1.subtract_(1) \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "t1: tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# Addition with broadcasting\n",
    "print(torch.tensor([[1,2]]) + torch.tensor([[0],[3]]))\n",
    "\n",
    "# Subtraction\n",
    "print(torch.tensor([3,4]) - 1)\n",
    "\n",
    "# Element-wise product\n",
    "t1 = torch.arange(1,10).reshape(3,3)\n",
    "t2 = (torch.randn(3,3) >= 0).long() #convert dtype to tensor.long\n",
    "print(t1 * t2)\n",
    "\n",
    "# Matrix multiplication\n",
    "print(t1 @ t2)\n",
    "print(torch.matmul(t1, t2))\n",
    "\n",
    "# Matrix transpose\n",
    "print(t1.T)\n",
    "\n",
    "# Sum\n",
    "print(torch.sum(t1, axis = 0))\n",
    "print(torch.sum(t1))\n",
    "print(f'Convert to a Python numerical value: {torch.sum(t1).item()}')\n",
    "\n",
    "# In-place operations\n",
    "print(f'Operation that is not in-place: t1.add(1) \\n{t1.add(1)}')\n",
    "print(f't1: {t1}')\n",
    "print(f'Operation that is in-place: t1.add_(1) \\n{t1.add_(1)}')\n",
    "print(f't1: {t1}')\n",
    "print(f'Operation that is in-place: t1.subtract_(1) \\n{t1.subtract_(1)}')\n",
    "print(f't1: {t1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad88cb-c4b2-4018-a87f-37a5440392da",
   "metadata": {},
   "source": [
    "### Bridge with Numpy\n",
    "<div class = 'alert alert-block alert-danger'> <b>Note:</b> Tensors on the <b>CPU</b> and NumPy arrays can share their underlying memory locations, and <b>changing one will change the other</b>. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93997568-88e9-4310-bd91-160b99438500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 0., 1., 1.]) [1. 1. 0. 1. 1.]\n",
      "[5. 5. 5.] tensor([5., 5., 5.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_tensor = torch.ones(5)\n",
    "x_np = x_tensor.numpy()\n",
    "print(x_tensor, x_np)\n",
    "x_tensor[2] = 0\n",
    "print(x_tensor, x_np)\n",
    "\n",
    "x_numpy = np.ones(3)\n",
    "x_t = torch.from_numpy(x_numpy)\n",
    "\n",
    "x_numpy *= 5\n",
    "print(x_numpy, x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c3f93f-d97b-417b-9355-996896d1c917",
   "metadata": {},
   "source": [
    "<a name = 'D&D'></a>\n",
    "## Datasets & DataLoaders\n",
    "\n",
    "PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the Dataset to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2907b7-005b-4dac-b862-64722115a842",
   "metadata": {},
   "source": [
    "### Load built-in datasets from TorchVision\n",
    "Find a comprehensive list of image classification datasets <a href = \"https://pytorch.org/vision/stable/datasets.html\">here</a>. Here we use the <a href = 'https://www.cs.toronto.edu/~kriz/cifar.html'>CIFAR-10 dataset</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "283dfed0-8c9d-453b-a928-2b519cc91645",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.CIFAR10(\n",
    "    root=\"CIFAR-10 dataset\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"CIFAR-10 dataset\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cacfa0-c84e-4e80-a21c-4f79de57e71e",
   "metadata": {},
   "source": [
    "### Preparing the data for training with DataLoader\n",
    "\n",
    "`Dataset` retrieves data one example at a time. To pass examples in mini-batches, reshuffle the data at each epoch (to avoid model overfitting) and use vectorization, we use `DataLoader`, an iterable that is wrapped around `Dataset`.\n",
    "\n",
    "Note:\n",
    "- By setting `shuffle = True`, the data is reshuffled after we iterate over *all* mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8af8be8-b6bd-4c9e-959e-6d82093cb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size = 64, shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cc7bcd-6c4f-4277-b28f-1b1a32d2d27a",
   "metadata": {},
   "source": [
    "### Iterating through a DataLoader\n",
    "- `iter()` is used to get an iterator from the iterable `DataLoader`\n",
    "- `next()` consumes the next item in the iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9108dfa3-eb74-401e-a285-0acd2bdc308e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature mini-batch shape: torch.Size([64, 3, 32, 32])\n",
      "Label mini-batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_dict = {0: 'airplane', 1: 'automobile', 2: 'bird', 3:'cat', 4:'deer',\n",
    "              5: 'dog', 6:'frog', 7:'horse', 8: 'ship', 9:'truck'}\n",
    "train_it = iter(train_dataloader)\n",
    "train_features, train_labels = next(train_it)\n",
    "print(f\"Feature mini-batch shape: {train_features.shape}\")\n",
    "print(f\"Label mini-batch shape: {train_labels.shape}\")\n",
    "plt.figure(figsize = (3,3))\n",
    "plt.imshow(torch.movedim(Grayscale()(train_features[0]),0,-1), cmap = \"gray\")\n",
    "plt.title(labels_dict[train_labels[0].item()])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b535eaf-bef5-4ddc-acf2-dffb9c9db7d8",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "`torchvision.transforms.Lambda` applies any user-defined lambda function to the data.\n",
    "\n",
    "### One-hot encoding of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55981ba5-0153-4d96-b731-95610e582c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(10, dtype = torch.float).scatter_(dim = 0, index = torch.tensor(y), value = 1))\n",
    "print(target_transform([1]))\n",
    "\n",
    "# training_data = datasets.CIFAR10(\n",
    "#     root=\"CIFAR-10 dataset\",\n",
    "#     train=True,\n",
    "#     download=False,\n",
    "#     transform=ToTensor(),\n",
    "#     target_transform = Lambda(lambda y: torch.zeros(10, dtype = torch.float).scatter_(dim = 0, index = torch.tensor(y), value = 1))\n",
    "# target_transform([1])\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f1ec8-5663-4cf3-a8e2-afede3456eca",
   "metadata": {},
   "source": [
    "<a name = \"BM\"></a>\n",
    "## Build Model\n",
    "\n",
    "We can build a neural network by subclassing `nn.Module`. Layers of a neural network are also subclasses of `nn.Module`. We can specify the neural network structure and initialize the layers in `__init__`. Every `nn.Module` implements operations on the input data through the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f93c78c-d970-44fb-97c1-fd1d109b3036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.grayscale = Grayscale()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.simple_network = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(self.grayscale(x))\n",
    "        logits = self.simple_network(x)\n",
    "        return logits\n",
    "    def predict_prob(self, x):\n",
    "        logits = self.forward(x)\n",
    "        softmax_prob = nn.Softmax(dim = 1)(logits)\n",
    "        return softmax_prob\n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        y_pred = logits.argmax(1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23cf8ea4-4933-4503-ac10-8da1ea90475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (grayscale): Grayscale(num_output_channels=1)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (simple_network): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797ebe1-8fb0-4f84-8d3d-4e81d49e5f65",
   "metadata": {},
   "source": [
    "### Model Parameters\n",
    "\n",
    "We can access the parameters of a model using `.parameters()` and `.named_parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b0b4a9e-395b-44ae-85cb-707bbc216734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: simple_network.0.weight, size: torch.Size([512, 1024]) \n",
      "First two rows of values: \n",
      "tensor([[-0.0149,  0.0002, -0.0225,  ...,  0.0069, -0.0056,  0.0110],\n",
      "        [ 0.0050, -0.0179, -0.0249,  ...,  0.0240,  0.0059, -0.0310]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer: simple_network.0.bias, size: torch.Size([512]) \n",
      "First two rows of values: \n",
      "tensor([-0.0008, -0.0101], grad_fn=<SliceBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer: simple_network.2.weight, size: torch.Size([512, 512]) \n",
      "First two rows of values: \n",
      "tensor([[ 0.0222, -0.0401,  0.0226,  ...,  0.0357, -0.0230,  0.0202],\n",
      "        [-0.0309, -0.0075, -0.0246,  ...,  0.0194, -0.0060,  0.0031]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer: simple_network.2.bias, size: torch.Size([512]) \n",
      "First two rows of values: \n",
      "tensor([-0.0290, -0.0202], grad_fn=<SliceBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer: simple_network.4.weight, size: torch.Size([512, 512]) \n",
      "First two rows of values: \n",
      "tensor([[ 0.0181, -0.0412, -0.0393,  ..., -0.0387,  0.0127,  0.0442],\n",
      "        [-0.0388, -0.0284, -0.0060,  ...,  0.0395, -0.0288,  0.0002]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer: simple_network.4.bias, size: torch.Size([512]) \n",
      "First two rows of values: \n",
      "tensor([0.0215, 0.0392], grad_fn=<SliceBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer: simple_network.6.weight, size: torch.Size([10, 512]) \n",
      "First two rows of values: \n",
      "tensor([[-0.0347,  0.0196, -0.0319,  ...,  0.0345, -0.0115, -0.0325],\n",
      "        [ 0.0106,  0.0003, -0.0085,  ...,  0.0149,  0.0404, -0.0049]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Layer: simple_network.6.bias, size: torch.Size([10]) \n",
      "First two rows of values: \n",
      "tensor([ 0.0106, -0.0313], grad_fn=<SliceBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, size: {param.shape} \\nFirst two rows of values: \\n{param[:2]}\\n\" + '-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d9ee5-6433-4314-8038-65cc0f24bc65",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "PyTorch has a built-in differentiation engine called `torch.autograd`, which supports automatic computation of gradient for any computational graph. A reference to the backpropagation function is stored in the `grad_fn` of a tensor. \n",
    "\n",
    "In order to run backprop on some parameter tensors, we need to set the `requires_grad` property of the tensor to be `True`. This can be done upon creating or later using the `theta.requires_grad_(True)`. Note that **by default, all tensors with `requires_grad = True` are tracking their computational history and support gradient computation.** To turn it off and stop tracking, we can surround the code with a `torch.no_grad()` block. Another way is to use `theta_det = theta.detach()`.\n",
    "\n",
    "To compute the derivatives of the loss with respect to some parameters $\\theta$, first call `loss.backward()` and then call `theta.grad`.\n",
    "\n",
    "### PyTorch accumulates the gradients\n",
    "When doing backpropgation, PyTorch accumulates the gradients, every time `.backward()` is called, the computed gradients are added to the gradient property of all leaf nodes of computational graph. To compute proper gradients, use `theta.grad.zero_()` ahead of the call or use an optimizer.\n",
    "- `.backward()` computes `torch.tensor(1.0) * J`\n",
    "- `.backward(v)` computes `torch.tensor(v.t()) * J`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3f0b31b-7b37-4ed5-9feb-bfa29701d4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]], requires_grad=True)\n",
      "tensor([[4., 1., 1., 1.],\n",
      "        [1., 4., 1., 1.],\n",
      "        [1., 1., 4., 1.],\n",
      "        [1., 1., 1., 4.],\n",
      "        [1., 1., 1., 1.]], grad_fn=<TBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(4,5, requires_grad = True)\n",
    "print(inp)\n",
    "out = (inp+1).pow(2).t()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe90c169-6237-4dcd-8dfe-280c0e351696",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward(torch.ones_like(out), retain_graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27dac461-f77e-41b3-835c-fca80e32ac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "New call\n",
      "tensor([[4., 0., 0., 0., 0.],\n",
      "        [0., 4., 0., 0., 0.],\n",
      "        [0., 0., 4., 0., 0.],\n",
      "        [0., 0., 0., 4., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"First call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.eye(*out.shape), retain_graph = True)\n",
    "print(f\"New call\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133fe3c-a41d-44da-a0ed-b50ad5c6891b",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7986c9c1-2ff2-4999-b7e0-d5821fef7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32bff83-7d53-46ca-9ca5-f61ceb72428c",
   "metadata": {},
   "source": [
    "### Typical loss functions\n",
    "- `nn.MSELoss()`\n",
    "- `nn.CrossEntropyLoss()`\n",
    "- `nn.L1Loss()`\n",
    "- `nn.NLLLoss()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e54d54c-5294-4452-8948-bf5f2967a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bffc75-2823-4abe-bfcc-ac4d850552e3",
   "metadata": {},
   "source": [
    "### Typical optimization algrithms\n",
    "- `torch.optim.Adam()`\n",
    "- `torch.optim.SGD()`\n",
    "- `torch.optim.RMSprop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4feed7da-96c6-4b86-99bd-cb852e089be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58c702a-06ee-4a94-90e8-996c2c02f6fc",
   "metadata": {},
   "source": [
    "### Optimization in the training loop\n",
    "- Step 1: reset the gradients of model parameters: `optimizer.zero_grad()`\n",
    "- Step 2: backprop the prediction loss (and the gradients are deposited by PyTorch): `loss.backward()`\n",
    "- Step 3: update the parameters using the stored gradients: `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b7cfdff-4e28-4dcd-83e4-9137102bbe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss:{loss:>7f} [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "850e0824-8a3e-494c-b691-9977b9abd4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss:2.301201 [    0/50000]\n",
      "loss:2.174696 [ 6400/50000]\n",
      "loss:2.068370 [12800/50000]\n",
      "loss:2.081838 [19200/50000]\n",
      "loss:2.041301 [25600/50000]\n",
      "loss:1.812771 [32000/50000]\n",
      "loss:1.843223 [38400/50000]\n",
      "loss:2.221197 [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 31.4%, Avg loss: 1.919091 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss:1.859148 [    0/50000]\n",
      "loss:1.930200 [ 6400/50000]\n",
      "loss:2.104660 [12800/50000]\n",
      "loss:1.889502 [19200/50000]\n",
      "loss:2.003397 [25600/50000]\n",
      "loss:1.970841 [32000/50000]\n",
      "loss:1.766366 [38400/50000]\n",
      "loss:1.952714 [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 1.868127 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss:1.784491 [    0/50000]\n",
      "loss:1.994488 [ 6400/50000]\n",
      "loss:1.750439 [12800/50000]\n",
      "loss:1.707920 [19200/50000]\n",
      "loss:1.759058 [25600/50000]\n",
      "loss:1.922291 [32000/50000]\n",
      "loss:1.813684 [38400/50000]\n",
      "loss:1.810835 [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 1.806244 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss:1.655811 [    0/50000]\n",
      "loss:1.738841 [ 6400/50000]\n",
      "loss:1.738340 [12800/50000]\n",
      "loss:1.788832 [19200/50000]\n",
      "loss:1.740561 [25600/50000]\n",
      "loss:1.686907 [32000/50000]\n",
      "loss:1.520932 [38400/50000]\n",
      "loss:1.860639 [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 36.7%, Avg loss: 1.762067 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss:1.727619 [    0/50000]\n",
      "loss:1.744125 [ 6400/50000]\n",
      "loss:1.615586 [12800/50000]\n",
      "loss:1.643948 [19200/50000]\n",
      "loss:1.544994 [25600/50000]\n",
      "loss:1.802172 [32000/50000]\n",
      "loss:1.820413 [38400/50000]\n",
      "loss:1.801411 [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 36.0%, Avg loss: 1.781310 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss:1.845591 [    0/50000]\n",
      "loss:1.619142 [ 6400/50000]\n",
      "loss:1.652564 [12800/50000]\n",
      "loss:1.601765 [19200/50000]\n",
      "loss:1.770922 [25600/50000]\n",
      "loss:1.641437 [32000/50000]\n",
      "loss:1.725989 [38400/50000]\n",
      "loss:1.914155 [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 1.714463 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss:1.613191 [    0/50000]\n",
      "loss:1.816157 [ 6400/50000]\n",
      "loss:1.712593 [12800/50000]\n",
      "loss:1.701889 [19200/50000]\n",
      "loss:1.756816 [25600/50000]\n",
      "loss:1.456781 [32000/50000]\n",
      "loss:1.538818 [38400/50000]\n",
      "loss:1.690465 [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.688893 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss:1.647784 [    0/50000]\n",
      "loss:1.527516 [ 6400/50000]\n",
      "loss:1.798254 [12800/50000]\n",
      "loss:1.620335 [19200/50000]\n",
      "loss:1.868740 [25600/50000]\n",
      "loss:1.648652 [32000/50000]\n",
      "loss:1.684743 [38400/50000]\n",
      "loss:1.670508 [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 39.9%, Avg loss: 1.670487 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss:1.687645 [    0/50000]\n",
      "loss:1.651435 [ 6400/50000]\n",
      "loss:1.784178 [12800/50000]\n",
      "loss:1.468071 [19200/50000]\n",
      "loss:1.571632 [25600/50000]\n",
      "loss:1.742608 [32000/50000]\n",
      "loss:1.483831 [38400/50000]\n",
      "loss:1.606868 [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 40.3%, Avg loss: 1.670653 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss:1.388459 [    0/50000]\n",
      "loss:1.539066 [ 6400/50000]\n",
      "loss:1.640181 [12800/50000]\n",
      "loss:1.602123 [19200/50000]\n",
      "loss:1.627127 [25600/50000]\n",
      "loss:1.603124 [32000/50000]\n",
      "loss:1.516087 [38400/50000]\n",
      "loss:1.810488 [44800/50000]\n",
      "Test Error: \n",
      " Accuracy: 41.5%, Avg loss: 1.653361 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7673cf0-4a85-4cb1-af43-d3da62d2075e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.6039, 0.4941, 0.4118,  ..., 0.3569, 0.3412, 0.3098],\n",
       "          [0.5490, 0.5686, 0.4902,  ..., 0.3765, 0.3020, 0.2784],\n",
       "          [0.5490, 0.5451, 0.4510,  ..., 0.3098, 0.2667, 0.2627],\n",
       "          ...,\n",
       "          [0.6863, 0.6118, 0.6039,  ..., 0.1647, 0.2392, 0.3647],\n",
       "          [0.6471, 0.6118, 0.6235,  ..., 0.4039, 0.4824, 0.5137],\n",
       "          [0.6392, 0.6196, 0.6392,  ..., 0.5608, 0.5608, 0.5608]],\n",
       " \n",
       "         [[0.6941, 0.5373, 0.4078,  ..., 0.3725, 0.3529, 0.3176],\n",
       "          [0.6275, 0.6000, 0.4902,  ..., 0.3882, 0.3137, 0.2863],\n",
       "          [0.6078, 0.5725, 0.4510,  ..., 0.3216, 0.2745, 0.2706],\n",
       "          ...,\n",
       "          [0.6549, 0.6039, 0.6275,  ..., 0.1333, 0.2078, 0.3255],\n",
       "          [0.6039, 0.5961, 0.6314,  ..., 0.3647, 0.4471, 0.4745],\n",
       "          [0.5804, 0.5804, 0.6118,  ..., 0.5216, 0.5255, 0.5216]],\n",
       " \n",
       "         [[0.7333, 0.5333, 0.3725,  ..., 0.2784, 0.2784, 0.2745],\n",
       "          [0.6627, 0.6039, 0.4627,  ..., 0.3059, 0.2431, 0.2392],\n",
       "          [0.6431, 0.5843, 0.4392,  ..., 0.2510, 0.2157, 0.2157],\n",
       "          ...,\n",
       "          [0.6510, 0.6275, 0.6667,  ..., 0.1412, 0.2235, 0.3569],\n",
       "          [0.5020, 0.5098, 0.5569,  ..., 0.3765, 0.4706, 0.5137],\n",
       "          [0.4706, 0.4784, 0.5216,  ..., 0.5451, 0.5569, 0.5647]]]),\n",
       " 9)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
