{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a7a0e3-6db0-4026-95dd-31a23666364a",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Optimization\n",
    "\n",
    "Author: Bingchen Wang\n",
    "\n",
    "Last Updated: 9 Nov, 2022\n",
    "\n",
    "\n",
    "<nav>\n",
    "    <b>Deep learning navigation:</b> <a href=\"./Deep Learning Basics.ipynb\">Basics</a>\n",
    "</nav>\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118eba6b-de21-4c22-8f67-7a85f42fba8f",
   "metadata": {},
   "source": [
    "## Contents\n",
    "Algorithms:\n",
    "- [Batch Gradient Descent](#BGD)\n",
    "- [Stochastic Gradient Descent](#SGD)\n",
    "- [Mini-batch Gradient Descent](#MBGD)\n",
    "- [Gradient Descent with Momentum](#GDwM)\n",
    "- [RMSprop](#RMSp)\n",
    "- [Adam](#ADAM)\n",
    "\n",
    "Technique:\n",
    "- [Learning Rate Decay](#LRD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b62635-4d04-42c7-8708-f24cbe2f5bc0",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "Consider the linear model/hypothesis\n",
    "$$\n",
    "h_{\\theta}(x^{(i)}) = \\theta^{\\mathsf{T}} x^{(i)}\n",
    "$$\n",
    "and the squared loss function\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{2} {\\left(y^{(i)} - h_{\\theta}(x^{(i)})\\right)}^2\n",
    "$$\n",
    "\n",
    "<a name = \"BGD\"></a>\n",
    "### Batch Gradient Descent\n",
    "Use the entire training set of $m$ examples in every iteration of the training process.\n",
    "<blockquote>\n",
    "    Start with some $\\theta \\in \\mathbb{R}^p$ <br>\n",
    "    Repeat until convergence:\n",
    "    <blockquote>\n",
    "        Compute $h_\\theta(x^{(i)})$ for <b>all</b> examples $i = 1, \\dots, m$. <br>\n",
    "        Calculate the cost $$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m {\\left(y^{(i)} - h_{\\theta}(x^{(i)})\\right)}^2.$$ <br>\n",
    "        Update the parameters $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}, \\;\\;\\; \\text{for } j = 1, \\dots, P$$\n",
    "    </blockquote>\n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91a440-97f6-4b38-9f4f-81fe227adf57",
   "metadata": {},
   "source": [
    "<a name =\"SGD\"></a>\n",
    "### Stochastic Gradient Descent\n",
    "Use a single example in every iteration of the training process.\n",
    "\n",
    "<blockquote>\n",
    "    Start with some $\\theta \\in \\mathbb{R}^p$ <br>\n",
    "    Repeat until an approximate minimum is obtained or a certain threshold is met (e.g., max numbers of epochs):\n",
    "    <blockquote>\n",
    "        Randomly shuffle examples in the training set. <br>\n",
    "        For $i = 1, \\dots, m$ do:\n",
    "        <blockquote>\n",
    "            Compute $h_\\theta(x^{(i)})$. <br>\n",
    "            Calculate the cost $$J(\\theta) = \\frac{1}{2} {\\left(y^{(i)} - h_{\\theta}(x^{(i)})\\right)}^2.$$ <br>\n",
    "            Update the parameters $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}, \\;\\;\\; \\text{for } j = 1, \\dots, P$$\n",
    "        <blockquote>\n",
    "    </blockquote>\n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c439007-958f-4c28-a9b3-ffa6226f0bf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name = \"MBGD\"></a>\n",
    "### Mini-batch Gradient Descent\n",
    "Use a subset (a mini-batch) of the training set in every iteration of the training process. Denote the size of a mini-batch as $m_t$ and the total number of mini-batches as $T$, such that:\n",
    "$$\n",
    "m = m_t T\n",
    "$$\n",
    "<div class = \"alert alert-block alert-info\"> Typical choices for $m_t$: 64, 128, 256, 512, 1024. (Make sure that the mini-batch fits in the CPU/GPU memory.)\n",
    "</div>\n",
    "Further, denote the data in the mini-batch $t = 1, \\dots, T$ as $\\{X^{\\{t\\}}, y^{\\{t\\}}\\}$.\n",
    "\n",
    "<blockquote>\n",
    "    Start with some $\\theta \\in \\mathbb{R}^p$ <br>\n",
    "    Repeat until an approximate minimum is obtained or a certain threshold is met (e.g., max numbers of epochs):\n",
    "    <blockquote>\n",
    "        Randomly shuffle examples in the training set. Split the training set into $T$ mini-batches.<br>\n",
    "        For $t = 1, \\dots, T$ do:\n",
    "        <blockquote>\n",
    "            Compute $h_\\theta(x^{\\{t\\}(i)})$ for all examples in the mini-batch $t$. <br>\n",
    "            Calculate the cost $$J(\\theta) = \\frac{1}{2m_t} \\sum_{i = 1}^{m_t}{\\left(y^{\\{t\\}(i)} - h_{\\theta}(x^{\\{t\\}(i)})\\right)}^2.$$ <br>\n",
    "            Update the parameters $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}, \\;\\;\\; \\text{for } j = 1, \\dots, P$$\n",
    "        </blockquote>\n",
    "    </blockquote>\n",
    "</blockquote>\n",
    "<div class = \"alert alert-block alert-success\"><b>Advantages of mini-batch gradient descent</b>:\n",
    "<ul>\n",
    "    <li> Make use of vectorization to speed up the training.\n",
    "    <li> Make progress without processing the entire training set.\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "#### Performance Comparisions: Batch, Stochastic & Mini-batch Gradient Descents\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/gradient descent first 100 epochs.png\" style=\"width:60%;\" >\n",
    "    <img src=\"./images/gradient descent last 10 epochs.png\" style=\"width:60%;\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a405642c-7449-423f-b053-d931d1adf66f",
   "metadata": {},
   "source": [
    "<a name = \"GDwM\"></a>\n",
    "### Gradient Descent with Momentum\n",
    "\n",
    "#### Expoentially Weighted Moving Averages\n",
    "Denote the original time series examples as $\\{x_t\\}_{t=1}^T$.\n",
    "\n",
    "The **exponentially weighted moving averages** are computed using the recursive formula:\n",
    "$$\n",
    "\\begin{align}\n",
    "v_0 = & 0 \\\\\n",
    "v_t = & \\beta v_{t-1} + (1-\\beta) x_t, \\text{ for } t=1, \\dots, T\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\beta$ is a hyper-parameter that governs the smoothness of the averages. Roughly speaking, $v_t$ can be interpreted as approximately averaging over $\\frac{1}{1-\\beta}$ days (related to the fact that $(1-\\frac{1}{n})^n \\approx \\frac{1}{e}$, which is considered small enough in contribution to the average).\n",
    "<div class = \"alert alert-block alert-info\"> <b>Advantage over simple moving averages/windows:</b> Saves on memory (just need to keep one running variable and keep overwriting it), and computation.<br>\n",
    "<b>Disadvantage vis-Ã -vis simple moving averages/windows:</b> not the most accurate way to compute an average.\n",
    "</div>\n",
    "\n",
    "A problem with expoentially weighted moving averages is that it requires a **burn-in period** to be able to give sensible estimates of averages, which is conspicuous from the follow equations:\n",
    "$$\n",
    "\\begin{align}\n",
    "v_1 = & (1-\\beta) x_1 \\\\\n",
    "v_2 = & (1-\\beta)\\beta x_1 + (1-\\beta) x_2 \\\\\n",
    "v_3 = & (1-\\beta)\\beta^2 x_1 + (1-\\beta)\\beta x_2 + (1-\\beta) x_3 \\\\\n",
    "\\cdots &\n",
    "\\end{align}\n",
    "$$\n",
    "The closer $\\beta$ is to 1, the longer the burn-in period needs to be. To solve this problem and ensure better estimates near the beginning of the time series, bias correction can be used to improve the algorithm:\n",
    "$$\n",
    "\\tilde v_t = \\frac{v_t}{1-\\beta^t}, \\text{ for } t=1, \\dots, T\n",
    "$$\n",
    "<div class = \"alert alert-block alert-danger\"> <b>Note:</b> To apply bias correction, compute the un-corrected series first and then multiply the values of the original uncorrected series by the bias correction terms respectively.\n",
    "</div>\n",
    "<div class = \"alert alert-block alert-success\"> This makes use of the following result in algebra:\n",
    "    $$\n",
    "    s_t = \\sum_{s = 1}^t (1-\\beta) \\beta^{s-1} = (1-\\beta) \\frac{1 - \\beta^t}{1-\\beta} = 1 - \\beta^t\n",
    "    $$\n",
    "so\n",
    "    $$\n",
    "    \\tilde s_t = \\frac{s_t}{1-\\beta^t} = 1\n",
    "    $$\n",
    "</div>\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/exponentially weighted moving averages.png\" style=\"width:80%;\" > <br>\n",
    "    Applying exponentially weighted moving averages to the Oxford daily temperature data. \n",
    "</div>\n",
    "\n",
    "#### Gradient Descent with Momentum\n",
    "<blockquote>\n",
    "    Initiate $v_{d\\theta_j} = 0, \\text{ for } j = 1, \\dots, P$. <br>\n",
    "    On iteration $t$ (e.g., with mini-batch gradient descent):\n",
    "    <blockquote>\n",
    "        Compute $d \\theta_j := \\frac{\\partial J}{\\partial \\theta_j}, \\text{ for } j = 1, \\dots, P$ on the current mini-batch. <br>\n",
    "        Update the momentum terms:\n",
    "        $$\n",
    "        v_{d\\theta_j} := \\beta v_{d\\theta_j} + (1-\\beta) d \\theta_j, \\text{ for } j = 1, \\dots, P\n",
    "        $$\n",
    "        Update the parameter:\n",
    "        $$\n",
    "        \\theta_j := \\theta_j - \\alpha v_{d\\theta_j}, \\text{ for } j = 1, \\dots, P\n",
    "        $$\n",
    "    </blockquote>\n",
    "</blockquote>\n",
    "\n",
    "**Hyperparameters:** $\\alpha, \\beta$ ($= 0.9$ usually works well; i.e., averaging over the last 10 gradients).\n",
    "<div class = \"alert alert-block alert-info\"> <b>Note:</b> Sometimes, the following equivalent updating rule is used:\n",
    "    $$\n",
    "    v_{d\\theta_j} := \\beta v_{d\\theta_j} + d \\theta_j, \\text{ for } j = 1, \\dots, P\n",
    "    $$\n",
    "where $\\alpha$ needs to be re-tuned with a factor of $(1-\\beta)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81848d2-b7d8-4c33-bebf-e33456ee1725",
   "metadata": {},
   "source": [
    "### RMSprop\n",
    "<blockquote>\n",
    "    Initiate $s_{d\\theta_j} = 0, \\text{ for } j = 1, \\dots, P$ and $\\epsilon = 10^{-8}$. <br>\n",
    "    On iteration $t$ (e.g., with mini-batch gradient descent):\n",
    "    <blockquote>\n",
    "        Compute $d \\theta_j := \\frac{\\partial J}{\\partial \\theta_j}, \\text{ for } j = 1, \\dots, P$ on the current mini-batch. <br>\n",
    "        Update the mean squared term:\n",
    "        $$\n",
    "        s_{d\\theta_j} := \\beta_2 s_{d\\theta_j} + (1-\\beta_2) d \\theta_j^2, \\text{ for } j = 1, \\dots, P\n",
    "        $$\n",
    "        Update the parameter:\n",
    "        $$\n",
    "        \\theta_j := \\theta_j - \\alpha \\frac{d\\theta_j}{\\sqrt{s_{d\\theta_j} + \\epsilon}}, \\text{ for } j = 1, \\dots, P\n",
    "        $$\n",
    "    </blockquote>\n",
    "</blockquote>\n",
    "\n",
    "<div class = \"alert alert-block alert-success\"> <b>Note:</b> We add $\\epsilon$ to the denominator to avoid the divide-by-zero problem.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276285c1-7173-40c0-b91c-22d4f9af1c8b",
   "metadata": {},
   "source": [
    "<a name = \"ADAM\"></a>\n",
    "### Adaptive Moment Estimation (Adam)\n",
    "<blockquote>\n",
    "    Initiate $v_{d\\theta_j} = 0, s_{d\\theta_j} = 0, \\text{ for } j = 1, \\dots, P$ and $\\epsilon = 10^{-8}$. <br>\n",
    "    On iteration $t$ (e.g., with mini-batch gradient descent):\n",
    "    <blockquote>\n",
    "        Compute $d \\theta_j := \\frac{\\partial J}{\\partial \\theta_j}, \\text{ for } j = 1, \\dots, P$ on the current mini-batch. <br>\n",
    "        Update the momentum and the mean squared term:\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        v_{d\\theta_j} := & \\beta_1 v_{d\\theta_j} + (1-\\beta_1) d \\theta_j \\\\\n",
    "        s_{d\\theta_j} := & \\beta_2 s_{d\\theta_j} + (1-\\beta_2) d \\theta_j^2\n",
    "        \\end{align}\n",
    "        $$\n",
    "        for $j = 1, \\dots, P$. <br><br>\n",
    "        Apply bias corrections:\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        v_{d\\theta_j}^{\\text{corrected}} = & \\frac{v_{d\\theta_j}}{1-\\beta_1^t} \\\\\n",
    "        s_{d\\theta_j}^{\\text{corrected}} = & \\frac{s_{d\\theta_j}}{1-\\beta_2^t}\n",
    "        \\end{align}\n",
    "        $$\n",
    "        for $j = 1, \\dots, P$. <br><br>\n",
    "        Update the parameter:\n",
    "        $$\n",
    "        \\theta_j := \\theta_j - \\alpha \\frac{v_{d\\theta_j}^{\\text{corrected}}}{\\sqrt{s_{d\\theta_j}^{\\text{corrected}} + \\epsilon}}, \\text{ for } j = 1, \\dots, P\n",
    "        $$\n",
    "    </blockquote>\n",
    "</blockquote>\n",
    "\n",
    "**Typical values for the hyperparameters:**\n",
    "- $\\alpha$: needs to be tuned\n",
    "- $\\beta_1$ (momentum update): 0.9\n",
    "- $\\beta_2$ (mean squared update): 0.999\n",
    "- $\\epsilon$ : $10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2436dfb-a870-4d1e-95a8-6a2bb98cb8f7",
   "metadata": {},
   "source": [
    "## Technique\n",
    "<a name = \"LRD\"></a>\n",
    "### Learning Rate Decay\n",
    "Denote the current epoch as $i$, the decay rate as $k$ and the current mini-batch as $t$.\n",
    "\n",
    "#### Time-based decay\n",
    "$$\n",
    "\\alpha = \\frac{1}{1 + k \\times (i-1)} \\alpha_0\n",
    "$$\n",
    "#### Exponential decay\n",
    "$$\n",
    "\\alpha = k^{i-1}\\alpha_0\n",
    "$$\n",
    "#### Inverse square root decay\n",
    "**Version 1**: Based on the epoch number,\n",
    "$$\n",
    "\\alpha = \\frac{k}{\\sqrt{i}} \\alpha_0\n",
    "$$\n",
    "**Version 2**: Based on the mini-batch number,\n",
    "$$\n",
    "\\alpha = \\frac{k}{\\sqrt{t}} \\alpha_0\n",
    "$$\n",
    "#### Discrete staircase decay\n",
    "Denote the length of a step as $L_\\text{step}$.\n",
    "$$\n",
    "\\alpha = k^{\\left\\lfloor \\frac{i-1}{L_\\text{step}} \\right\\rfloor}\\alpha_0\n",
    "$$\n",
    "\n",
    "#### Comparisons of different decay methods\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Learning rate decay.png\" style=\"width:60%;\" > <br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
