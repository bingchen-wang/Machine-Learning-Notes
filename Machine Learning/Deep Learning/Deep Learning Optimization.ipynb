{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a7a0e3-6db0-4026-95dd-31a23666364a",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Optimization\n",
    "\n",
    "Author: Bingchen Wang\n",
    "\n",
    "Last Updated: 9 Nov, 2022\n",
    "\n",
    "\n",
    "<nav>\n",
    "    <b>Deep learning navigation:</b> <a href=\"./Deep Learning Basics.ipynb\">Basics</a>\n",
    "</nav>\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118eba6b-de21-4c22-8f67-7a85f42fba8f",
   "metadata": {},
   "source": [
    "## Contents\n",
    "Algorithms:\n",
    "- [Batch Gradient Descent](#BGD)\n",
    "- [Stochastic Gradient Descent](#SGD)\n",
    "- [Mini-batch Gradient Descent](#MBGD)\n",
    "- Gradient Descent with Momentum\n",
    "- RMSprop\n",
    "- Adam\n",
    "\n",
    "Technique:\n",
    "- Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b62635-4d04-42c7-8708-f24cbe2f5bc0",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "Consider the linear model/hypothesis\n",
    "$$\n",
    "h_{\\theta}(x^{(i)}) = \\theta^{\\mathsf{T}} x^{(i)}\n",
    "$$\n",
    "and the squared loss function\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{2} {\\left(y^{(i)} - h_{\\theta}(x^{(i)})\\right)}^2\n",
    "$$\n",
    "\n",
    "<a name = \"BGD\"></a>\n",
    "### Batch Gradient Descent\n",
    "Use the entire training set of $m$ examples in every iteration of the training process.\n",
    "<blockquote>\n",
    "    Start with some $\\theta \\in \\mathbb{R}^p$ <br>\n",
    "    Repeat until convergence:\n",
    "    <blockquote>\n",
    "        Compute $h_\\theta(x^{(i)})$ for <b>all</b> examples $i = 1, \\dots, m$. <br>\n",
    "        Calculate the cost $$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m {\\left(y^{(i)} - h_{\\theta}(x^{(i)})\\right)}^2.$$ <br>\n",
    "        Update the parameters $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}, \\;\\;\\; \\text{for } j = 1, \\dots, P$$\n",
    "    </blockquote>\n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a91a440-97f6-4b38-9f4f-81fe227adf57",
   "metadata": {},
   "source": [
    "<a name =\"SGD\"></a>\n",
    "### Stochastic Gradient Descent\n",
    "Use a single example in every iteration of the training process.\n",
    "\n",
    "<blockquote>\n",
    "    Start with some $\\theta \\in \\mathbb{R}^p$ <br>\n",
    "    Repeat until an approximate minimum is obtained or a certain threshold is met (e.g., max numbers of epochs):\n",
    "    <blockquote>\n",
    "        Randomly shuffle examples in the training set. <br>\n",
    "        For $i = 1, \\dots, m$ do:\n",
    "        <blockquote>\n",
    "            Compute $h_\\theta(x^{(i)})$. <br>\n",
    "            Calculate the cost $$J(\\theta) = \\frac{1}{2} {\\left(y^{(i)} - h_{\\theta}(x^{(i)})\\right)}^2.$$ <br>\n",
    "            Update the parameters $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}, \\;\\;\\; \\text{for } j = 1, \\dots, P$$\n",
    "        <blockquote>\n",
    "    </blockquote>\n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c439007-958f-4c28-a9b3-ffa6226f0bf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name = \"MBGD\"></a>\n",
    "### Mini-batch Gradient Descent\n",
    "Use a subset (a mini-batch) of the training set in every iteration of the training process. Denote the size of a mini-batch as $m_t$ and the total number of mini-batches as $T$, such that:\n",
    "$$\n",
    "m = m_t T\n",
    "$$\n",
    "<div class = \"alert alert-block alert-info\"> Typical choices for $m_t$: 64, 128, 256, 512, 1024. (Make sure that the mini-batch fits in the CPU/GPU memory.)\n",
    "</div>\n",
    "Further, denote the data in the mini-batch $t = 1, \\dots, T$ as $\\{X^{\\{t\\}}, y^{\\{t\\}}\\}$.\n",
    "\n",
    "<blockquote>\n",
    "    Start with some $\\theta \\in \\mathbb{R}^p$ <br>\n",
    "    Repeat until an approximate minimum is obtained or a certain threshold is met (e.g., max numbers of epochs):\n",
    "    <blockquote>\n",
    "        Randomly shuffle examples in the training set. Split the training set into $T$ mini-batches.<br>\n",
    "        For $t = 1, \\dots, T$ do:\n",
    "        <blockquote>\n",
    "            Compute $h_\\theta(x^{\\{t\\}(i)})$ for all examples in the mini-batch $t$. <br>\n",
    "            Calculate the cost $$J(\\theta) = \\frac{1}{2m_t} \\sum_{i = 1}^{m_t}{\\left(y^{\\{t\\}(i)} - h_{\\theta}(x^{\\{t\\}(i)})\\right)}^2.$$ <br>\n",
    "            Update the parameters $$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}, \\;\\;\\; \\text{for } j = 1, \\dots, P$$\n",
    "        </blockquote>\n",
    "    </blockquote>\n",
    "</blockquote>\n",
    "<div class = \"alert alert-block alert-success\"><b>Advantages of mini-batch gradient descent</b>:\n",
    "<ul>\n",
    "    <li> Make use of vectorization to speed up the training.\n",
    "    <li> Make progress without processing the entire training set.\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "#### Performance Comparisions: Batch, Stochastic & Mini-batch Gradient Descents\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/gradient descent first 100 epochs.png\" style=\"width:60%;\" >\n",
    "    <img src=\"./images/gradient descent last 10 epochs.png\" style=\"width:60%;\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ca6b9-d839-4433-87ed-a4576037b72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2187857-e666-4bb7-8ea4-85da32b6d157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
