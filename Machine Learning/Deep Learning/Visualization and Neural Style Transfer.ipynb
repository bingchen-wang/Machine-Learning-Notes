{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafecca3-2837-4aa3-af2d-d7cc6aa514ff",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "## Visualization and Neural Style Transfer\n",
    "\n",
    "Author: Binghen Wang\n",
    "\n",
    "Last Updated: 25 Dec, 2022\n",
    "\n",
    "<nav>\n",
    "    <b>Deep learning navigation:</b> <a href=\"./Deep Learning Basics.ipynb\">Deep Learning Basics</a> |\n",
    "    <a href=\"./Deep Learning Optimization.ipynb\">Optimization</a> |\n",
    "    <a href=\"./Recurrent Neural Networks.ipynb\">Recurrent Neural Networks</a> \n",
    "    <br>\n",
    "    <b>CNN navigation:</b> <a href=\"./Convolutional Neural Networks.ipynb\">CNN Basics</a> |\n",
    "    <a href=\"./Object Detection.ipynb\">Object Detection</a> |\n",
    "    <a href=\"./Face Recognition.ipynb\">Face Recognition</a>\n",
    "</nav>\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037822e7-db40-4f23-a817-9c8a35cd650a",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [Visualizing Deep CNN](#Visual)\n",
    "    - [Network Structure](#Visual-NS)\n",
    "    - [Visualization](#Visual-Vis)\n",
    "        - [Feature maps at different layers](#Visual-Vis-1)\n",
    "        - [Feature evolution during training](#Visual-Vis-2)\n",
    "    - [Other Takeaways](#Visual-OT)\n",
    "- [Neural Style Transfer](#NST)\n",
    "    - [Cost Function](#NST-CF)\n",
    "    - [Content Cost Function](#NST-CCF)\n",
    "    - [Style Cost Function](#NST-SCF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a92442-1da7-4dff-980a-80efe54fabbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name = \"Visual\"></a>\n",
    "## Visualizing Deep CNN\n",
    "Source: <a href='https://arxiv.org/pdf/1311.2901.pdf'>Visualizing and Understanding Convolutional Networks</a>, Fergus & Zeiler (2013)\n",
    "\n",
    "**Approach**: a multi-layered Deconvolutional Network (decovnet) (to project feature activations back to the input pixel space)\n",
    "<ol>\n",
    "    <li> set all other activations of a convnet layer to 0\n",
    "    <li> pass the feature maps as input to the attached decovnet layer\n",
    "    <li> repeat until input pixel space is reeached:\n",
    "    <ol>\n",
    "        <li> <b>unpool</b> (using <b>switch variables</b> passed from the convnet which store the locations of the maxima)\n",
    "        <li> <b>rectify</b> (reLU)\n",
    "        <li> <b>filter</b> (using transposed versions of the same filters as those in the convnet)\n",
    "    </ol>\n",
    "        to reconstruct the activity in the layer beneath that gave rise to the chosen activations\n",
    "</ol>\n",
    "\n",
    "<a name = \"Visual-NS\"></a>\n",
    "### Network Structure\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Visual CNN network structure.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "<a name = \"Visual-Vis\"></a>\n",
    "### Visualization\n",
    "\n",
    "<a name = \"Visual-Vis-1\"></a>\n",
    "#### Feature maps at different layers\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Visual CNN.png\" style=\"width:100%;\" >\n",
    "</div>\n",
    "\n",
    "**Takeaways:**\n",
    "1. **Visualizations (constructed via deconvnet) have lower variation than the corresponding image patches**. They also reveal much more readily the focus of a given activation (e.g. row 1, col 2 in layer 5).\n",
    "2. **Higher layers show greater invariance** to input deformations compared with lower layers.\n",
    "3. **There is strong grouping within each feature map**.\n",
    "\n",
    "<a name = \"Visual-Vis-2\"></a>\n",
    "#### Feature evolution during training\n",
    "The evolution of a randomly chosen subset of model features through training at epochs [1,2,5,10,20,30,40,64] is shown below:\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Visual CNN feature evolution.png\" style=\"width:100%;\" > <br>\n",
    "</div>\n",
    "\n",
    "**Main Takeaway: it is important to let the models train until fully converged.**\n",
    "- lower layers converge quickly within a few epochs\n",
    "- upper layers take longer to converge\n",
    "\n",
    "<a name = \"Visual-OT\"></a>\n",
    "### Other Takeaways\n",
    "- **Samll transformations to the image** (translate & scale) impact the lower layers more than upper layers. (Feature invariance)\n",
    "- The output of the studied model is **not invariant to rotation**, except for object with rotational symmetry.\n",
    "- The studied model does establish some degree of **correspondence** (analyzed through occlusion of different parts of an object).\n",
    "- The **overall depth** of the model is **important** for obtaining good performance.\n",
    "- **Increasing the size of the middle convolution** layers give a useful gain in performance. However, on top of this, enlarging the fully connected layers results in over-fitting.\n",
    "- **Using an ensemble of models** typically improve predictions.\n",
    "- **Transfer learning** of the improved ImageNet feature extractor shows a better performance than leading benchmarks for some other datasets, casting **doubt on the utility of benchmarks with small (i.e. $<10^4$) training sets**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa9ad8-4c13-47fd-83bc-7cd7cba978e2",
   "metadata": {},
   "source": [
    "<a name = \"NST\"></a>\n",
    "## Neural Style Transfer\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Neural Style Transfer.png\" style=\"width:80%;\" > <br>\n",
    "    Source: <a href = \"https://arxiv.org/pdf/1508.06576.pdf\">A Neural Algorithm of Artistic Style</a>, Bethge, Ecke &amp; Gatys (2015)\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "To perform a **neural style transfer**, we need a **content image $C$** and a **style image $S$**. The goal is to **generate an image $G$** that has the content of $C$ in the style of $S$. The approach involes the following steps:\n",
    "1. pick a **pre-trained model**, e.g. VGG-16.\n",
    "2. define a **cost function** based on $C,S,G$.\n",
    "3. initiate an image $G$ and run **gradient descent in input space** (rather than the parameters).\n",
    "\n",
    "<a name = \"NST-CF\"></a>\n",
    "### Cost Function\n",
    "$$\n",
    "J(G) = \\alpha \\, J_{\\text{content}}(C,G) + \\beta \\, J_{\\text{style}}(S,G)\n",
    "$$\n",
    "where $\\alpha$ and $\\beta$ controls the weights of content and style in the generated image. A larger $\\alpha$ (a lower $\\beta$) emphasizes content over style.\n",
    "\n",
    "<a name = \"NST-CCF\"></a>\n",
    "### Content Cost Function\n",
    "An example of a **content cost function** is:\n",
    "$$\n",
    "J_{\\text{content}}(C,G) = \\frac{1}{2} {\\left\\Vert a^{[l](C)} - a^{[l](G)} \\right\\Vert}^2\n",
    "$$\n",
    "If $a^{[l](C)}$ and $a^{[l](G)}$ are similar, both images have similar content.\n",
    "\n",
    "<a name = \"NST-SCF\"></a>\n",
    "### Style Cost Function\n",
    "As a measure of the similarity in style, we consider correlation between activations across channels. (E.g. Stripes tend to be in orange and with a certain stroke.)\n",
    "#### Style Matrix\n",
    "To define the style cost funciotn, we first introduce the concept of **Style Matrix** (also known as **Gram Matrix**). Let $a^{[l]}_{i,j,k} = $ activation at $(i,j,k)$ (height, width, channel). Then the Style Matrix $G^[l]$ is of shape $n_C^{[l]} \\times n_C^{[l]}$ and is defined by:\n",
    "$$\n",
    "G^{[l]} = \\left[\\begin{array}{ccccc}\n",
    "G_{11}^{[l]} & G_{12}^{[l]} & \\cdots & G_{1,n_C^{[l]}-1}^{[l]} & G_{1,n_C^{[l]}}^{[l]} \\\\\n",
    "G_{21}^{[l]} & G_{22}^{[l]} & \\cdots & G_{2,n_C^{[l]}-1}^{[l]} & G_{2,n_C^{[l]}}^{[l]} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "G_{n_C^{[l]}-1,1}^{[l]} & G_{n_C^{[l]}-1,2}^{[l]} & \\cdots & G_{n_C^{[l]}-1,n_C^{[l]}-1}^{[l]} & G_{n_C^{[l]}-1,n_C^{[l]}}^{[l]} \\\\\n",
    "G_{n_C^{[l]},1}^{[l]} & G_{n_C^{[l]},2}^{[l]} & \\cdots & G_{n_C^{[l]},n_C^{[l]}-1}^{[l]} & G_{n_C^{[l]},n_C^{[l]}}^{[l]}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "where\n",
    "$$\n",
    "G_{kk^\\prime}^{[l]} = \\sum_{i=1}^{n_H^{[l]}}\\sum_{j=1}^{n_W^{[l]}} a_{ijk}^{[l]}a_{ijk^\\prime}^{[l]}\n",
    "$$\n",
    "\n",
    "#### Style Cost Function\n",
    "Using the style matrixes of $S$ and $G$, we define the **style cost function** of layer l as follows:\n",
    "$$\n",
    "J_{\\text{style}}^{[l]}(S,G) = \\frac{1}{{\\left(2n_H^{[l]}n_W^{[l]}n_C^{[l]}\\right)}^2} {\\left\\Vert G^{[l](S)} - G^{[l](G)} \\right\\Vert}_F^2 = \\frac{1}{{\\left(2n_H^{[l]}n_W^{[l]}n_C^{[l]}\\right)}^2} \\sum_{k} \\sum_{k^\\prime}  {\\left(G_{kk^\\prime}^{[l](S)} - G_{kk^\\prime}^{[l](G)}\\right)}^2\n",
    "$$\n",
    "\n",
    "We can use information from multiple layers for improved performance:\n",
    "$$\n",
    "J_{\\text{style}}(S,G) = \\sum_{l} \\lambda^{[l]} J_{\\text{style}}^{[l]}(S,G)\n",
    "$$\n",
    "where $\\lambda^{[l]}$ governs the contribution of layer l to the style cost function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
