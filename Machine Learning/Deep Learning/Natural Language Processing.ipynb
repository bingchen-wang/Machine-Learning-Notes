{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b895e932-bafa-4f58-ab6b-6106652e75fb",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "## Natural Language Processing\n",
    "\n",
    "Author: Binghen Wang\n",
    "\n",
    "Last Updated: 1 Jan, 2023\n",
    "\n",
    "<nav>\n",
    "    <b>Deep learning navigation:</b> <a href=\"./Deep Learning Basics.ipynb\">Deep Learning Basics</a> |\n",
    "    <a href=\"./Deep Learning Optimization.ipynb\">Optimization</a> |\n",
    "    <a href=\"./Convolutional Neural Networks.ipynb\">Convolutional Neural Networks</a>\n",
    "    <br>\n",
    "    <b>RNN navigation:</b> <a href=\"./Recurrent Neural Networks.ipynb\">Basics</a> \n",
    "</nav>\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca4a5d-8ce8-4989-8d88-5fa5e0a340c1",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- Word Embeddings\n",
    "- Learning Word Embeddings\n",
    "- Applying Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9df46-8cf5-4f11-9c2b-e112f2ed1524",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "### One-hot Representation vs Featurized Representation\n",
    "Previously, the language models make use of the **one-hot representation** of a word, due largely to its ease of implementation. Yet, one-hot representation has a drawback in that it regards each word as a new class and does not establish any relationship/similarity between the words.\n",
    "\n",
    "Consider for instance the following two sentences:\n",
    "<blockquote>\n",
    "    I want a glass of orange <u>juice</u>. <br>\n",
    "    I want a glass of apple _____.\n",
    "</blockquote>\n",
    "\n",
    "Learning that juice usually comes after orange does not easily generalize to the case of apple.<br>\n",
    "\n",
    "From a lingual perspective, apple and orange are closely related in that they are both sweet fruits that contain a lot of water. To be able to make the learning more efficient and generalizable, we can use a **featurized representation**.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/word embeddings.png\" style=\"width:80%;\" >\n",
    "</div>\n",
    "\n",
    "### Transfer Learning and Word Embeddings\n",
    "Word embeddings could make learning more efficient by:\n",
    "- requiring fewer labelled data to train the model\n",
    "- dealing better with unseen words\n",
    "- using more compact representations for words (each word is represented with a shorter vector)\n",
    "\n",
    "A popular way to train language models using word embeddings is through **transfer learning**, which takes the following steps:\n",
    "1. Learn word embeddings from large text corpus (1-100 billion words) or **download** pre-trained embeddings online.\n",
    "2. **Transfer** the embeddings to new task with a smaller training set (say, 100k words).\n",
    "3. (Optional) Continue to **finetune** the word embeddings with new data (only if there is a large volumne of training data).\n",
    "\n",
    "### Analogy Reasoning\n",
    "<blockquote>\n",
    "    <b>Man</b> is to <b>woman</b>, as <b>king</b> is to <b>queen</b>.\n",
    "</blockquote>\n",
    "Let $e_{\\text{man}}$ denote the featurized representation of the word man. We expect:\n",
    "$$\n",
    "e_{\\text{man}} - e_{\\text{woman}} \\approx e_{\\text{king}} - e_{\\text{queen}}\n",
    "$$\n",
    "\n",
    "**Formalized Problem**: Find word $w$ that satisfies:\n",
    "$$\n",
    "\\text{argmax}_w \\text{sim}(e_w,  e_{\\text{king}} - e_{\\text{man}} + e_{\\text{woman}})\n",
    "$$\n",
    "where $\\text{sim}$ is a similarity function. \n",
    "\n",
    "A commonly used similarity function is the **cosine similarity**:\n",
    "$$\n",
    "\\text{sim}(u,v) = \\frac{u^{\\mathsf{T}}v}{\\Vert u \\Vert_2\\Vert v \\Vert_2}\n",
    "$$\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/cosine similarity.png\" style=\"width:80%;\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703d19b-c10a-48c4-bced-04c1cf217028",
   "metadata": {},
   "source": [
    "## Learning Word Embeddings\n",
    "\n",
    "### Embedding Matrix\n",
    "Given a vocabulary of size 10,000: \\[a, aaron, $\\dots$, zulu, \\<UNK\\>\\], consider a **word embedding** of length 300. The embedding matrix is $300 \\times 10,000$ and is given by:\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/embedding matrix.png\" style=\"width:80%;\" >\n",
    "</div>\n",
    "\n",
    "The embedding matrix could be used to convert a **one-hot representation** into a **featurized representation**. \n",
    "$$\n",
    "E\\,o_{6527} = e_{6527}\n",
    "$$\n",
    "It can also employ vectorization and convert a entire sentence in one fell swoop.\n",
    "$$\n",
    "E\\,[o_{6527}, o_{456}, \\dots, o_{271}] = [e_{6527}, e_{456}, \\dots, e_{271}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e487c-7ca1-4bde-8e4a-90ad01d32fab",
   "metadata": {},
   "source": [
    "### Neural Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d4221f-bb90-41d6-a5f2-2d1090431311",
   "metadata": {},
   "source": [
    "### Word2Vec Skip-grams Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c9d4f7-dfa8-4ffb-b934-3fd74177a168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74803cd9-a2dc-4321-8ee2-a53c1c73d481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f43f64-1354-4c9c-a600-4db658cd99c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec2022-3df3-408a-b3ee-03466e7cbe42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
