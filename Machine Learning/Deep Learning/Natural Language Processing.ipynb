{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b895e932-bafa-4f58-ab6b-6106652e75fb",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "## Natural Language Processing\n",
    "\n",
    "Author: Binghen Wang\n",
    "\n",
    "Last Updated: 8 Jan, 2023\n",
    "\n",
    "<nav>\n",
    "    <b>Deep learning navigation:</b> <a href=\"./Deep Learning Basics.ipynb\">Deep Learning Basics</a> |\n",
    "    <a href=\"./Deep Learning Optimization.ipynb\">Optimization</a> |\n",
    "    <a href=\"./Convolutional Neural Networks.ipynb\">Convolutional Neural Networks</a>\n",
    "    <br>\n",
    "    <b>RNN navigation:</b> <a href=\"./Recurrent Neural Networks.ipynb\">Basics</a> \n",
    "</nav>\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca4a5d-8ce8-4989-8d88-5fa5e0a340c1",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [Word Embeddings](#WE)\n",
    "    - [One-hot Representation vs Featurized Representation](#WE-1)\n",
    "    - [Transfer Learning and Word Embeddings](#WE-2)\n",
    "    - [Analogical Reasoning](#WE-3)\n",
    "- [Learning Word Embeddings](#LWE)\n",
    "    - [Embedding Matrix](#LWE-1)\n",
    "    - [Neural Language Model](#LWE-2)\n",
    "    - [The Skip-gram Model](#LWE-3)\n",
    "        - [Basic Model](#LWE-3-1)\n",
    "        - [Hierarchical Softmax](#LWE-3-2)\n",
    "        - [Negative Sampling](#LWE-3-3)\n",
    "    - [GloVe (Global Vectors for Word Representation)](#LWE-4)\n",
    "- [Applying Word Embeddings](#AWE)\n",
    "    - [Debiasing Word Embeddings](#AWE-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9df46-8cf5-4f11-9c2b-e112f2ed1524",
   "metadata": {},
   "source": [
    "<a name = \"WE\"></a>\n",
    "## Word Embeddings\n",
    "\n",
    "<a name = \"WE-1\"></a>\n",
    "### One-hot Representation vs Featurized Representation\n",
    "Previously, the language models make use of the **one-hot representation** of a word, due largely to its ease of implementation. Yet, one-hot representation has a drawback in that it regards each word as a new class and does not establish any relationship/similarity between the words.\n",
    "\n",
    "Consider for instance the following two sentences:\n",
    "<blockquote>\n",
    "    I want a glass of orange <u>juice</u>. <br>\n",
    "    I want a glass of apple _____.\n",
    "</blockquote>\n",
    "\n",
    "Learning that juice usually comes after orange does not easily generalize to the case of apple.<br>\n",
    "\n",
    "From a lingual perspective, apple and orange are closely related in that they are both sweet fruits that contain a lot of water. To be able to make the learning more efficient and generalizable, we can use a **featurized representation**.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/word embeddings.png\" style=\"width:80%;\" >\n",
    "</div>\n",
    "\n",
    "<a name = \"WE-2\"></a>\n",
    "### Transfer Learning and Word Embeddings\n",
    "Word embeddings could make learning more efficient by:\n",
    "- requiring fewer labelled data to train the model\n",
    "- dealing better with unseen words\n",
    "- using more compact representations for words (each word is represented with a shorter vector)\n",
    "\n",
    "A popular way to train language models using word embeddings is through **transfer learning**, which takes the following steps:\n",
    "1. Learn word embeddings from large text corpus (1-100 billion words) or **download** pre-trained embeddings online.\n",
    "2. **Transfer** the embeddings to new task with a smaller training set (say, 100k words).\n",
    "3. (Optional) Continue to **finetune** the word embeddings with new data (only if there is a large volumne of training data).\n",
    "\n",
    "<a name = \"WE-3\"></a>\n",
    "### Analogical Reasoning\n",
    "<blockquote>\n",
    "    <b>Man</b> is to <b>woman</b>, as <b>king</b> is to <b>queen</b>.\n",
    "</blockquote>\n",
    "Let $e_{\\text{man}}$ denote the featurized representation of the word man. We expect:\n",
    "$$\n",
    "e_{\\text{man}} - e_{\\text{woman}} \\approx e_{\\text{king}} - e_{\\text{queen}}\n",
    "$$\n",
    "\n",
    "**Formalized Problem**: Find word $w$ that satisfies:\n",
    "$$\n",
    "\\text{argmax}_w \\text{sim}(e_w,  e_{\\text{king}} - e_{\\text{man}} + e_{\\text{woman}})\n",
    "$$\n",
    "where $\\text{sim}$ is a similarity function. \n",
    "\n",
    "A commonly used similarity function is the **cosine similarity**:\n",
    "$$\n",
    "\\text{sim}(u,v) = \\frac{u^{\\mathsf{T}}v}{\\Vert u \\Vert_2\\Vert v \\Vert_2}\n",
    "$$\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/cosine similarity.png\" style=\"width:80%;\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703d19b-c10a-48c4-bced-04c1cf217028",
   "metadata": {},
   "source": [
    "<a name = \"LWE\"></a>\n",
    "## Learning Word Embeddings\n",
    "\n",
    "<a name = \"LWE-1\"></a>\n",
    "### Embedding Matrix\n",
    "Given a vocabulary of size 10,000: \\[a, aaron, $\\dots$, zulu, \\<UNK\\>\\], consider a **word embedding** of length 300. The embedding matrix is $300 \\times 10,000$ and is given by:\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/embedding matrix.png\" style=\"width:80%;\" >\n",
    "</div>\n",
    "\n",
    "The embedding matrix could be used to convert a **one-hot representation** into a **featurized representation**. \n",
    "$$\n",
    "E\\,o_{6527} = e_{6527}\n",
    "$$\n",
    "It can also employ vectorization and convert a entire sentence in one fell swoop.\n",
    "$$\n",
    "E\\,[o_{6527}, o_{456}, \\dots, o_{271}] = [e_{6527}, e_{456}, \\dots, e_{271}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e487c-7ca1-4bde-8e4a-90ad01d32fab",
   "metadata": {},
   "source": [
    "<a name = \"LWE-2\"></a>\n",
    "### Neural Language Model\n",
    "\n",
    "#### Basic idea \n",
    "- Use a fixed historical window to process the input (say, a four-word history)\n",
    "- Train a language model and treat the embedding matrix $E$ as a parameter of the model\n",
    "- Use gradient descent and backprop to update the parameters of the model\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/learning word embeddings nlm.png\" style=\"width:50%;\" >\n",
    "</div>\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Note:</b> The <b>accuracy</b> of the model is of <b>less importance</b> than the training of the model.</div>\n",
    "\n",
    "#### Other context/target pairs\n",
    "<blockquote>\n",
    "    I want a glass of orange <u>juice</u> to go along with my cereal.\n",
    "</blockquote>\n",
    "\n",
    "**Target**: <font color= \"red\">juice</font>\n",
    "\n",
    "**Context**: \n",
    "- Last 4 words: <font color = 'blue'>a glass of orange</font>\n",
    "- 4 words on the left & right: <font color = 'blue'>a glass of orange to go along with</font>\n",
    "- Last 1 word: <font color = 'blue'>orange</font>\n",
    "- Nearby 1 word (skip-gram model) <font color = 'blue'>glass</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d4221f-bb90-41d6-a5f2-2d1090431311",
   "metadata": {},
   "source": [
    "<a name = \"LWE-3\"></a>\n",
    "### The Skip-gram Model\n",
    "<a name = \"LWE-3-1\"></a>\n",
    "#### Basic Model\n",
    "Instead of choosing the last 4 words to learn word embeddings, we can also choose a nearby 1 word (e.g., sample a word from the surrounding 10 words of a word), which gives us the **Skip-gram Model**.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/word2vec.png\" style=\"width:30%;\" >\n",
    "</div>\n",
    "\n",
    "Denote the context word (input word) as $c$ and the target word (output word) as $t$, so the word embeddings for them are $e_c$ and $e_t$ respectively. Then the softmax probabilities are calculated as:\n",
    "$$\n",
    "P(t\\vert c) = \\frac{\\exp\\left(\\theta_t^Te_c\\right)}{\\sum_{j=1}^{10000} \\exp\\left(\\theta_j^Te_c\\right)}\n",
    "$$\n",
    "\n",
    "Using the one-hot representation for the labels and predictions, the **loss function** is defined by:\n",
    "$$\n",
    "L(\\hat y, y) = - \\sum_{i=1}^{10000} y_i \\log \\hat y_i.\n",
    "$$\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Tip on sampling the context word:</b> Avoid sampling uniformly (which would result in most sampled words belong to a small class of common words) and instead try to make a balanced sample of common and uncommon words.</div>\n",
    "\n",
    "<a name = \"LWE-3-2\"></a>\n",
    "#### Hierarchical Softmax\n",
    "\n",
    "Calculating 10,000 probabilities and adding them up can cause the learning of the model to be really inefficient. Different ways have been come up with to speed up the algorithm, one of them being the **hierarchical softmax**. It works by using a binary **Huffman tree** in place of the original softmax layer to reduce the number of classes that are being predicted at one time and consequently reduces the cost of computation from $c$ to $\\log_2 c$. In a binary Huffman tree, common words tend to appear near the root while uncommon words are buried deep down the tree.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/hierarchical softmax.png\" style=\"width:30%;\" >\n",
    "</div>\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>A speedup technique:</b> Group words together by their frequency.</div>\n",
    "\n",
    "\n",
    "<a href = \"https://arxiv.org/pdf/1310.4546.pdf\">Mikolov et al. (2013)</a> gave a convenient mathematical formula for the probabilities (fed into the loss function) using hierarchical softmax.\n",
    "<blockquote>\n",
    "    Using the notation in <a href = \"https://arxiv.org/pdf/1310.4546.pdf\">Mikolov et al. (2013)</a>, the <b>objective</b> of the basic Skip-gram model is\n",
    "    $$\n",
    "    \\frac{1}{T} \\sum_{t = 1}^{T} \\sum_{-c\\leq j \\leq c, j \\neq 0} \\log p(w_{t+j} \\vert w_t)\n",
    "    $$\n",
    "    where\n",
    "    <ul>\n",
    "        <li> $c$ is the size of the training context (which can be a function of the center word $w_t$).\n",
    "    </ul>\n",
    "    The <b>$p(w_O \\vert w_I)$ defined using the softmax function</b> is\n",
    "    $$\n",
    "    p(w_O \\vert w_I) = \\frac{\\exp\\left(\\theta_{w_O}^{\\mathsf{T}} e_{w_I}\\right)}{\\sum_{w=1}^{W}\\exp\\left(\\theta_{w}^{\\mathsf{T}} e_{w_I}\\right)}\n",
    "    $$\n",
    "    where\n",
    "    <ul>\n",
    "        <li> $w_O$ and $w_I$ are the output and input words;\n",
    "        <li> $W$ is the number of words in the vocabulary.\n",
    "    </ul>\n",
    "    The <b>$p(w \\vert w_I)$ defined using the hierarchical softmax</b> is\n",
    "    $$\n",
    "    p(w \\vert w_I)= \\Pi_{j=1}^{L(w)-1} \\sigma\\left( \\text{Cond}\\left\\{n(w,j+1) = \\text{ch}(n(w,j))\\right\\}\\cdot \\theta_{n(w,j)}^{\\mathsf{T}}e_{w_I}\\right)\n",
    "    $$\n",
    "    where\n",
    "    <ul>\n",
    "        <li> $n(w,j)$ is the $j$-th node on the path from the root to $w$;\n",
    "        <li> $\\text{ch}(n)$ is any <b>arbitrary</b> fixed child of $n$;\n",
    "        <li> $\\text{Cond}\\{x\\}$ is 1 if $x$ is true and 0 otherwise;\n",
    "        <li> $\\sigma(x) = \\frac{1}{1+\\exp{(-x)}}$.\n",
    "    </ul>\n",
    "</blockquote>\n",
    "\n",
    "<a name = \"LWE-3-3\"></a>\n",
    "#### Negative Sampling\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/negative sampling.png\" style=\"width:40%;\" >\n",
    "</div>\n",
    "\n",
    "The **objective** of Negative sampling:\n",
    "$$\n",
    "\\log \\sigma(\\theta_{w_O}^{\\mathsf{T}}e_{w_I}) + \\sum_{i=1}^k \\mathbb{E}_{w_i\\sim P_n(w)}\\left[\\log\\sigma(-\\theta_{w_i}^{\\mathsf{T}}e_{w_I})\\right]\n",
    "$$\n",
    "\n",
    "The choice for $k$ can be:\n",
    "- 5-20 for small training datasets.\n",
    "- 2-5 for large datasets.\n",
    "\n",
    "The choice for $P_n(w)$ can be:\n",
    "$$\n",
    "P_n(w) = \\frac{{U(w)}^{3/4}}{\\sum_{j=1}^W {U(w_j)}^{3/4}}\n",
    "$$\n",
    "where\n",
    "$U(\\cdot)$ is the unigram distribution (each word's sample frequency in the corpus).\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Tip on sampling the context word:</b> Downsampling frequent words help speed up the learning process. <a href = \"https://arxiv.org/pdf/1310.4546.pdf\">Mikolov et al. (2013)</a> suggest using the following probability to discard each word $w_i$ upon training: $$P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}$$where $f(w_i)$ is the frequency of word $w_i$ and $t$ is a chosen threshold.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b59bd-2fb7-432a-83bb-1a177916794f",
   "metadata": {},
   "source": [
    "<a name = \"LWE-4\"></a>\n",
    "### GloVe (Global Vectors for Word Representation)\n",
    "Let \n",
    "$$\n",
    "X_{ij} = \\text{numebr of times word i appears in the context of word j}.\n",
    "$$\n",
    "\n",
    "<div class = \"alert alert-block alert-success\"><b>Note:</b> If we use the previous definition for the context (i.e., $\\pm c$ words), then it follows that $$X_{ij}= X_{ji}.$$ But this need not be the case.</div>\n",
    "\n",
    "The **objective** of GloVe is\n",
    "$$\n",
    "\\min \\sum_{i=1}^{W} \\sum_{j=1}^{W} f(X_{ij})\\left(\\theta_i^{\\mathsf{T}}e_j +b_i + b_j^\\prime - \\log X_{ij}\\right)\n",
    "$$\n",
    "where $f(X_{ij})$ is the weight function, with $f(X_{ij})= 0$ if $X_{ij} = 0$. $f(X_{ij})$ can assign different weights to frequent and infrequent words.\n",
    "\n",
    "Two important **features** of GloVe:\n",
    "- $\\theta_i$ and $e_j$ are **symmetric**. Therefore, for the final output, we can take the average of the two for each word, $e_w^{\\text{(final)}} = \\frac{\\theta_w + e_w}{2}$.\n",
    "- Representations are **not unique** and thus the featurization uninterpretable. For any compatible matrix $A$ such that $A^{\\mathsf{T}}A = I$,\n",
    "$$\n",
    "{(A\\theta_i)}^{\\mathsf{T}}(Ae_j) = \\theta_i^{\\mathsf{T}} A^{\\mathsf{T}}A e_j = \\theta_i^{\\mathsf{T}} e_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d36aea-3487-47a1-984c-c7ab79e79c48",
   "metadata": {},
   "source": [
    "<a name = \"AWE\"></a>\n",
    "## Applying Word Embeddings\n",
    "<a name = \"AWE-1\"></a>\n",
    "### Debiasing Word Embeddings\n",
    "Word embeddings could reflect the biases of the text corpus from which they are trained. Debiasing the word embeddings so that they do not discriminate against certain gender, ethnicity, sexual orientation and age group is an important pre-requisite for trustworthy learning algorithms. Examples of biases could be illustrated using analogical reasoning:\n",
    "<blockquote>\n",
    "    Man:Doctor - Woman:Nurse\n",
    "</blockquote>\n",
    "\n",
    "#### A Simplified Debiasing Algorithm (from Deep Learning Specialization by Deeplearning.AI)\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/debiasing word embeddings.png\" style=\"width:80%;\" >\n",
    "</div>\n",
    "\n",
    "**Steps**:\n",
    "1. Identify the bias direction.\n",
    "$$\n",
    "\\text{bias}_{\\text{gender}} = \\text{avg}(e_{he}-e_{she} , e_{male}-e_{female}, \\dots , e_{grandpa} - e_{grandma})\n",
    "$$\n",
    "2. Neutralize: For every word that is not definitional, project to get rid of bias.\n",
    "3. Equalize pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a40a7ea-a98a-47f3-a84a-3c64e1c36dee",
   "metadata": {},
   "source": [
    "## References\n",
    "- <a href = \"https://www.coursera.org/learn/nlp-sequence-models/\">Sequence Models</a>, **Deep Learning Specialization** (Andrew Ng, DeepLearning.AI)\n",
    "- <a href = \"https://arxiv.org/pdf/1310.4546.pdf\">Distributed Representations of Words and Phrases and their Compositionality.</a> (Mikolov et al., 2013)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
