{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af06b28e-5b7f-4f76-9516-e9a23ee9e423",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Author: Binghen Wang\n",
    "\n",
    "Last Updated: 31 Dec, 2022\n",
    "\n",
    "<nav>\n",
    "    <b>Deep learning navigation:</b> <a href=\"./Deep Learning Basics.ipynb\">Deep Learning Basics</a> |\n",
    "    <a href=\"./Deep Learning Optimization.ipynb\">Optimization</a> |\n",
    "    <a href=\"./Convolutional Neural Networks.ipynb\">Convolutional Neural Networks</a>\n",
    "    <br>\n",
    "    <b>RNN navigation:</b>\n",
    "</nav>\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0241e3-562c-45da-8f15-250bb84049b8",
   "metadata": {},
   "source": [
    "## Contents\n",
    "Basics:\n",
    "- [Examples of Sequence Models](#ESM)\n",
    "- [Standard Notation](#StandardN)\n",
    "- [Recurrent Neural Networks](#RNN)\n",
    "    - [Other Variants](#RNN-OV)\n",
    "    - [Bidirectional RNN](#RNN-BRNN)\n",
    "    - [Deep RNN](#RNN-DRNN)\n",
    "    - [Language Model](#RNN-LM)\n",
    "- [Gated Recurrent Unit (GRU)](#GRU)\n",
    "- [Long Short Term Memory (LSTM)](#LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac992288-1fd8-482d-9590-283532d0872a",
   "metadata": {},
   "source": [
    "## Basics\n",
    "<a name = \"ESM\"></a>\n",
    "### Examples of Sequence Models\n",
    "<table>\n",
    "    <tr>\n",
    "    <th>Task</th>\n",
    "    <th>Input</th>\n",
    "    <th>Output</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Name entity recognition</td>\n",
    "        <td>text</td>\n",
    "        <td>text with located and classified names</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>DNA sequence analysis</td>\n",
    "        <td>DNA sequence</td>\n",
    "        <td>DNA sequence with located and classified segments</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Machine translation</td>\n",
    "        <td>text in language A</td>\n",
    "        <td>text in language B</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Speech recognition</td>\n",
    "        <td>audio</td>\n",
    "        <td>text</td>\n",
    "    </tr><tr>\n",
    "        <td>Video activity recognition</td>\n",
    "        <td>video</td>\n",
    "        <td>label</td>\n",
    "    </tr><tr>\n",
    "        <td>Sentiment classification</td>\n",
    "        <td>text</td>\n",
    "        <td>label (e.g. Likert scale rating)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Music generation</td>\n",
    "        <td>N/A</td>\n",
    "        <td>audio</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>poem generation</td>\n",
    "        <td>keywords</td>\n",
    "        <td>text</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb8f3e4-3aa6-4241-af94-b91fb352e58a",
   "metadata": {},
   "source": [
    "<a name = \"StandardN\"></a>\n",
    "### Standard Notation\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <th>Notation</th>\n",
    "    <th>Meaning</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$T_x^{(i)}$</td>\n",
    "        <td>length of input $i$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$T_y^{(i)}$</td>\n",
    "        <td>length of output $i$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$x^{(i)<t>}$</td>\n",
    "        <td>input $i$ at temporal location $t$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$y^{(i)<t>}$</td>\n",
    "        <td>out $i$ at temporal location $t$</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa2946-43f9-44f5-8d3b-598e6500200e",
   "metadata": {},
   "source": [
    "<a name = \"RNN\"></a>\n",
    "### Recurrent Neural Networks\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/RNN.png\" style=\"width:80%;\" >\n",
    "</div>\n",
    "\n",
    "Note that the parameters are shared across time. The **loss function** used for backpropagation takes the following form:\n",
    "$$\n",
    "L = \\sum_t L(\\hat y^{<t>}, y^{<t>}) = - \\sum_t \\sum_i y_i^{<t>} \\log \\hat y_i^{<t>}\n",
    "$$\n",
    "\n",
    "<a name = \"RNN-OV\"></a>\n",
    "#### Other RNN Variants\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/RNN types.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "<a name = \"RNN-BRNN\"></a>\n",
    "#### Bidirectional RNN\n",
    "Unidirectional RNN suffers from the drawback of not taking into consideration the context that comes after a word. A bidirectional RNN helps address this.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/BRNN.png\" style=\"width:70%;\" >\n",
    "</div>\n",
    "\n",
    "<a name = \"RNN-DRNN\"></a>\n",
    "#### Deep RNN\n",
    "A deep RNN stacks together a bunch of simple RNNs vertically and would require much longer time to train.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Deep RNN.png\" style=\"width:70%;\" >\n",
    "</div>\n",
    "\n",
    "\n",
    "<a name = \"RNN-LM\"></a>\n",
    "#### Language Model\n",
    "Consider the task of training a **language model** that generates poems based on a keyword or nothing. We can train a one-to-many RNN to achieve this goal.\n",
    "\n",
    "**Data:** a large corpus of poems text $\\{{y^{<1>(1)}, \\dots, y^{<T_y^{(i)}>(i)}}\\}_{i=1}^m$<br>\n",
    "**Data preparation**: **Tokenize** the unique words of the corpus into a vocabulary/dictionary, which is then used to generate one-hot representations of words in a sentence. In the vocabulary, it would be helpful to add two extra tokens, `<EOS>` (end of sentence) and `<UNK>` (unknown). <br>\n",
    "**Loss function**: $$\n",
    "L = \\sum_t L(\\hat y^{<t>}, y^{<t>}) = - \\sum_t \\sum_j y_j^{<t>} \\log \\hat y_j^{<t>}\n",
    "$$\n",
    "**Model during training**: We pass the true $y^{<t>}$ (*not the predicted $\\hat y^{<t>}$*) as the input $x^{<t+1>}$ at time step $t+1$.\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/language model training.png\" style=\"width:50%;\" >\n",
    "</div><br>\n",
    "\n",
    "**Model during application**: There are several nuances when applying the model to generate sequences:\n",
    "1. From the second time step onwards, we **sample** a word based on the **softmax distribution** of the outcome in the previous time step $P(y^{<t-1>}| \\mathcal{I_{t-2}})$ and feed it as the input $x^{<t>}$ to the recurrent neural network. <br>*<font color = darkblue>Note: in the context of the task, we do not use word with maximum probability as the input in the subsequent time step, as doing so would yield identical sequences all the time. We want the model to generate a <b>different sequence</b> each time we use it.</font>*\n",
    "2. There are a few ways to determine the end of the generating process. One is to **stop the process** when the token `<EOS>` is selected. Another is to stop when the sequence achieves a pre-determined length.\n",
    "3. Occasionally, the **unknown word** token `<UNK>` may be selected from a softmax distribution. One can deal with this in two waysâ€“i)keep it in the sentence, ii) reject it and redraw another word.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/language model application.png\" style=\"width:45%;\" >\n",
    "</div><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a54da1-397d-4573-be78-0df3db05f582",
   "metadata": {},
   "source": [
    "<a name = \"GRU\"></a>\n",
    "### Gated Recurrent Unit\n",
    "#### Vanishing Gradients Problem\n",
    "Backpropagation of the RNN through time can be affected by the problem of vanishing gradients, which could cause the model to be slow to update. **The basic RNN has many local inferences**, meaning the output $\\hat y^{<t>}$ is mainly affected by values (e.g, $x^{<t-1>}$, $x^{<t-2>}$) close to it not the ones that are far back (e.g. $x^{<t-10>}$).\n",
    "<blockquote>\n",
    "    The <b>cats</b>, which had already ate a bowl of cat food, <b>were</b> full.\n",
    "</blockquote>\n",
    "\n",
    "Special network units have been designed to address this issue. Two popular used units are **Gated Recurrent Unit (GRU)** and **Long Short Term Memory (LSTM)**.\n",
    "\n",
    "#### Gated Recurrent Unit\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/GRU explainer.png\" style=\"width:100%;\" >\n",
    "</div><br>\n",
    "\n",
    "The gates are bounded between 0 and 1. The **reset gate** (also known as the **relevance gate**) helps determine how relevant the previous hidden state is in the creation of the candidate hidden state. The **update gate** determines the weights of the candidate hidden state and the previous hidden state in the output hidden state. If $\\Gamma_u = 0$, the previous hidden state is directly passed as the hidden state to the next time step, thereby help resolving the vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb165318-2061-43f1-b6de-8541eb6da8d3",
   "metadata": {},
   "source": [
    "<a name = \"LSTM\"></a>\n",
    "### Long Short Term Memory\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/LSTM.png\" style=\"width:100%;\" >\n",
    "</div><br>\n",
    "\n",
    "Unlike GRUs, the **Long Short Term Memory (LSTM)** units pass as hidden states both the **long-term memory** $c^{<t>}$ and the **short-term memory** $a^{<t>}$, and use three gates to govern their updating. In GRUs, $c^{<t>} = a^{<t>}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
