{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af06b28e-5b7f-4f76-9516-e9a23ee9e423",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Author: Binghen Wang\n",
    "\n",
    "Last Updated: 28 Jan, 2023\n",
    "\n",
    "<nav>\n",
    "    <b>Deep learning navigation:</b> <a href=\"./Deep Learning Basics.ipynb\">Deep Learning Basics</a> |\n",
    "    <a href=\"./Deep Learning Optimization.ipynb\">Optimization</a> |\n",
    "    <a href=\"./Convolutional Neural Networks.ipynb\">Convolutional Neural Networks</a> |\n",
    "    <a href=\"./Transformer.ipynb\">The Transformer</a>\n",
    "    <br>\n",
    "    <b>RNN navigation:</b> <a href=\"./Natural Language Processing.ipynb\">Natural Language Processing</a> \n",
    "</nav>\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0241e3-562c-45da-8f15-250bb84049b8",
   "metadata": {},
   "source": [
    "## Contents\n",
    "Basics:\n",
    "- [Examples of Sequence Models](#ESM)\n",
    "- [Standard Notation](#StandardN)\n",
    "- [Recurrent Neural Networks](#RNN)\n",
    "    - [Other Variants](#RNN-OV)\n",
    "    - [Bidirectional RNN](#RNN-BRNN)\n",
    "    - [Deep RNN](#RNN-DRNN)\n",
    "    - [Language Model](#RNN-LM)\n",
    "    - [Machine Translation](#RNN-MT)\n",
    "- [Gated Recurrent Unit (GRU)](#GRU)\n",
    "- [Long Short Term Memory (LSTM)](#LSTM)\n",
    "\n",
    "Advanced Models:\n",
    "- [Attention Mechanism](#Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac992288-1fd8-482d-9590-283532d0872a",
   "metadata": {},
   "source": [
    "## Basics\n",
    "<a name = \"ESM\"></a>\n",
    "### Examples of Sequence Models\n",
    "<table>\n",
    "    <tr>\n",
    "    <th>Task</th>\n",
    "    <th>Input</th>\n",
    "    <th>Output</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Name entity recognition</td>\n",
    "        <td>text</td>\n",
    "        <td>text with located and classified names</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>DNA sequence analysis</td>\n",
    "        <td>DNA sequence</td>\n",
    "        <td>DNA sequence with located and classified segments</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Machine translation</td>\n",
    "        <td>text in language A</td>\n",
    "        <td>text in language B</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Speech recognition</td>\n",
    "        <td>audio</td>\n",
    "        <td>text</td>\n",
    "    </tr><tr>\n",
    "        <td>Video activity recognition</td>\n",
    "        <td>video</td>\n",
    "        <td>label</td>\n",
    "    </tr><tr>\n",
    "        <td>Sentiment classification</td>\n",
    "        <td>text</td>\n",
    "        <td>label (e.g. Likert scale rating)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Music generation</td>\n",
    "        <td>N/A</td>\n",
    "        <td>audio</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>poem generation</td>\n",
    "        <td>keywords</td>\n",
    "        <td>text</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb8f3e4-3aa6-4241-af94-b91fb352e58a",
   "metadata": {},
   "source": [
    "<a name = \"StandardN\"></a>\n",
    "### Standard Notation\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "    <th>Notation</th>\n",
    "    <th>Meaning</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$T_x^{(i)}$</td>\n",
    "        <td>length of input $i$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$T_y^{(i)}$</td>\n",
    "        <td>length of output $i$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$x^{(i)<t>}$</td>\n",
    "        <td>input $i$ at temporal location $t$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$y^{(i)<t>}$</td>\n",
    "        <td>out $i$ at temporal location $t$</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa2946-43f9-44f5-8d3b-598e6500200e",
   "metadata": {},
   "source": [
    "<a name = \"RNN\"></a>\n",
    "### Recurrent Neural Networks\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/RNN.png\" style=\"width:80%;\" >\n",
    "</div>\n",
    "\n",
    "Note that the parameters are shared across time. The **loss function** used for backpropagation takes the following form:\n",
    "$$\n",
    "L = \\sum_t L(\\hat y^{<t>}, y^{<t>}) = - \\sum_t \\sum_i y_i^{<t>} \\log \\hat y_i^{<t>}\n",
    "$$\n",
    "\n",
    "<a name = \"RNN-OV\"></a>\n",
    "#### Other RNN Variants\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/RNN types.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "<a name = \"RNN-BRNN\"></a>\n",
    "#### Bidirectional RNN\n",
    "Unidirectional RNN suffers from the drawback of not taking into consideration the context that comes after a word. A bidirectional RNN helps address this.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/BRNN.png\" style=\"width:70%;\" >\n",
    "</div>\n",
    "\n",
    "<a name = \"RNN-DRNN\"></a>\n",
    "#### Deep RNN\n",
    "A deep RNN stacks together a bunch of simple RNNs vertically and would require much longer time to train.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Deep RNN.png\" style=\"width:70%;\" >\n",
    "</div>\n",
    "\n",
    "\n",
    "<a name = \"RNN-LM\"></a>\n",
    "#### Language Model\n",
    "Consider the task of training a **language model** that generates poems based on a keyword or nothing. We can train a one-to-many RNN to achieve this goal.\n",
    "\n",
    "**Data:** a large corpus of poems text $\\{{y^{<1>(1)}, \\dots, y^{<T_y^{(i)}>(i)}}\\}_{i=1}^m$<br>\n",
    "**Data preparation**: **Tokenize** the unique words of the corpus into a vocabulary/dictionary, which is then used to generate one-hot representations of words in a sentence. In the vocabulary, it would be helpful to add two extra tokens, `<EOS>` (end of sentence) and `<UNK>` (unknown). <br>\n",
    "**Loss function**: $$\n",
    "L = \\sum_t L(\\hat y^{<t>}, y^{<t>}) = - \\sum_t \\sum_j y_j^{<t>} \\log \\hat y_j^{<t>}\n",
    "$$\n",
    "**Model during training**: We pass the true $y^{<t>}$ (*not the predicted $\\hat y^{<t>}$*) as the input $x^{<t+1>}$ at time step $t+1$.\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/language model training.png\" style=\"width:50%;\" >\n",
    "</div><br>\n",
    "\n",
    "**Model during application**: There are several nuances when applying the model to generate sequences:\n",
    "1. From the second time step onwards, we **sample** a word based on the **softmax distribution** of the outcome in the previous time step $P(y^{<t-1>}| \\mathcal{I_{t-2}})$ and feed it as the input $x^{<t>}$ to the recurrent neural network. <br>*<font color = darkblue>Note: in the context of the task, we do not use word with maximum probability as the input in the subsequent time step, as doing so would yield identical sequences all the time. We want the model to generate a <b>different sequence</b> each time we use it.</font>*\n",
    "2. There are a few ways to determine the end of the generating process. One is to **stop the process** when the token `<EOS>` is selected. Another is to stop when the sequence achieves a pre-determined length.\n",
    "3. Occasionally, the **unknown word** token `<UNK>` may be selected from a softmax distribution. One can deal with this in two waysâ€“i)keep it in the sentence, ii) reject it and redraw another word.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/language model application.png\" style=\"width:45%;\" >\n",
    "</div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91abeca-8397-469d-babc-591ea0f0ccf1",
   "metadata": {},
   "source": [
    "<a name = \"RNN-MT\"></a>\n",
    "#### Machine Translation\n",
    "\n",
    "**Machine translation** can be viewed as training a conditional language model. With an encoder-decoder structure, the output is generated with the aim to find\n",
    "$$\n",
    "\\underset{y^{<1>}, y^{<2>}, \\dots, y^{<T_y>}}{\\arg\\max} P(y^{<1>}, y^{<2>}, \\dots, y^{<T_y>}\\vert x)\n",
    "$$\n",
    "where x is the input (e.g., audio clip, French sentence.)\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/machine translation.png\" style=\"width:80%;\" >\n",
    "</div><br>\n",
    "\n",
    "Enumerating all the possibilities is inpractical given a large sized vocabulary and a target output sentence of a modest length. Instead, we adopt a search strategy to approximate the best solution. \n",
    "\n",
    "##### Greedy Search Algorithm\n",
    "<blockquote>\n",
    "    In the decoder stage, pick $y^{<1>} := \\arg\\max_{y^{<1>}} P(y^{<1>}\\vert x)$ and set $t=1$. <br>\n",
    "    Repeat until $y^{<t>}=\\text{<EOS>}$:\n",
    "    <blockquote>\n",
    "        $\n",
    "        t := t+1\n",
    "        $<br>\n",
    "        $\n",
    "        y^{<t>} := \\arg\\max_{y^{<t>}} P(y^{<t>}\\vert x, y^{<1>}, \\dots, y^{<t-1>})\n",
    "        $\n",
    "    </blockquote>\n",
    "</blockquote>\n",
    "\n",
    "<div class =\"alert alert-block alert-danger\">\n",
    "    <b>Problem with the greedy search algorithm:</b> It may miss a better solution (which gives a higher overall probability) by focusing on only those that perform the best in the initial few words of the sentence.\n",
    "</div>\n",
    "\n",
    "##### Beam Search Algorithm\n",
    "Introduce $B$ as a hyperparameter of the beam search algorithm that governs the number of candidates to keep in the memory. (e.g. $B=3$ keeps 3 best candidates in the memory during the search). In the decoder stage,\n",
    "<blockquote>\n",
    "    <b>Step 1:</b> calculate the probabilities of $P(y^{<1>}\\vert x)$ for all words $y^{<1>} \\in \\mathcal{V}$, store the $B$ words that yield the highest probabilities $\\{y^{<1>}_{(1)}, \\dots, y^{<1>}_{(B)}\\}$ and their corresponding probabilities. Denote the word set as $\\mathcal{M}^{<1>}$. <br><br>\n",
    "    <b>Step 2:</b> For each element $y^{<1>}_{(b)} \\in \\mathcal{M}^{<1>}$, enumerate the probabilities $P(y^{<2>}\\vert x, y^{<1>}_{(b)})$ for all words $y^{<2>} \\in \\mathcal{V}$. Calculate the joint probabilities $$ P(y^{<1>}_{(b)}, y^{<2>} \\vert x) = P(y^{<2>} \\vert x, y^{<1>}_{(b)})  \\underbrace{P(y^{<1>}_{(b)} \\vert x)}_{\\text{previously stored}}.$$ Rank the $B \\times \\vert V\\vert$ joint probabilities across all $B$ words. Store the $B$ combinations that yield the highest probabilities $\\{\\{y^{<1>}_{(1)},y^{<2>}_{(1)}\\}, \\dots, \\{y^{<1>}_{(B)},y^{<2>}_{(B)}\\}\\}$ and their corresponding probabilities. Denote the set of combinations as $\\mathcal{M}^{<2>}$.<br><br>\n",
    "    ...\n",
    "    <br><br>\n",
    "    <b>Step t:</b> Repeat until a stopping criterion is met (e.g., the top combination (with the highest joint probability) ends in $\\text{<EOS>}$):\n",
    "    <blockquote>\n",
    "        For each element $c^{<t-1>}_{(b)} \\in \\mathcal{M}^{<t-1>}$, enumerate the probabilities $P(y^{<t>}\\vert x, c^{<t-1>}_{(b)})$ for all words $y^{<t>} \\in \\mathcal{V}$. Calculate the joint probabilities $$ P(c^{<t-1>}_{(b)}, y^{<t>} \\vert x) = P(y^{<t>} \\vert x, c^{<t-1>}_{(b)})  \\underbrace{P(c^{<t-1>}_{(b)} \\vert x)}_{\\text{previously stored}}.$$ Rank the $B \\times \\vert V\\vert$ joint probabilities across all $B$ elements. Store the $B$ combinations that yield the highest probabilities $\\{\\{y^{<1>}_{(1)},\\dots,y^{<t>}_{(1)}\\}, \\dots, \\{y^{<1>}_{(B)},\\dots,y^{<t>}_{(B)}\\}\\}$ and their corresponding probabilities. Denote the set of combinations as $\\mathcal{M}^{<t>}$.\n",
    "    </blockquote>\n",
    "</blockquote>\n",
    "\n",
    "<div class =\"alert alert-block alert-danger\">\n",
    "    <b>Problem with the vanila beam search algorithm:</b> As a result of the cost function, it might undesirably <b>favors shorter sentences</b> as outputs. \n",
    "    $$\\arg\\max_y \\Pi_{t=1}^{T_y} P(y^{<t>}\\vert x, y^{<1>}, \\dots, y^{<t-1>})$$\n",
    "    (Note: Longer sentences would involve the multiplication of more conditional probabilities less than one.)\n",
    "</div>\n",
    "\n",
    "<div class =\"alert alert-block alert-success\">\n",
    "    <b>Length normalization:</b> To refine the beam search algorithm, one could use logarithm of the loss function (to avoid numerical underflow) and employ <b>length normalization</b> to balance between short and long sentences.\n",
    "$$\n",
    "\\arg\\max_y \\frac{1}{T_y^{\\alpha}}\\sum_{t=1}^{T_y} \\log P(y^{<t>}\\vert x, y^{<1>}, \\dots, y^{<t-1>})\n",
    "$$\n",
    "where $\\alpha$ is a hyperparameter that governs the tolerance of long sentences. ($\\alpha = 0$ is no normalization; $\\alpha = 1$ is full normalization and $\\alpha = 0.7$ is recommended.) <br><br>\n",
    "Also note that to use length normalization, one needs a slightly different search algorithm from before. Specifically, in Step $t$ consider reaching a fixed length as the stopping criterion, then run the search algorithm several times for different fixed lengths and finally compute the costs across different lengths to find the best.\n",
    "</div>\n",
    "\n",
    "##### Evaluating a Machine Translation System\n",
    "How do we evaluate the output of a machine translation, noting that a sentence could have multiple equally good translations in another language? One popular way is to use the **Bleu (Bilingual evaluation understudy) score**: <a href = \"https://aclanthology.org/P02-1040.pdf\">link</a>.\n",
    "\n",
    "##### Problem with the Encoder-Decoder Structure\n",
    "The encoder-decoder structure connected via a single hidden state is **not good at processing very long sentences**. The intuition is that the entire sentence of the input language needs to be processed before generating the output sentence. Yet, there could be more efficient ways to performance translation via clever use of <a href= \"#Attention\">**attention**</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a54da1-397d-4573-be78-0df3db05f582",
   "metadata": {},
   "source": [
    "<a name = \"GRU\"></a>\n",
    "### Gated Recurrent Unit\n",
    "#### Vanishing Gradients Problem\n",
    "Backpropagation of the RNN through time can be affected by the problem of vanishing gradients, which could cause the model to be slow to update. **The basic RNN has many local inferences**, meaning the output $\\hat y^{<t>}$ is mainly affected by values (e.g, $x^{<t-1>}$, $x^{<t-2>}$) close to it not the ones that are far back (e.g. $x^{<t-10>}$).\n",
    "<blockquote>\n",
    "    The <b>cats</b>, which had already ate a bowl of cat food, <b>were</b> full.\n",
    "</blockquote>\n",
    "\n",
    "Special network units have been designed to address this issue. Two popular used units are **Gated Recurrent Unit (GRU)** and **Long Short Term Memory (LSTM)**.\n",
    "\n",
    "#### Gated Recurrent Unit\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/GRU explainer.png\" style=\"width:100%;\" >\n",
    "</div><br>\n",
    "\n",
    "The gates are bounded between 0 and 1. The **reset gate** (also known as the **relevance gate**) helps determine how relevant the previous hidden state is in the creation of the candidate hidden state. The **update gate** determines the weights of the candidate hidden state and the previous hidden state in the output hidden state. If $\\Gamma_u = 0$, the previous hidden state is directly passed as the hidden state to the next time step, thereby help resolving the vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb165318-2061-43f1-b6de-8541eb6da8d3",
   "metadata": {},
   "source": [
    "<a name = \"LSTM\"></a>\n",
    "### Long Short Term Memory\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/LSTM.png\" style=\"width:100%;\" >\n",
    "</div><br>\n",
    "\n",
    "Unlike GRUs, the **Long Short Term Memory (LSTM)** units pass as hidden states both the **long-term memory** $c^{<t>}$ and the **short-term memory** $a^{<t>}$, and use three gates to govern their updating. In GRUs, $c^{<t>} = a^{<t>}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a8566-268f-461e-960b-bbfa4c52852e",
   "metadata": {},
   "source": [
    "## Advanced Models\n",
    "<a name = \"Attention\"></a>\n",
    "### Attention Mechanism\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/attention.png\" style=\"width:100%;\" >\n",
    "</div><br>\n",
    "\n",
    "In addition to its use in machine translation, the attention mechanism has also been adapted to the task of **image captioning** by applying an attention mechanism to the visual input.\n",
    "\n",
    "<div class =\"alert alert-block alert-warning\">\n",
    "    <b>Drawback with the attention model:</b> The computation of the attention weights is <b>quadratic</b> in computation cost, $T_x \\times T_y$. This mandates the use of relatively simple neural network structure for the computation of $e^{<t,t^{\\prime}>}$. Arguably, provided the lengths of the sentences for the translation task are not that long, the quadratic cost seems acceptable. Further research is being conducted to reduce the computation cost.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
