{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0fadb2d-c9bb-48ee-b6ca-57cdc12f7688",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Basics\n",
    "\n",
    "Author: Bingchen Wang\n",
    "\n",
    "Last Updated: 1 Nov, 2022\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996f4ac-0e3c-4a77-b2ee-6476641efa1e",
   "metadata": {},
   "source": [
    "## Contents\n",
    "Section 1: Getting started:\n",
    "- [Motivating example: Logistic regression](#LR)\n",
    "- [Standard notation](#SN)\n",
    "- [Activation functions](#AF)\n",
    "- [Cost functions](#CF)\n",
    "\n",
    "Section 2: Backpropagation:\n",
    "- [Backpropagation](#Backprop)\n",
    "- [Gradient Checking](#GC)\n",
    "\n",
    "Section 3: Practical aspects:\n",
    "- [Initialization](#Init)\n",
    "- [Regularization](#Reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c4c0e-306e-4f23-903e-a68ef85208ce",
   "metadata": {},
   "source": [
    "# Section 1: Getting Started\n",
    "<a name = \"LR\"></a>\n",
    "## Logistic regression\n",
    "### Logistic model\n",
    "**Data**: $\\{x^{(i)}, y^{(i)}\\}_{i=1}^m$. <br>\n",
    "**Parameters**: $w$, $b$. <br>\n",
    "**Hypothesis/model fit**:\n",
    "$$\n",
    "h(x^{(i)}) = \\sigma(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "z^{(i)} = w^T x^{(i)} + b.\n",
    "$$\n",
    "\n",
    "### Logistic loss\n",
    "$$\n",
    "L(h(x^{(i)}), y^{(i)}) = - y^{(i)}\\log\\left(h(x^{(i)})\\right) - (1 -  y^{(i)}) \\log\\left(1-h(x^{(i)})\\right)\n",
    "$$\n",
    "\n",
    "### Logistic cost\n",
    "$$\n",
    "J(w,b) = - \\frac{1}{m} \\sum_{i=1}^{m}\\left[y^{(i)}\\log\\left(h(x^{(i)})\\right) + (1 -  y^{(i)}) \\log\\left(1-h(x^{(i)})\\right)\\right]\n",
    "$$\n",
    "<br>\n",
    "Objective:\n",
    "$$\n",
    "    \\min_{w,b} J(w,b) \\;\\;\\;(+ \\;\\;\\;\\;\\text{regularization})\n",
    "$$\n",
    "\n",
    "#### Why using the logistic cost?\n",
    "1. Connection to **maximum likelihood estimation**.\n",
    "2. Using a matching cost function can facilitate **optimization**. For example, compare the logistic cost function with the least square cost function, both used to train the logistic model using a 20-example data set. Below is a visualization of the cost functions for different combinations of $w$ and $b$:\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Logistic cost vs least squares cost.png\" style=\"width:80%;\" >\n",
    "    <br>\n",
    "    Detailed implementation: <a href = \"./Deep Learning simulations/Logistic cost function.ipynb\">here</a>.\n",
    "</div>\n",
    "\n",
    "### Connection to neural networks\n",
    "Logistic regression models can be viewed as a special case of neural networks, in which there is only one output layer with a single neuron.\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Logistic Regression (NN).jpg\" style=\"width:40%;\" >\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4c223-bbe3-4f0f-b473-27345b5dd5b3",
   "metadata": {},
   "source": [
    "<a name = \"SN\"></a>\n",
    "## Standard notation\n",
    "<table>\n",
    "    <tr>\n",
    "        <th colspan = \"2\" style=\"font-size:16px\"> Sizes </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$m$$ </td>\n",
    "        <td> number of examples in the dataset </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$n_x$$ </td>\n",
    "        <td> input size </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$n_y$$ </td>\n",
    "        <td> output size (= number of output classes $K$) </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$n_h^{[l]}$$ </td>\n",
    "        <td> number of hidden units/neurons of the $l$th layer </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$L$$ </td>\n",
    "        <td> number of layers in the neural network (hidden + output)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th colspan = \"3\" style=\"font-size:16px\"> Observations </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> Object</th>\n",
    "        <th> Dimension</th>\n",
    "        <th> Meaning</th>\n",
    "    <tr>\n",
    "        <td> $$X$$ </td>\n",
    "        <td> $$\\mathbb{R}^{n_x \\times m}$$ </td>\n",
    "        <td> input matrix </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$Z^{[l]}$$ </td>\n",
    "        <td> $$\\mathbb{R}^{n_h^{[l]} \\times m}$$ </td>\n",
    "        <td> matrix of weighted sums terms for the $l$th layer </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$A^{[l]}$$ </td>\n",
    "        <td> $$\\mathbb{R}^{n_h^{[l]} \\times m}$$ </td>\n",
    "        <td> output matrix of the $l$th layer </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$Y$$ </td>\n",
    "        <td> $$\\mathbb{R}^{n_y \\times m}$$ </td>\n",
    "        <td> label matrix </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$\\widehat Y$$ </td>\n",
    "        <td> $$\\mathbb{R}^{n_y \\times m}$$ </td>\n",
    "        <td> predicted label matrix ($A^{[L]}$) </td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td> $$ x^{(i)}_{j}$$ </td>\n",
    "        <td> $$\\mathbb{R}$$ </td>\n",
    "        <td> $j$th feature of the $i$th example </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$z^{[l](i)}_{j}$$ </td>\n",
    "        <td> $$\\mathbb{R}$$ </td>\n",
    "        <td> weighted sum of the $j$ hidden unit in the $l$th layer for the $i$th example  </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$a^{[l](i)}_{j}$$ </td>\n",
    "        <td> $$\\mathbb{R}$$ </td>\n",
    "        <td> output of the $j$ hidden unit in the $l$th layer for the $i$th example  </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$ y^{(i)}$$ </td>\n",
    "        <td> $$\\mathbb{R}^{n_y}$$ </td>\n",
    "        <td> label of the $i$th example </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$ \\hat y^{(i)}$$ </td>\n",
    "        <td> $$\\mathbb{R}^{n_y}$$ </td>\n",
    "        <td> predicted label for the $i$th example </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th colspan = \"3\" style=\"font-size:16px\"> Parameters </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> Object</th>\n",
    "        <th> Dimension</th>\n",
    "        <th> Meaning</th>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> $$ W^{[l]}$$ </td>\n",
    "        <td> $$ \\mathbb{R}^{n_h^{[l]} \\times n_h^{[l-1]}}$$ </td>\n",
    "        <td> weight matrix in the $l$th layer </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$ b^{[l]}$$ </td>\n",
    "        <td> $$ \\mathbb{R}^{n_h^{[l]}}$$ </td>\n",
    "        <td> bias vector in the $l$th layer </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$ g^{[l]}$$ </td>\n",
    "        <td> $$ g: S \\rightarrow S,$$ for some space $S$ </td>\n",
    "        <td> activation function(s) in the $l$th layer </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### Relations between the objects\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A^{[0]} =& X \\\\\n",
    "Z^{[l]} =& W^{[l]}A^{[l]} + b^{[l]} \\\\\n",
    "A^{[l]} =& g^{[l]}(Z^{[l]}) \\\\\n",
    "\\widehat Y =& A^{[L]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### A simple feedforward neural network\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Standard notation.jpg\" style=\"width:70%;\" >\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a72505-17e2-41ec-8b99-34b26b4505e6",
   "metadata": {},
   "source": [
    "<a name = \"AF\"></a>\n",
    "## Activation functions\n",
    "The **sigmoid**/logistic function $a(z) = \\frac{1}{1+ e^{-z}}$ is but one of the activation functions for neural networks. Other commonly used activation functions include:\n",
    "- **linear**: $a(z) = z$\n",
    "- **tanh**: $a(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "- **Rectified Linear Unit (ReLU)**: $a(z) = \\max(0, z)$ (most popular choice for hidden layers)\n",
    "- **Leaky ReLU**: $a(z) = \\max(0.01z,z)$\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/activation functions.png\" style=\"width:70%;\" >\n",
    "    <br>\n",
    "</div>\n",
    "\n",
    "- **softmax**: used as the activation function for the output layer for multi-class classification.\n",
    "\n",
    "$$a_i(z) = \\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}}$$\n",
    "\n",
    "Note that each neuron in the output layer (with softmax as the activation function) depends on the other neurons as well.\n",
    "\n",
    "### Derivatives/Jacobians\n",
    "**sigmoid**: \n",
    "$$\n",
    "a^\\prime(z) = \\frac{e^{-z}}{(1+e^{-z})^2} = a(z) \\left(1-a(z)\\right)\n",
    "$$\n",
    "**linear**:\n",
    "$$\n",
    "a^\\prime(z) = 1\n",
    "$$\n",
    "**tanh**:\n",
    "$$\n",
    "a^\\prime(z) = \\frac{{(e^z+e^{-z})}^2 - {(e^z-e^{-z})}^2}{{(e^z+e^{-z})}^2} = 1 - a^2(z)\n",
    "$$\n",
    "**ReLU**:\n",
    "$$\n",
    "a^\\prime(z) = \\left\\{ \\begin{array}{cc} 1 & z > 0 \\\\ 0 & z < 0 \\end{array} \\right.\n",
    "$$\n",
    "**Leaky ReLU**:\n",
    "$$\n",
    "a^\\prime(z) = \\left\\{ \\begin{array}{cc} 1 & z > 0 \\\\ 0.01 & z < 0 \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "**softmax**: $a$ and $z$ are $K \\times 1$.\n",
    "$$\n",
    "J = \\left[\\begin{array}{cccc} \\frac{\\partial a}{\\partial z_1} & \\frac{\\partial a}{\\partial z_2} & \\cdots & \\frac{\\partial a}{\\partial z_K} \\end{array}\\right] \n",
    "= \\left[\\begin{array}{c} \\nabla_z a_1 \\\\ \\nabla_z a_2 \\\\ \\vdots \\\\ \\nabla_z a_K \\end{array}\\right] \n",
    "= \\left[\\begin{array}{cccc} \n",
    "\\frac{\\partial a_1}{\\partial z_1} & \\frac{\\partial a_1}{\\partial z_2}& \\cdots& \\frac{\\partial a_1}{\\partial z_K}\\\\\n",
    "\\frac{\\partial a_2}{\\partial z_1} & \\frac{\\partial a_2}{\\partial z_2}& \\cdots& \\frac{\\partial a_2}{\\partial z_K}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial a_K}{\\partial z_1} & \\frac{\\partial a_K}{\\partial z_2}& \\cdots& \\frac{\\partial a_K}{\\partial z_K}\\\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial a_i}{\\partial z_j} \n",
    "=& \\left\\{\\begin{array}{cc} \n",
    "\\frac{e^{z_i}\\sum_{j=1}^{K}e^{z_j} - {(e^{z_i})}^2}{{(\\sum_{j=1}^{K}e^{z_j})}^2} & \\text{if } i = j \\\\\n",
    "- \\frac{e^{z_i}e^{z_j}}{{(\\sum_{j=1}^{K}e^{z_j})}^2} & \\text{if } i \\neq j\n",
    "\\end{array}\\right. \\\\\n",
    "=& \\left\\{\\begin{array}{cc} \n",
    "a_i (1-a_i)  & \\text{if } i = j \\\\\n",
    "- a_i a_j & \\text{if } i \\neq j\n",
    "\\end{array}\\right.\n",
    "\\end{align}\n",
    "$$\n",
    "Namely,\n",
    "$$\n",
    "J = \\left[\\begin{array}{cccc} \n",
    "a_1 (1-a_1) &  - a_1 a_2 & \\cdots& - a_1 a_K\\\\\n",
    "- a_2 a_1 & a_2 (1-a_2)& \\cdots& - a_2 a_K\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "- a_K a_1 & - a_K a_2 & \\cdots& a_K (1 - a_K)\\\\\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208cf210-8381-4903-b7f7-8ef07c3df7ef",
   "metadata": {},
   "source": [
    "<a name = \"CF\"></a>\n",
    "## Cost functions\n",
    "\n",
    "### Cross-entropy cost\n",
    "$$\n",
    "J_{CE} = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K y_k^{(i)} \\log \\hat y_k^{(i)}\n",
    "$$\n",
    "#### Special case: logisitc loss ($K=2$)\n",
    "$$\n",
    "J_{logistic} = - \\frac{1}{m} \\sum_{i=1}^m \\left(y^{(i)} \\log \\hat y^{(i)} + (1-y^{(i)}) \\log (1- \\hat y^{(i)}) \\right)\n",
    "$$\n",
    "\n",
    "### Mean squared error cost\n",
    "$$\n",
    "J_{MSE} = - \\frac{1}{m} \\sum_{i=1}^m {(\\hat y^{(i)} - y^{(i)})}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867240ee-96c0-4f1b-91a2-a2e04e5f621f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Section 2: Backpropagation\n",
    "<a name = \"Backprop\"></a>\n",
    "## Backpropagation\n",
    "Wikipedia: <a href = \"https://en.wikipedia.org/wiki/Backpropagation\">here</a>. <br>\n",
    "\n",
    "<div class = \"alert alert-block alert-success\"><b>Tips:</b><br>\n",
    "1. When in doubt, check the <b>dimensions</b> of each term and make sure they are compatible. <br>\n",
    "2. If still in doubt, focus on the <b>individual elements</b> of a vector/matrix and use scalar calculus.\n",
    "</div>\n",
    "\n",
    "Write the loss function for a single observation as $\\mathcal{L}(\\hat y, y)$. <br>\n",
    "**Define**\n",
    "$$\n",
    "\\delta^{[l]} = \\nabla_{z^{[l]}} \\mathcal{L}(\\hat y , y)\n",
    "$$\n",
    "and $\\circ$ is the Hadamard product (element-wsie product).\n",
    "1. For the output layer $L$,\n",
    "    - if the loss function is binary cross entropy, and the activation function is sigmoid:\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        \\nabla_{a^{[L]}} \\mathcal{L}  =&  - \\frac{y}{a^{[L]}} + \\frac{1-y}{1-a^{[L]}} \\\\\n",
    "        \\nabla_{z^{[L]}} \\mathcal{L} =& g^{[L]\\prime}(z^{[L]}) \\circ \\nabla_{a^{[L]}} \\mathcal{L} \\\\\n",
    "        =& a^{[L]} (1-a^{[L]})  \\cdot \\left(- \\frac{y}{a^{[L]}} + \\frac{1-y}{1-a^{[L]}}\\right) \\\\\n",
    "        =& a^{[L]} - y\n",
    "        \\end{align}\n",
    "        $$\n",
    "    - if the activation function is cross entropy:\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        \\nabla_{a^{[L]}} \\mathcal{L} =&  - \\left[\\begin{array}{c} \\frac{y_1}{a^{[L]}_1} \\\\ \\frac{y_2}{a^{[L]}_2}\\\\ \\vdots \\\\ \\frac{y_K}{a^{[L]}_K}\\end{array}\\right] \\\\\n",
    "        \\nabla_{z^{[L]}} \\mathcal{L} =&  \\underbrace{J^T}_{\\mathbb{R}^{n_y \\times n_y}} \\underbrace{\\nabla_{a^{[L]}} \\mathcal{L}}_{\\mathbb{R}^{n_y \\times 1}} \\\\\n",
    "        = & - \\left[\\begin{array}{cccc} \n",
    "a_1^{[L]} (1-a_1^{[L]}) &  - a_2^{[L]} a_1^{[L]} & \\cdots& - a_K^{[L]} a_1^{[L]}\\\\\n",
    "- a_1^{[L]} a_2^{[L]} & a_2^{[L]} (1-a_2^{[L]})& \\cdots& - a_K^{[L]} a_2^{[L]}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "- a_1^{[L]} a_K^{[L]} & - a_2^{[L]} a_K^{[L]} & \\cdots& a_K^{[L]} (1 - a_K^{[L]})\\\\\n",
    "\\end{array}\\right]\\left[\\begin{array}{c} \\frac{y_1}{a^{[L]}_1} \\\\ \\frac{y_2}{a^{[L]}_2}\\\\ \\vdots \\\\ \\frac{y_K}{a^{[L]}_K}\\end{array}\\right] \\\\\n",
    "        = & \\left[\\begin{array}{c} a^{[L]}_1 - y_1 \\\\ a^{[L]}_2 - y_2\\\\ \\vdots \\\\ a^{[L]}_K - y_K \\end{array}\\right] \\\\\n",
    "        = & a^{[L]} - y\n",
    "        \\end{align}\n",
    "        $$\n",
    "Therefore, **for both cases**, $$ \\delta^{[L]} = \\nabla_{z^{[L]}} \\mathcal{L} = a^{[L]} - y.$$\n",
    "2. For layers $l = 1, \\dots, L-1$,\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\nabla_{a^{[l]}} \\mathcal{L} = & \\underbrace{W^{[l+1]\\mathsf{T}}}_{n^{[l]}_h \\times n^{[l+1]}_h} \\underbrace{\\delta^{[l+1]}}_{n^{[l+1]}_h \\times 1} \\\\\n",
    "    \\nabla_{z^{[l]}} \\mathcal{L} = & g^{[l]\\prime}(z^{[l]}) \\circ \\nabla_{a^{[l]}} \\mathcal{L} \\;\\;\\; (:=\\delta^{[l]})\\\\\n",
    "    \\nabla_{W^{[l]}} \\mathcal{L} = & \\underbrace{\\delta^{[l]}}_{n^{[l]}_h \\times 1} \\underbrace{a^{[l-1]\\mathsf{T}}}_{1 \\times n^{[l-1]}_h} \\\\\n",
    "    \\nabla_{b^{[l]}} \\mathcal{L} = & \\underbrace{\\delta^{[l]}}_{n^{[l]}_h \\times 1}\n",
    "    \\end{align}\n",
    "    $$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84369e14-ac04-4074-a871-9dc33f59c902",
   "metadata": {},
   "source": [
    "<a name = \"GC\"></a>\n",
    "## Gradient Checking\n",
    "To check if backpropagation is running correctly, we can use gradient checking. It is motivated by the definition of derivatives:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta} = \\lim_{\\epsilon \\rightarrow 0}\\frac{J(\\theta+\\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon}\n",
    "$$\n",
    "When $\\epsilon$ is small (say, `epsilon = 1e^-7`), ${\\left[\\frac{\\partial J}{\\partial \\theta}\\right]}_{approx} = \\frac{J(\\theta+\\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon}$ should be very close to $\\frac{\\partial J}{\\partial \\theta}$.\n",
    "\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Gradient Checking Algorithm</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        Write the parameters in a model (e.g., neural network) as $\\theta = {\\left[\\begin{array}{cccc} \\theta_1, \\theta_2, \\dots, \\theta_P \\end{array}\\right]}^{\\mathsf{T}}$, where $P$ is the total number of parameters. <br>\n",
    "        Specify a small $\\epsilon$ (say $10^{-7}$). <br>\n",
    "        For each $p = 1, \\dots, P$:\n",
    "        <blockquote>\n",
    "            <ol>\n",
    "                <li> compute the approximated gradient,\n",
    "                    $$\n",
    "                    d\\theta_{p,approx} = \\frac{J(\\theta_1, \\theta_2, \\dots, \\theta_p + \\epsilon, \\dots, \\theta_P) -J(\\theta_1, \\theta_2, \\dots, \\theta_p - \\epsilon, \\dots, \\theta_P)}{2\\epsilon}\n",
    "                    $$\n",
    "                <li> compute the distance between the backprop result $d\\theta_p$ and the approximated gradient $d\\theta_{p,approx}$,\n",
    "                    $$\n",
    "                    D_p = \\frac{{\\Vert d\\theta_p - d\\theta_{p,approx} \\Vert}_2}{{\\Vert d\\theta_p \\Vert}_2 + {\\Vert d\\theta_{p,approx} \\Vert}_2}\n",
    "                    $$\n",
    "                 <li> determine if there is a possible error:\n",
    "                     <ul>\n",
    "                         <li> if $D_p \\approx 10^{-7}$, it looks fine.\n",
    "                         <li> if $D_p \\approx 10^{-3}$, there is probably something wrong.\n",
    "                     </ul>\n",
    "            </ol>\n",
    "        </blockquote>\n",
    "    </div>\n",
    "</section>\n",
    "<br>\n",
    "<div class = \"alert alert-block alert-warning\"><b>Gradient checking dos and don'ts </b>:\n",
    "<ol>\n",
    "    <li> Don't use in training and only use for debug, as computation is expensive.\n",
    "    <li> Don't work with dropout.\n",
    "    <li> Do check the components of the backprop (e.g., $db^{[l]}$ and $dW^{[l]}$) in the presence of a failed check.\n",
    "    <li> Do remember the regularization term if used in the training process, i.e. $\\tilde J = J + \\Omega$ rather than $J$.\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a75e5-d9a4-4a2d-8d57-df3b4feb648f",
   "metadata": {},
   "source": [
    "# Section 3: Practical aspects\n",
    "<a name = \"Init\"></a>\n",
    "## Initialization\n",
    "- He initialization\n",
    "- Xavier initialization\n",
    "- Xavier initialization (Version 2)\n",
    "\n",
    "<br>\n",
    "To use optimization methods such as gradient descent, we need to initialize the parameters in the neural networks. A good initialization is important, as it can:\n",
    "1. **speed up the convergence** of the optimization algorithm, e.g. gradient descent.\n",
    "2. increase the possibility that the optimization algorithm, e.g. gradient descent, converges to a **lower training error** (and generalization error).\n",
    "3. partially solve the vanishing/exploding gradients problem. For another commonly used technique, see *gradient clipping*.\n",
    "\n",
    "### He initialization\n",
    "$$\n",
    "W^{[l]}_{ij} = \\mathcal{N}\\left(0, \\frac{2}{n^{[l-1]}_h}\\right) \n",
    "$$\n",
    "<br>\n",
    "**Ideal activation functions**: ReLU <br>\n",
    "**Implementation**: `W = np.random.randn(size_current, size_prev)*np.sqrt(2/size_prev)`\n",
    "\n",
    "### Xavier initialization\n",
    "$$\n",
    "W^{[l]}_{ij} = \\mathcal{N}\\left(0, \\frac{1}{n^{[l-1]}_h}\\right) \n",
    "$$\n",
    "<br>\n",
    "**Ideal activation functions**: tanh <br>\n",
    "**Implementation**: `W = np.random.randn(size_current, size_prev)*np.sqrt(1/size_prev)`\n",
    "\n",
    "### Xavier initialization (Version 2)\n",
    "$$\n",
    "W^{[l]}_{ij} = \\mathcal{N}\\left(0, \\frac{2}{n^{[l-1]}_h + n^{[l]}_h}\\right) \n",
    "$$\n",
    "<br>\n",
    "**Interpretation of the variance**: the harmonic mean of $n^{[l-1]}_h$ and $n^{[l]}_h$. <br>\n",
    "**Implementation**: `W = np.random.randn(size_current, size_prev)*np.sqrt(2/(size_prev+size_current))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c47274-53f3-4f2b-97a4-db8227681e5d",
   "metadata": {},
   "source": [
    "<a name = \"Reg\"></a>\n",
    "## Regularization\n",
    "- $L_q$ penalties\n",
    "- Dropout regularization\n",
    "- Data augmentation\n",
    "- Early stopping\n",
    "\n",
    "<br>\n",
    "Regularization is an important technique often used in the training of neural networks to prevent overfitting. Here we focus on a few approaches.\n",
    "\n",
    "### $L_q$ penalties\n",
    "$$\n",
    "\\min_{w,b} J(W,b) = \\frac{1}{m} \\sum_{i}^m L(\\hat y^{(i)}, y^{(i)}) + \\underbrace{\\lambda}_{\\text{regularization parameter}} \\Omega(W)\n",
    "$$\n",
    "Commonly used regularizations include:\n",
    "- **$L_2$ regularization**: $$\\Omega(W) = \\frac{1}{2}\\Vert W \\Vert_2^2 = \\frac{1}{2} \\sum_l \\sum_i \\sum_j {(W_{ij}^{[l]})}^2$$\n",
    "    - Weight decay (consider gradient descent with regularization): $$W^{[l]} := \\left(1-\\alpha \\lambda\\right)W^{[l]} - \\alpha \\frac{\\partial J}{\\partial W^{[l]}}$$\n",
    "- **$L_1$ regularization**: $$\\Omega(W) = \\Vert W \\Vert_1  = \\sum_l \\sum_i \\sum_j \\vert W_{ij}^{[l]} \\vert$$\n",
    "    - $W$ will be sparse.\n",
    "    \n",
    "#### $L_q$ penalties in linear regression\n",
    "In the context of linear regression, many more forms of regularization terms have been studied. Consider the general criterion:\n",
    "$$\n",
    " \\min_\\beta \\left\\{\\sum_{i=1}^m \\left(y^{(i)} - x^{(i)\\mathsf{T}}\\beta\\right) + \\lambda \\sum_{j=1}^p \\vert \\beta_j \\vert^q \\right\\}\n",
    "$$\n",
    "- $q = 0$ amounts to variable subset selection.\n",
    "- $q = 1$ corresponds to the lasso.\n",
    "- $q = 2$ corresponds to ridge regression.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/L_q regularization.png\" style=\"width:100%;\" >\n",
    "    <br>\n",
    "</div>\n",
    "\n",
    "<blockquote> \n",
    "    Values of $q \\in (1,2)$ suggests a compromise between the lasso and ridge regression. Although this is the case with $q >1$, $\\vert \\beta_j \\vert^q$ is <b>differntiable</b> at $0$, and so <b>does not share the ability of the lasso ($q = 1$) for setting coefficients exactly to zero.</b> Partly for this reason as well as for computational tractability, Zou and Hastie (2005) introduced the <b>elastic net</b> penalty. -- The Elements of Statistical Learning, p.73.  \n",
    "</blockquote>\n",
    "\n",
    "The elastic net penalty:\n",
    "$$\n",
    "\\lambda \\sum_{j=1}^p \\left(\\alpha \\beta_j^2 + (1-\\alpha) \\vert \\beta_j \\vert \\right)\n",
    "$$\n",
    "where $\\alpha \\in [0,1]$ governs how close the penalty is to $L_2$.\n",
    "\n",
    "The elastic-net selects variables (by setting some to zero) like the lasso, and shrinks together the coefficients of correlated predictors like ridge, while also having considerable computational advantage  over the $L_q$ penalties.\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Elastic nets.png\" style=\"width:90%;\" >\n",
    "    <br>\n",
    "    <img src=\"./images/Elastic nets vs L_q.png\" style=\"width:30%;\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba2636-ab42-488f-ab04-4a1882c52fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
