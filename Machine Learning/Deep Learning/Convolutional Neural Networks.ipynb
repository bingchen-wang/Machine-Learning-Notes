{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf83c2f0-f96d-40e0-969d-c3a991698c0c",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "Author: Binghen Wang\n",
    "\n",
    "Last Updated: 6 Dec, 2022\n",
    "\n",
    "<nav>\n",
    "    <b>Deep learning navigation:</b> <a href=\"./Deep Learning Basics.ipynb\">Deep Learning Basics</a> |\n",
    "    <a href=\"./Deep Learning Optimization.ipynb\">Optimization</a>\n",
    "    <br>\n",
    "    <b>CNN navigation:</b> <a href=\"./Object Detection.ipynb\">Object Detection</a> |\n",
    "    <a href=\"./Face Recognition.ipynb\">Face Recognition</a> |\n",
    "    <a href=\"./Visualization and Neural Style Transfer.ipynb\">Visualization and Neural Style Transfer</a>\n",
    "</nav>\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc8998-18b6-4358-b2ab-850a6161fdd4",
   "metadata": {},
   "source": [
    "## Contents\n",
    "Basics:\n",
    "- [Standard Notation](#StN)\n",
    "- [Convolutional Layer](#CL)\n",
    "- [Pooling Layer](#PL)\n",
    "- [$1\\times1$ Convolutional Layer](#1b1)\n",
    "\n",
    "Classic Models:\n",
    "- [LeNet-5](#LeN)\n",
    "- [AlexNet](#AlexN)\n",
    "- [VGG-16](#VGG16)\n",
    "\n",
    "Advanced Models:\n",
    "- [ResNets](#ResN)\n",
    "- [Inception Network](#IncN)\n",
    "- [MobileNet](#MobN)\n",
    "- [EfficientNet](#EffN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48442737-10ae-4886-aa83-9c9438610781",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "<a name ='StN'></a>\n",
    "### Standard Notation (for a layer of CNN)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th colspan = \"2\" style=\"font-size:16px\"> Shape parameters </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> Object</th>\n",
    "        <th> Meaning</th>\n",
    "    <tr>\n",
    "        <td> $$f^{[l]}$$ </td>\n",
    "        <td> filter size </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$p^{[l]}$$ </td>\n",
    "        <td> padding </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$s^{[l]}$$ </td>\n",
    "        <td> stride </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$n^{[l]}_C$$ </td>\n",
    "        <td> number of channels </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$n^{[l]}_H$$ </td>\n",
    "        <td> height of 2D layer </td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td> $$n^{[l]}_W$$ </td>\n",
    "        <td> width of 2D layer </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th colspan = \"3\" style=\"font-size:16px\"> Model parameters and values </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th> Object</th>\n",
    "        <th> Shape </th>\n",
    "        <th> Meaning</th>\n",
    "    <tr>\n",
    "        <td> N/A </td>\n",
    "        <td> $$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_C$$ </td>\n",
    "        <td> one filter/kernel </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$a^{[l-1]}$$ </td>\n",
    "        <td> $$n^{[l-1]}_H \\times n^{[l-1]}_W \\times n^{[l-1]}_C$$ </td>\n",
    "        <td> input to layer $l$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$a^{[l]}$$ </td>\n",
    "        <td> $$n^{[l]}_H \\times n^{[l]}_W \\times n^{[l]}_C$$ </td>\n",
    "        <td> output of layer $l$ / activations of layer $l$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$A^{[l-1]}$$ </td>\n",
    "        <td> $$m \\times n^{[l-1]}_H \\times n^{[l-1]}_W \\times n^{[l-1]}_C$$  </td>\n",
    "        <td> vectorized input to layer $l$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$A^{[l]}$$ </td>\n",
    "        <td> $$m \\times n^{[l]}_H \\times n^{[l]}_W \\times n^{[l]}_C$$  </td>\n",
    "        <td> vectorized output of layer $l$ </td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td> $$W^{[l]}$$ </td>\n",
    "        <td> $$f^{[l]} \\times f^{[l]} \\times n^{[l-1]}_C \\times n^{[l]}_C$$ </td>\n",
    "        <td> weight matrix of layer $l$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> $$b^{[l]}$$ </td>\n",
    "        <td> $$1 \\times 1 \\times 1 \\times n^{[l]}_C$$ </td>\n",
    "        <td> bias of layer $l$ </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "#### Relationship between the terms\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "n^{[l]}_H = \\left\\lfloor \\frac{n^{[l-1]}_H + 2p^{[l]} - f}{s^{[l]}} + 1 \\right\\rfloor \\\\\n",
    "n^{[l]}_W = \\left\\lfloor \\frac{n^{[l-1]}_W + 2p^{[l]} - f}{s^{[l]}} + 1 \\right\\rfloor\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "z^{[l]} = a^{[l-1]} *_{\\text{conv}} W^{[l]} + b^{[l]} \\\\\n",
    "a^{[l]} = g^{[l]}(z^{[l]})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "<a name ='CL'></a>\n",
    "### Convolutional Layer\n",
    "\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/a CNN layer.png\" style=\"width:80%;\" >\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Padding\n",
    "\n",
    "**Problems/properties without padding**:\n",
    "- Shrinking output\n",
    "- Counting less the information from the edges of the image\n",
    "\n",
    "**Valid convolution (no padding)**:\n",
    "\n",
    "$$\n",
    "h \\times w * f \\times f \\rightarrow (h-f+1) \\times (w-f+1)\n",
    "$$\n",
    "\n",
    "**Same convolution (output-size-same-as-input-size padding)**:\n",
    "\n",
    "Set\n",
    "$$\n",
    "\\begin{equation}\n",
    " h + 2p - f +  1 = h \\\\\n",
    " w + 2p - f +  1 = w \n",
    "\\end{equation}\n",
    "$$\n",
    "and solve for $p$. We get:\n",
    "$$\n",
    "p = \\frac{f-1}{2}\n",
    "$$\n",
    "where $f$ is usually odd (to allow for symmetric padding).\n",
    "\n",
    "\n",
    "#### Stride\n",
    "Denote the stride as $s$. Then the output shape of a strided convolution is:\n",
    "$$\n",
    "\\left\\lfloor \\frac{h+2p-f}{s}\\right\\rfloor + 1 \\;\\;\\; \\times \\;\\;\\; \\left\\lfloor \\frac{w+2p-f}{s}\\right\\rfloor + 1\n",
    "$$\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/padding and strided convolution.jpg\" style=\"width:80%;\" >\n",
    "</div>\n",
    "\n",
    "<a name ='PL'></a>\n",
    "### Pooling Layer\n",
    "\n",
    "**Hyperparameters**:\n",
    "- $f$: filter size\n",
    "- $s$: stride\n",
    "- type of pooling: max or average\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/pooling layers.png\" style=\"width:80%;\" >\n",
    "</div>\n",
    "\n",
    "<a name='1b1'></a>\n",
    "### $1 \\times 1$ Convolutions (Networks in Networks)\n",
    "- Provides a way to adjust the size of the channels $n_C$ (increase, decrease, keep it the same)\n",
    "- Can be used as a 'bottleneck' layer to save on computation (for inception network).\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/1x1 conv.png\" style=\"width:80%;\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c9e21a-7b15-4724-b5dd-c726d6217cee",
   "metadata": {},
   "source": [
    "## Classic Models\n",
    "\n",
    "<a name ='LeN'></a>\n",
    "### LeNet-5\n",
    "**Note:** \n",
    "- To make sure smaller images match the input dimensions, padding could be used.\n",
    "- It is usually the convention to call the combination of a convolutional layer and a subsampling layer (e.g., max pooling) a **layer** in CNN.\n",
    "- The original input in the LeNet-5 has only **one channel** (grayscale), so the shape is $32\\times32\\times1$. Here we use an RGB input (with three channels) for illustration purposes.\n",
    "- The third layer in the visualization uses a convolution layer with 120 filters of size $5\\times5\\times16$, resulting in activations of size $1\\times1\\times120$. This is then flattened. An **alternative** equivalent way is to first flatten the $5\\times5\\times16$ layer input into an input of size $400$ and then use a fully connected layer with $120$ units.\n",
    "- In the original paper, the activation functions for the hidden layers were chosen to be sigmoid/tanh, but nowadays ReLU usually works better.\n",
    "**Key Patterns:**\n",
    "- Around 60k parameters.\n",
    "- Going from left to right, $n_H$, $n_W \\downarrow$ and $n_C \\uparrow$\n",
    "- conv + pool + conv + pool + conv/fc + fc + fc/output\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/LeNet5.png\" style=\"width:100%;\" >\n",
    "</div>\n",
    "\n",
    "**Key Sections of the original paper**: **II**, III\n",
    "\n",
    "<a name ='AlexN'></a>\n",
    "### AlexNet\n",
    "**Key Pattern:**\n",
    "- Around 60m parameters.\n",
    "- Use of ReLU activation functions.\n",
    "- Trained using ImageNet.\n",
    "- A lot of hyperparameters. (Different choices of filter sizes, padding options, strides etc.)\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/AlexNet.png\" style=\"width:100%;\" >\n",
    "</div>\n",
    "\n",
    "<a name ='VGG16'></a>\n",
    "### VGG-16\n",
    "**Key Pattern:**\n",
    "- Around 138m parameters.\n",
    "- Going from left to right, $n_H$, $n_W \\downarrow$ and $n_C \\uparrow$\n",
    "- Made of 16 layers.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/VGG16.png\" style=\"width:90%;\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcccb0ef-305e-4d39-be71-ac0d9cf76558",
   "metadata": {},
   "source": [
    "## Advanced Models\n",
    "\n",
    "<a name ='ResN'></a>\n",
    "### ResNets\n",
    "#### Residual Block\n",
    "- The activations two layers earlier $a^{[l]}$ are added **before applying the ReLU**.\n",
    "- The addition works because in a residual block the two layers have the **same number of units**.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/residual block.png\" style=\"width:100%;\" >\n",
    "</div>\n",
    "\n",
    "#### Residual Network\n",
    "<br>\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/residual network.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "**ResNet helps with the vanishing and exploding gradients problems, which helps training deeper networks:**\n",
    "- In theory, a deeper network could result in a smaller training error. In reality, it is usually not the case for a plain network as the optimization algorithm has a much harder time training.\n",
    "- With ResNet, the training error can keeps going down with the depth of the network. \n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/ResNet helps training.jpeg\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "**Key properties**:\n",
    "- It is easy for residual blocks to learn identity functions. \n",
    "    - Assume ReLU activation is used for the hidden layers of the network, so $a^{[l]}\\geq 0$.\n",
    "      $$\n",
    "      a^{[l+2]} = g(W^{[l+2]}a^{[l+1]} + b^{[l+2]} + a^{[l]})\n",
    "      $$\n",
    "      Set $W^{[l+2]} = 0$ and $b^{[l+2]} = 0$ and we have:\n",
    "      $$\n",
    "      a^{[l+2]} = a^{[l]}\n",
    "      $$\n",
    "- In case $a^{[l+2]}$ and $a^{[l]}$ do not match dimensionalities. Use the following equation:\n",
    "    $$\n",
    "    a^{[l+2]} = g(W^{[l+2]}a^{[l+1]} + b^{[l+2]} + W^{[l+2]}_s a^{[l]})\n",
    "    $$\n",
    "    where $W^{[l+2]}_s$ can be learnt or pre-set to perform zero padding.\n",
    "\n",
    "<a name ='IncN'></a>\n",
    "### Inception Network\n",
    "#### Raw inception module\n",
    "**Idea:** Instead of choosing among several different types of filters, one could apply multiple types of filters in one layer (and use same convolution to make sure the output could be stacked together).  \n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/inception module.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "#### Computation cost of a raw inception module\n",
    "One drawback of an inception module is the high computation cost. Take the above illustration as an example. The computation cost involving the $5\\times 5$ CONV filter (counting only the numebr of multiplications, with that for additions similar) is:\n",
    "$$\n",
    "\\begin{align}\n",
    " (n_H \\times n_W \\times n_{C,\\text{out}}) \\times ( n_{\\text{kernel},H} \\times n_{\\text{kernel},W} \\times n_{C,\\text{in}}) = & 28\\times28 \\times 32 \\times 5\\times 5\\times 192 \\\\ = & 120,422,400\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Bottleneck layer\n",
    "In comparison, if we add a bottleneck layer ($1\\times1$ convolution layer) in the middle, the computation cost can be reduced to:\n",
    "$$\n",
    "28\\times28\\times16\\times 192 + 28\\times28\\times 32 \\times (5\\times 5\\times 16) = 12,443,648\n",
    "$$\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/bottleneck layer.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "#### Inception module (with bottleneck layers)\n",
    "- add bottleneck layers before applying $3\\times3$ and $5\\times5$ convolutions.\n",
    "- add bottleneck layer after the MAX POOL layer to reduce the number of channels.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/inception module with bottleneck layers.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "#### GoogLeNet\n",
    "- **MAX POOL layers** are used in between blocks of inception modules to reduce the height and width of activations.\n",
    "- **Auxiliary classifiers** (with softmax activations at the end of each) are added to intermediate layers of the network. They are **utilized only during training** and removed during inference. The loss of each classifier is weighted and then added to the total loss. This has a regularization effect and prevents the model from overfitting–making sure the the prediction using layers early on is not too bad.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/GoogLeNet.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "<a name ='MobN'></a>\n",
    "### MobileNet\n",
    "**Key features:**\n",
    "- Low computational cost at deployment\n",
    "- Useful for mobile and embedded vision application\n",
    "- Make use of depthwise separable convolutions to reduce computational cost\n",
    "\n",
    "#### Depthwise separable convolution\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/normal conv.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "The computational cost of a normal convolution:\n",
    "$$\n",
    "3\\times 3 \\times 192 \\times \\underbrace{4 \\times 4 \\times 5}_{\\text{output size}} = 138,240\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/depthwise separable conv.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "The computational cost of a depthwise separable convolution:\n",
    "$$\n",
    "3 \\times 3 \\times \\underbrace{4\\times 4 \\times 192}_{\\text{output size}} + 192 \\times \\underbrace{4 \\times 4 \\times 5}_{\\text{output size}} = 43,008\n",
    "$$\n",
    "Ratio of the costs: $\\frac{43,008}{138,240} = 0.31$\n",
    "\n",
    "In general, the ratio of costs can be computed using the following formula:\n",
    "$$\n",
    " \\text{Ratio of costs} = \\frac{1}{n_{C, \\text{out}}} + \\frac{1}{f^2}\n",
    "$$\n",
    "\n",
    "#### MobileNet V1\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/MobileNetv1.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "#### MobileNet V2\n",
    "Two improvements upon the first version:\n",
    "- Added an expansion step ($1\\times1$ convolution) before the depthwise convolution, allowing for more computation inside each bottleneck block.\n",
    "- Added a skip connection to aid optimization.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/MobileNetv2.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "\n",
    "<a name ='EffN'></a>\n",
    "### EfficientNet\n",
    "- Helps you choose an efficient combination of resolution, depth and width of your convolutional neural networks for deployment on different devices.\n",
    "- [Link](https://arxiv.org/pdf/1905.11946.pdf) to the paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
