{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42433134-4246-42b6-9b1a-46d673e2d4a0",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "## Face Recognition\n",
    "\n",
    "Author: Binghen Wang\n",
    "\n",
    "Last Updated: 21 Dec, 2022\n",
    "\n",
    "<nav>\n",
    "    <b>Deep learning navigation:</b> <a href=\"./Deep Learning Basics.ipynb\">Deep Learning Basics</a> |\n",
    "    <a href=\"./Deep Learning Optimization.ipynb\">Optimization</a>\n",
    "    <br>\n",
    "    <b>CNN navigation:</b> <a href=\"./Convolutional Neural Networks.ipynb\">CNN Basics</a> |\n",
    "    <a href=\"./Object Detection.ipynb\">Object Detection</a>\n",
    "</nav>\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebe0cd6-1f59-44b9-8229-c78f14e07102",
   "metadata": {},
   "source": [
    "## Content\n",
    "- [Face Verification vs Face Recognition](#FVvsFR)\n",
    "- [Siamese Neural Network](#SNN)\n",
    "- [Triplet Loss](#TL)\n",
    "- [Face Verification as Binary Classification](#FVasBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f290b1a-d315-4506-8276-07c50df77d57",
   "metadata": {},
   "source": [
    "<a name = \"FVvsFR\"></a>\n",
    "## Face Verification vs Face Recognition\n",
    "\n",
    "### Face Verification\n",
    "**Training**: pre-train the system using images of different individuals (each person in the training set should ideally have at least two images).<br>\n",
    "**Database/Storage**: images of $K$ people and their IDs<br>\n",
    "**Input**: image, name/ID<br>\n",
    "**Output**: whether image matches with the claimed person\n",
    "\n",
    "### Face Recognition\n",
    "**Training**: pre-train the system using images of different individuals (each person in the training set should ideally have at least two images).<br>\n",
    "**Database/Storage**: images of $K$ people and their IDs<br>\n",
    "**Input**: image<br>\n",
    "**Output**: ID if the image is a person in the database; 'not recognized' otherwise\n",
    "\n",
    "### One-Shot Learning\n",
    "Both face verification and face recognition are problems of **one-shot learning**–learning from one example to recognize the person again. Traditional network structures that use a softmax layer do not work well in this case, and they do not address the issue of staff turnover. A better approach is to learn a **similarity function** that outputs the degree of difference between images.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{d}(\\mathrm{img1}, \\mathrm{img2}) \\leq \\tau &\\implies \\text{same person} \\\\\n",
    "\\mathrm{d}(\\mathrm{img1}, \\mathrm{img2}) > \\tau &\\implies \\text{different people}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c00bff-1c45-441e-90d3-48b6bedb5e66",
   "metadata": {},
   "source": [
    "<a name = \"SNN\"></a>\n",
    "## Siamese Neural Network\n",
    "<blockquote>\n",
    "    A <b>Siamese neural network</b> (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors.\n",
    "    <div style = \"text-align: right\">– Siamese neural network, <a href = \"https://en.wikipedia.org/wiki/Siamese_neural_network\">Wikipedia</a></div>\n",
    "</blockquote>\n",
    "    \n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Siamese Neural Network.png\" style=\"width:70%;\" >\n",
    "</div>\n",
    "\n",
    "The idea is to take a usual CNN structure and remove the output layer (e.g. softmax). The last flattened layer outputs an **encoding** $f(x^{(i)})$ of an input image $x^{(i)}$. In the above, it is a vector of length 128. Encodings of two inputs are then passed into a **similarity function** to get an estimate of the difference. This difference is then used in the **loss** calculation, the results of which provides information for backpropagation and updating of the network. \n",
    "\n",
    "Here, the similarity function is defined as the squared **Frobenius norm** ($L_2$ norm).\n",
    "\n",
    "$$\n",
    "d(x^{(1)}, x^{(2)}) = \\left\\Vert f(x^{(1)} - f(x^{(2)})\\right\\Vert^2_2\n",
    "$$\n",
    "\n",
    "$d(x^{(1)}, x^{(2)})$ should be small if $x^{(1)}$ and $x^{(2)}$ represent the same person and large otherwise. For the above illustration, we expect it to be large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e37f7-3b88-4259-8a08-dd9bf987c193",
   "metadata": {},
   "source": [
    "<a name = \"TL\"></a>\n",
    "## Triplet Loss\n",
    "\n",
    "To learn the parameters for the above network, we need to specify a suitable loss function. One commonly used candidate is the **triplet loss** function.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/triplet loss.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "Rearrange the inequality and we get:\n",
    "$$\n",
    "\\left\\Vert f(A) - f(P) \\right\\Vert^2 - \\left\\Vert f(A) - f(N) \\right\\Vert^2 + \\alpha \\leq 0\n",
    "$$\n",
    "\n",
    "The triplet loss is defined as:\n",
    "$$\n",
    "L(A,P,N) = \\max\\left(\\left\\Vert f(A) - f(P) \\right\\Vert^2 - \\left\\Vert f(A) - f(N) \\right\\Vert^2 + \\alpha, 0\\right)\n",
    "$$\n",
    "\n",
    "The cost function for a training set of $m$ triplets $(A,P,N)$ is:\n",
    "$$\n",
    "J = \\sum_{i=1}^m L(A^{(i)},P^{(i)},N^{(i)})\n",
    "$$\n",
    "\n",
    "<div class = \"alert alert-block alert-success\"><b>Note:</b> For the training of a face recognition algorithm, we need <b>multiple images</b> (at least 2) for the same person. This is important in the formation of training triplets. For the application of the algorithm, we can keep a <b>single image</b> for a person in the database. <b>Mind the distinction between training and application of a face recognition algorithm.</b></div>\n",
    "\n",
    "<div class = \"alert alert-block alert-warning\"><b>Training tip:</b> To make the training more effective, when forming the training set, we want to <b>select triplets that are difficult to train on</b>. (E.g. select negatives that look similar to the anchors, select positives from different angles, select anchors under different settings.)</div>\n",
    "<div class = \"alert alert-block alert-warning\"><b>Application tip:</b> To save time on inference, images can be <b>encoded ahead of time</b> with their <b>encodings stored in the database</b>. Therefore, when a new image is given, the encoding process is conducted only once. The encoding of the new image can then be compared with existing encodings in the database for verification/recognition. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a07336-4ed3-413a-94f5-4365f0c1cc65",
   "metadata": {},
   "source": [
    "<a name = \"FVasBC\"></a>\n",
    "## Face Verification as Binary Classification\n",
    "\n",
    "Instead of using the triplet loss function, an alternative way is to train the face verification/recognition algorithm as a binary classification problem using **pairs of images**. The training process takes **two input images** at a time and uses a Siamese neural network to conduct inference on the images separately before connecting the two strands and outputting a binary classification (whether they are the same person or not).\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/face verification as binary classification.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "An example of a training set using this approach looks like the following:\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/face verification binary training set.png\" style=\"width:30%;\" >\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
