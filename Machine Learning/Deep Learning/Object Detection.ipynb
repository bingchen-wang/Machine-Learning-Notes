{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6baaae8a-a19e-48ac-ab6a-404d2c35ef5d",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "## Object Detection\n",
    "\n",
    "Author: Binghen Wang\n",
    "\n",
    "Last Updated: 19 Dec, 2022\n",
    "\n",
    "<nav>\n",
    "    <b>Deep learning navigation:</b> <a href=\"./Deep Learning Basics.ipynb\">Deep Learning Basics</a> |\n",
    "    <a href=\"./Deep Learning Optimization.ipynb\">Optimization</a>\n",
    "    <br>\n",
    "    <b>CNN navigation:</b> <a href=\"./Convolutional Neural Networks.ipynb\">CNN Basics</a>\n",
    "</nav>\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "- [Object Classification with Localization](#OCL)\n",
    "- [Object Detection](#OD)\n",
    "    - [Sliding Windows](#SW)\n",
    "- [YOLO Algorithm](#YOLO)\n",
    "    - [Bounding Boxes](#BB)\n",
    "    - [Intersection Over Union](#IoU)\n",
    "    - [Non-max Suppression](#NMS)\n",
    "    - [Anchor Boxes](#AB)\n",
    "- [Semantic Segmentation](#SS)\n",
    "    - [Transposed Convolution](#TC)\n",
    "    - [U-Net](#UN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e197fa-dfb5-4efc-8c3c-36324c2090f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Previously, the discussed models are mainly for the task of **object classification**. Namely, given an input image, the network classifies it into one of the known classes. The CNN can also be used to perform more complicated tasks.\n",
    "\n",
    "<a name = 'OCL'></a>\n",
    "## Object Classification with Localization\n",
    "An improvement of the classification algorithm is to output concurrently a rectangle surrounding the identified object. This process is called **localization**. This can be achieved by modifying the output layer of the CNN.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Object classification with localization.png\" style=\"width:90%;\" >\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Example\n",
    "Consider a classification with localization problem, where the total number of classes equals 4 (pedestrian, car, motorcycle, background). The output/target is a $8\\times1$ vector and takes the following form:\n",
    "$$\n",
    "y = \\left[\\begin{array}{c}\n",
    "p_c \\\\\n",
    "b_x \\\\\n",
    "b_y \\\\\n",
    "b_h \\\\\n",
    "b_w \\\\\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "c_3 \\\\\n",
    "\\end{array}\\right] \\begin{array}{l}\n",
    "\\cdots \\text{is there any object?} \\\\\n",
    "\\cdots \\text{bounding box location x} \\\\\n",
    "\\cdots \\text{bounding box location y} \\\\\n",
    "\\cdots \\text{bounding box height} \\\\\n",
    "\\cdots \\text{bounding box width} \\\\\n",
    "\\cdots \\text{is the object a pedestrian?} \\\\\n",
    "\\cdots \\text{is the object a car?} \\\\\n",
    "\\cdots \\text{is the object a motorcycle?} \n",
    "\\end{array}\n",
    "$$\n",
    "The loss function can be defined as:\n",
    "$$\n",
    "L(\\hat y, y) = \\text{BinaryCrossEntropy}(\\hat y_1, y_1) + \\mathbb{1}\\{y_1 = 1\\} \\cdot \\sum_{j=2}^{j=5}{(\\hat y_j - y_j)}^2 + \\mathbb{1}\\{y_1 = 1\\} \\cdot \\text{CrossEntropy}\\left(\\left[\\begin{array}{c} \\hat y_6 \\\\ \\hat y_7 \\\\ \\hat y_8 \\end{array}\\right], \\left[\\begin{array}{c} y_6 \\\\ y_7 \\\\ y_8 \\end{array} \\right]\\right)\n",
    "$$\n",
    "Therefore, <br>\n",
    "if $y = 1$,\n",
    "$$\n",
    "L(\\hat y, y) = \\text{BinaryCrossEntropy}(\\hat y_1, y_1) + \\sum_{j=2}^{j=5}{(\\hat y_j - y_j)}^2 + \\text{CrossEntropy}\\left(\\left[\\begin{array}{c} \\hat y_6 \\\\ \\hat y_7 \\\\ \\hat y_8 \\end{array}\\right], \\left[\\begin{array}{c} y_6 \\\\ y_7 \\\\ y_8 \\end{array} \\right]\\right)\n",
    "$$\n",
    "if $y = 0$,\n",
    "$$\n",
    "L(\\hat y, y) = \\text{BinaryCrossEntropy}(\\hat y_1, y_1)\n",
    "$$\n",
    "\n",
    "<a name ='OD'></a>\n",
    "## Object Detection\n",
    "<a name ='SW'></a>\n",
    "### Sliding windows\n",
    "One simple yet computationally costly way to turn an object classification algorithm into an object detection algorithm is through the use of sliding windows. To save on computation, one could use the convolutional implementation of sliding windows.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/sliding windows.png\" style=\"width:100%;\" >\n",
    "</div>\n",
    "\n",
    "A **drawback** of the sliding windows approach is that the size of sliding windows is **fixed** (by the original classification algorithm input). To achieve better localization, we want windows of different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c705e-525a-4911-8347-6b651b9e3144",
   "metadata": {},
   "source": [
    "## YOLO Algorithm\n",
    "\n",
    "YOLO stands for 'You Only Look Once'. It was invented by <a href = 'https://arxiv.org/pdf/1506.02640.pdf'>Redmon et al. (2015)</a>, making use of the **convolutional implementation of sliding windows** and **object classification with localization**.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/YOLO Algorithm.png\" style=\"width:50%;\" ><br>\n",
    "    Source: <a href = 'https://arxiv.org/pdf/1506.02640.pdf'>Redmon et al. (2015)</a>\n",
    "</div>\n",
    "\n",
    "<a name ='BB'></a>\n",
    "### Bounding Box Predictions for Grid Cells\n",
    "The algorithm works by:\n",
    "- dividing the input into a $S\\times S$ grid\n",
    "- for each grid cells, predicting $B$ bounding boxes, confidences for those boxes, and $C$ class probabilities.\n",
    "- labeling a grid as having an object if the midpoint of the object falls into it.\n",
    "\n",
    "#### Example\n",
    "**Three classes**: {pedestrian, car, motorcycle}. <br>\n",
    "**The target label for each grid cell** is of length 8: ${\\left[p_c, b_x, b_y, b_h, b_w, c_1, c_2, c_3 \\right]}^{\\mathsf{T}}$\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/bounding box predictions.png\" style=\"width:80%;\" >\n",
    "</div>\n",
    "The diagonal-stripes-shaded cell has the label\n",
    "$$\n",
    "{\\left[1, 0.4, 0.15, 0.5, 0.85, 0, 1, 0 \\right]}^{\\mathsf{T}},\n",
    "$$\n",
    "whereas the checkered cell has label\n",
    "$$\n",
    "{\\left[0, ?, ?, ?, ?, ?, ?, ? \\right]}^{\\mathsf{T}}.\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<div class = \"alert alert-block alert-success\"><b>Note:</b> the bounding box can exceed the boundaries of a grid. </div>\n",
    "\n",
    "<a name ='IoU'></a>\n",
    "### Intersection Over Union\n",
    "Localization can generally not be exactly the same as the labelled location of bounding box. In order to evaluate the goodness of the localization, we employ the idea of **intersection over union** (IoU). It works by calculating the ratio of the intersection of two bounding boxes over their union. Generally speaking, an $$\\text{IoU} \\geq 0.5$$ is considered **correct localization**.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/IoU.png\" style=\"width:20%;\" >\n",
    "</div>\n",
    "\n",
    "<a name ='NMS'></a>\n",
    "### Non-max Suppression\n",
    "In the inference phase, multiple bounding boxes could be predicted for a certain object. (This happens when multiple grids 'think' they contian the middle points of the object.) To make sure that only one bounding box (the best one) is predicted for each object, the **non-max suppression algorithm** can be used. It works as follows:\n",
    "\n",
    "<blockquote>\n",
    "    For each object class (e.g. $c_1, c_2, c_3$):\n",
    "    <blockquote>\n",
    "    Rank all the bounding boxes by confidence $p_c$. <br>\n",
    "    Repeat while there is any remaining bounding boxes:\n",
    "    <blockquote>\n",
    "        <ul>\n",
    "            <li> Pick the box with the current largest confidence and output it as a prediction.\n",
    "            <li> Delete all boxes that have an IoU $\\geq 0.5$ with the selected box.\n",
    "        </ul>\n",
    "    </blockquote>\n",
    "    </blockquote>\n",
    "</blockquote>\n",
    "\n",
    "<a name ='AB'></a>\n",
    "### Anchor Boxes\n",
    "While the chance of the midpoints of two objects falling into one grid is small with small grid cells, the **Anchor Boxes** algorithm has been invented to deal with this possibility. It works by pre-specifying a couple of anchor boxes of different shapes and then label the objects with an anchor box which has the **largest IoU** with the object bounding box.\n",
    "\n",
    "One way to pick the shapes of anchor boxes is through **K-means** classification using commonly observed objects. The anchor boxes algorithm allows the detection algorithm to specializ\n",
    "e better in detecting objects of different shapes.\n",
    "\n",
    "#### Example\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/anchor boxes.png\" style=\"width:50%;\" >\n",
    "</div>\n",
    "\n",
    "The output shape is $3\\times 3\\times 16$ or $3 \\times 3 \\times 2 \\times 8$. For this input photo, grid cell 8 has the following label:\n",
    "\n",
    "$$\n",
    "{\\left[\\underbrace{0, ?, ?, ?, ?, ?, ?, ?}_{\\text{anchor box 1}}, \\underbrace{1, 0.3, 0.3, 0.4, 0.75, 0, 1, 0}_{\\text{anchor box 2}} \\right]}^{\\mathsf{T}}\n",
    "$$\n",
    "\n",
    "Grid cell 9 has the following label:\n",
    "\n",
    "$$\n",
    "{\\left[\\underbrace{1, 0.7, 0.4, 0.6, 0.25, 1, 0, 0}_{\\text{anchor box 1}}, \\underbrace{1, 0.18, 0.3, 0.4, 0.75, 0, 1, 0}_{\\text{anchor box 2}} \\right]}^{\\mathsf{T}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59aa60-ce49-4a28-a8a4-7cdbf94cdf7e",
   "metadata": {},
   "source": [
    "<a name = 'SS'></a>\n",
    "## Semantic Segmentation\n",
    "\n",
    "**Semantic segmentation** is the labelling of **each pixel** with a class of what it belongs to. It allows for finer boundaries of different objects with in an image and has been applied widely in medical image analysis and autonomous driving.\n",
    "\n",
    "<a name ='TC'></a>\n",
    "### Transposed Convolution\n",
    "Previous classification and detection algorithms follow the general pattern of decreasing height and width as an input passes through the neural network. Yet to be able to label each pixel with a class, the output of the neural network is required to be of the same shape as the input. This is achieved by the employment of **Transposed Convolutional Layers**.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/transposed convolution.png\" style=\"width:75%;\" >\n",
    "</div>\n",
    "\n",
    "A good explanation of transposed convolution: <a href =\"https://naokishibuya.medium.com/up-sampling-with-transposed-convolution-9ae4f2df52d0\">here</a>.\n",
    "<blockquote>\n",
    "    [W]e <b>can emulate the transposed convolution using a convolution</b>. We up-sample the input by adding zeros between the values in the input matrix in a way that the direct convolution produces the same effect as the transposed convolution. You may find some article explains the transposed convolution in this way. However, it is less efficient due to the need to add zeros to up-sample the input before the convolution. <br>\n",
    "    <div style = \"text-align: right\">â€“ Up-sampling with Transposed Convolution, Naoki (2017)</div>\n",
    "</blockquote>\n",
    "\n",
    "<a name ='UN'></a>\n",
    "### U-Net\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/U-net.png\" style=\"width:75%;\" > <br>\n",
    "    Source: <a href = \"https://arxiv.org/pdf/1505.04597.pdf\">Olaf Ronneberger, Philipp Fischer, and Thomas Brox (2015)</a>\n",
    "</div>\n",
    "\n",
    "**Note**:\n",
    "- The up-conv in the picture refers to the **transposed convolution**.\n",
    "- The **skip connection** here *differs from* that in ResNets in that it takes the early activations and **concatenate** them with the activations from the normal strand. In ResNets, skip connections add early activations and *add* them to the linear part before feeding them the activation function.\n",
    "- The **skip connection** helps provide high-resolution low-level feature information, while the usual strand provides the low-resolution high-level contextual information.\n",
    "- The final **out_channels** equals the number of classes $n_c$ of the classification problem, which happens to be 2 in the above illustration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
