{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reinforcement Learning\n",
    "Author: Bingchen Wang\n",
    "\n",
    "Last Updated: 6 Sep, 2022\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel='stylesheet' type='text/css' media='screen' href='../styles/custom.css'>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel='stylesheet' type='text/css' media='screen' href='../styles/custom.css'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<section class = \"section--outline\">\n",
    "    <div class = \"outline--header\">Outline </div>\n",
    "    <div class = \"outline--content\">\n",
    "        <b>Concepts:</b>\n",
    "        <ul>\n",
    "            <li> <a href='#Motivation'>Motivation</a>\n",
    "                <ul>\n",
    "                    <li> <a href='#MDP'>Markov Decision Processes (MDPs)</a>\n",
    "                    <li> <a href='#VF'>Value Function</a>\n",
    "                    <li> <a href='#BE'>Bellman's Equations</a>   \n",
    "                </ul>\n",
    "            <li> <a href='#Solution'>Solution</a>\n",
    "                <ul>\n",
    "                    <li> <a href='#LearningPsa'>Learning $P_{sa}$</a>\n",
    "                    <li> <a href='#VI'>Fitted Value Iteration</a>\n",
    "                        <ul><li> Deep Q-Network <ul><li> Soft updating</ul></ul>\n",
    "                </ul>\n",
    "            <li> <a href = '#FHMDP'>Finite-horizon MDP</a>\n",
    "                <ul>\n",
    "                    <li> <a href = '#SAR'>State-action Rewards</a>\n",
    "                    <li> <a href = '#FHMDP'>Finite Horizon MDP</a>\n",
    "                    <li> <a href = '#LQR'>Linear Quadratic Regulation (LQR)</a>\n",
    "                </ul>        \n",
    "        </ul>\n",
    "        <b>Implementation:</b>\n",
    "        <ul>\n",
    "            <li> <a href='./Tensorflow Implementation.ipynb'>TensorFlow</a>\n",
    "            <li> PyTorch\n",
    "            <li> <a href='../../Machine Learning Specialization/Course 3 Unsupervised Learning, Recommenders, Reinforcement Learning/Week 3/Practice Lab/C3_W3_A1_Assignment.ipynb'>Machine Learning Specialization<a>\n",
    "        </ul>\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Notation\n",
    "\n",
    "| Concept | Notation |\n",
    "| :----------| :----------- |\n",
    "| states | $S$ |\n",
    "| actions | $A$ |\n",
    "| state transition probabilities | $P_{sa}$ |\n",
    "| discount factor| $\\gamma$ |\n",
    "| reward | $R$ |\n",
    "| return | $Q$ |\n",
    "| policy| $\\pi$|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='Motivation'></a>\n",
    "### Motivation\n",
    "\n",
    "\n",
    "<a name='MDP'></a>\n",
    "#### Markov Decision Process (MDP)\n",
    "A Markov Decision Process (MDP) is described by <mark>$(S, A, \\{P_{sa}\\}, \\gamma, R)$</mark>.\n",
    "\n",
    "<a name='VF'></a>\n",
    "#### Value Function\n",
    "For a policy $\\pi$, $V^{\\pi}:S\\mapsto A$ is such that $V^{\\pi}(s)$ is the expected total payoff for starting in state $s$ and executing $\\pi$.\n",
    "$$\n",
    "    V^{\\pi}(s) = \\mathrm{E}[R(s_0)+ \\gamma R(s_1) + \\cdots | \\pi, s_0 = s]\n",
    "$$\n",
    "\n",
    "<a name='BE'></a>\n",
    "#### Bellman's equations\n",
    "Any policy:\n",
    "$$\n",
    "    V^{\\pi}(s) = R(s_0)+ \\gamma \\sum_{s'}P_{s\\pi(s)} V^{\\pi}(s)\n",
    "$$\n",
    "Optimal policy:\n",
    "$$\n",
    "    V^{*}(s) = R(s_0)+ \\max_{a} \\gamma \\sum_{s'}P_{sa} V^{*}(s)\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Goal of Reinforcement Learning:</b> Learn the optimal action at any given state. (i.e. find the optimal policy $\\pi^{*}$) </div>\n",
    "<div class=\"alert alert-block alert-warning\"> <b>Learning strategy:</b> <ol>\n",
    "    <li> Model or learn $P_{sa}$\n",
    "    <li> Find $V^{*}$\n",
    "    <li> Implicitly find $\\pi^{*}$ (given by the argmax equation)\n",
    "</ol> </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"Solution\"></a>\n",
    "### Solution\n",
    "\n",
    "<a name=\"LearningPsa\"></a>\n",
    "#### Learning $P_{sa}$\n",
    "##### Approach 1: Learn from reality\n",
    "$$P_{sa}(s') = \\frac{\\text{# times took action \"a\" in state s and got to s'}}{\\text{# times took action \"a\" in state s}}$$\n",
    "(or $1/|S|$ if the denominator is 0.)\n",
    "##### Approach 2: Build a simulator based on domain knowledge, e.g. physics simulator\n",
    "##### Approach 3: Build a supervised learning model and add noise\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Learning the state transition probabilities</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        <ul>\n",
    "            <li> Collect data ${\\{s^{(i)}_t\\}}^{i=1,...,m}_{t=1,...,T}$.\n",
    "            <li> Train $h: S \\times A \\mapsto S$.\n",
    "            <li> Use $s_{t+1} = h(s_t,a_t) + \\epsilon_t$ where $\\epsilon_t \\sim N(0,\\sigma^2 I)$.\n",
    "        </ul>\n",
    "    </div>\n",
    "</section>\n",
    "\n",
    "<a name = 'VI'></a>\n",
    "#### Fitted Value Iteration\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Fitted Value Iteration</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        <blockquote>\n",
    "            Sample $\\{s^{(1)}, s^{(2)},\\dots,s^{(m)}\\} \\subseteq S$ randomly. <br>\n",
    "            Initialise $\\theta := 0$. <br>\n",
    "            <b>Repeat:</b>\n",
    "            <blockquote> For $i = 1, \\dots, m$,\n",
    "                <blockquote> For each action $a \\in A$, \n",
    "                    <blockquote>\n",
    "                        Sample $s_1^\\prime,s_2^\\prime,\\dots,s_k^\\prime \\sim P_{s^{(i)},a}$. <br>\n",
    "                        Set $ q(a) = \\frac{1}{k}\\sum^{k}_{j=1}[R(s^{(i)}) + \\gamma V(s^\\prime_j)]$. <br>\n",
    "                    </blockquote>  \n",
    "                    set $y^{(i)} = \\max_a q(a)$ \n",
    "                </blockquote>\n",
    "            Update $\\theta := \\arg\\min_{\\theta} \\frac{1}{2}\\sum^m_{i=1}(f_\\theta(s^{(i)})-y^{(i)})^2$\n",
    "            <div class=\"alert alert-block alert-success\"> <b>Note:</b> Can choose $f_{\\theta}$ to be any suitable supervised learner.</div>    \n",
    "            </blockquote>    \n",
    "        </blockquote>\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name = \"FHMDP\"></a>\n",
    "### Finite-horizon MDP\n",
    "\n",
    "<a name = \"SAR\"></a>\n",
    "#### State-action rewards\n",
    "**Intuition**: Actions may have costs.\n",
    "\n",
    "**Bellman's euqations**:\n",
    "$$ V^*(s) = \\max_a [R(s,a) + \\gamma \\sum_{s^\\prime}P_{sa}(s^\\prime)V^*(s^\\prime)]$$\n",
    "\n",
    "**Value iteration**:\n",
    "$$V := RHS$$\n",
    "\n",
    "<a name = \"FHMDP-detail\"></a>\n",
    "#### Finite-horizon MDP\n",
    "**MDP**: $(S,A,\\{P_{sa}\\}, T, R)$ where $T$ is the horizon time\n",
    "<div class=\"alert alert-block alert-danger\"> <b>Note:</b> There is no gamma ($\\gamma$). </div>\n",
    "\n",
    "**Notations**:\n",
    "\n",
    "| Concept | Notation |\n",
    "| :----------| :----------- |\n",
    "| state | $s_t$ |\n",
    "| action | $a_t$ |\n",
    "| state transition probabilities | $P_{sa}^{(t)}$ |\n",
    "| horizon time| $T$ |\n",
    "| reward | $R^{(t)}$ |\n",
    "| return/value | $V_t^*$ |\n",
    "| policy| $\\pi^*_t$|\n",
    "\n",
    "**Value iteration**:\n",
    "$$\n",
    "V^*_T(s) = \\max_a R^{(T)}(s,a)\n",
    "$$\n",
    "Repeat:\n",
    "$$\n",
    "V^*_t(s) = \\max_a \\{R^{(t)}(s,a) + \\sum_{s^\\prime} P_{sa}^{(t)} V^*_{t+1}(s^\\prime)\\} \\\\\n",
    "\\pi^*_t(s) = \\arg\\max_a \\{R(s,a) + \\sum_{s^\\prime} P_{sa}^{(t)} V^*_{t+1}(s^\\prime)\\}\n",
    "$$\n",
    "\n",
    "<a name = \"LQR\"></a>\n",
    "#### Linear Quadratic Regulation (LQR)\n",
    "<div class=\"alert alert-block alert-info\"> <b>Assumptions:</b> \n",
    "    <ol>\n",
    "        <li> linear state-transition probabilities $P_{sa}: s_{t+1} = As_t+Ba_t + w_t$ where $w_t \\sim N(0,\\Sigma_w)$\n",
    "        <li> quadratic state-action reward function $R(s,a)= -(s^TUs + a^TVa)$\n",
    "    </ol>        \n",
    "</div>\n",
    "\n",
    "##### Learning A and B\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Method 1: Learn from it</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        <ul>\n",
    "            <li> Collect data ${\\{s^{(i)}_t\\}}^{i=1,...,m}_{t=1,...,T}$.\n",
    "            <li> Model $s_{t+1} \\approx As_t+ Ba_t$.\n",
    "            <li> Loss $ \\min_{A,B} \\sum^m_{i=1} \\sum^{T-1}_{t=0}||s_{t+1} - (As_t+Ba_t)||^2$.\n",
    "        </ul>\n",
    "    </div>\n",
    "</section>\n",
    "\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Method 2: Linearise a non-linear model</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        <ul>\n",
    "            <li> Known model $s_{t+1} = f(s_t, a_t)$.\n",
    "            <li> Pick a pair of typical values $(\\bar{s}_t, \\bar{a}_t)$ for $s_t$ and $a_t$\n",
    "            <li> Linearise $s_{t+1} \\approx f(\\bar{s}_t, \\bar{a}_t) + (\\nabla_s f(\\bar{s}_t, \\bar{a}_t))^T (s_t - \\bar{s}_t)+ (\\nabla_a f(\\bar{s}_t, \\bar{a}_t))^T (a_t - \\bar{a}_t)$\n",
    "        </ul>\n",
    "    </div>\n",
    "</section>\n",
    "\n",
    "##### Base case\n",
    "$$\n",
    "V^*_T(s_T) = \\max_{a_T} R(s_T,a_T) = \\max_{a_T} [-s^T_TUs_T - a^T_TVa_T] = - s^T_TUs_T \\\\\n",
    "\\pi^*_T(s_T) = \\vec{0}\n",
    "$$\n",
    "##### Intermediate case\n",
    "Suppose\n",
    "$$\n",
    "V^*_{t+1}(s_{t+1}) = s^T_{t+1} \\Phi_{t+1} s_{t+1} + \\Psi_{t+1}\n",
    "$$\n",
    "\n",
    "where $\\Phi_{t+1} \\in \\mathbb{R}^{n \\times n}$, $\\Psi_{t+1} \\in \\mathbb{R}$. Then,\n",
    "$$\n",
    "V^*_t = \\max_{a_t} R(s_t, a_t) + \\mathrm{E}_{s_{t+1} \\sim P_{s_ta_t}}[V^*_{t+1}(s_{t+1})] \\\\\n",
    "V^*_t = \\max_{a_t} -s^T_tUs_t - a^T_tVa_t + \\mathrm{E}_{s_{t+1} \\sim N(As_t + Ba_t, \\Sigma_w)}[s^T_{t+1} \\Phi_{t+1} s_{t+1} + \\Psi_{t+1}] \n",
    "$$\n",
    "\n",
    "Take derivative with respect to $a_t$ and set it to $0$ to solve for $a_t$:\n",
    "$$\n",
    "a_t = \\underbrace{(B^T\\Phi_{t+1}B - V)^{-1}B^{T} \\Phi_{t+1}A}_{L_t} s_t \\\\\n",
    "\\pi^*_t(s_t) = L_ts_t\n",
    "$$\n",
    "<div class = \"alert alert-block alert-warning\">The optimal action is a linear function of the state $s_t$.</div>\n",
    "Then,\n",
    "$$\n",
    "V^*_t = s^T_t\\Phi_ts_t + \\Psi_t\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\Phi_t = A(\\Phi_{t+1}- \\Phi_{t+1}B(B^T\\Phi_{t+1}B-V)^{-1}B\\Phi_{t+1})A - U \\\\\n",
    "\\Psi_t = -\\mathrm{tr}\\Sigma_w\\Phi_{t+1} + \\Psi_{t+1}\n",
    "$$\n",
    "<div class = \"alert alert-block alert-danger\"><b>Note:</b> The covariance matrix of the noise $w_t$ affects only $\\psi_t$, which does not affect the optimal policy.</div>\n",
    "\n",
    "##### Putting it together\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Linear Quadratic Regulation</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        <blockquote>\n",
    "            Initialise $\\Phi_T = -U$, $\\Psi_t = 0$. <br>\n",
    "            Solve the base case. $V^*_T = -s^T_TUS_T$ and $\\pi^*_T(s_T) = 0$. <br>\n",
    "            Recursively calculate: <blockquote>\n",
    "            $\\Phi_t, \\Psi_t$ using $\\Phi_{t+1},\\Psi_{t+1}$ (for $t=T-1, T-2, \\dots, 0$):<blockquote>\n",
    "            $\\Phi_t = A(\\Phi_{t+1}- \\Phi_{t+1}B(B^T\\Phi_{t+1}B-V)^{-1}B\\Phi_{t+1})A - U$<br>\n",
    "            $\\Psi_t = -\\mathrm{tr}\\Sigma_w\\Phi_{t+1} + \\Psi_{t+1}$</blockquote>\n",
    "            </blockquote>\n",
    "            Calculate $L_t = (B^T\\Phi_{t+1}B - V)^{-1}B^{T} \\Phi_{t+1}A$.\n",
    "        </blockquote>\n",
    "        Solution: $\\pi^*(s_t) = L_ts_t$ (exact result of LQR).\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
