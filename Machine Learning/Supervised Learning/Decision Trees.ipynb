{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56223402-1dd8-4475-a6f9-feb2dcb82acf",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "## Decision Trees\n",
    "\n",
    "Author: Bingchen Wang\n",
    "\n",
    "Last Updated: 19 Oct, 2022\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"./Supervised Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fcc0487-58f9-4d63-8613-5cf41d156c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel='stylesheet' type='text/css' media='screen' href='../styles/custom.css'>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel='stylesheet' type='text/css' media='screen' href='../styles/custom.css'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b151f-56d7-4380-9528-96006e708b7d",
   "metadata": {},
   "source": [
    "<section class = \"section--outline\">\n",
    "    <div class = \"outline--header\">Outline </div>\n",
    "    <div class = \"outline--content\">\n",
    "        <b>Concepts:</b>\n",
    "        <ul>\n",
    "            <li> <a href = \"#DTL\">Decision tree learning</a>\n",
    "            <li> <a href = \"#RT\">Regression trees</a>\n",
    "            <li> <a href = \"#CT\">Classification trees</a>\n",
    "                <ul>\n",
    "                    <li> <a href = \"#RA\">Recursive Algorithm</a>\n",
    "                </ul>\n",
    "        </ul>\n",
    "        <b>Implementation:</b>\n",
    "        <ul>\n",
    "            <li> <a href = \"./Decision Trees/Sklearn Implementation.ipynb\">Sklearn Implementation</a>\n",
    "        </ul>\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2daf570-722a-4a5c-a2f1-9b1210ce4aac",
   "metadata": {},
   "source": [
    "<a name = \"DTL\"></a>\n",
    "## Decision tree learning\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Key decisions:</b> \n",
    "<ol>\n",
    "    <li> <b>(Splitting criterion)</b> How to choose what feature (and value, for continuous features) to split on at each node? \n",
    "        <ul>\n",
    "            <li> (regression) minimize sum of squares\n",
    "            <li> (classification) minimize impunity\n",
    "        </ul>\n",
    "    <li> <b>(Stopping criterion)</b> When to stop splitting?\n",
    "        <ul>\n",
    "            <li> fully grown (cost reaches 0)\n",
    "            <li> a certain number of terminal nodes is reached\n",
    "            <li> maximum tree depth is reached\n",
    "            <li> improvements in the reduction of the cost (information gain) below a certain threshold\n",
    "            <li> number of examples in a node below a certain threshold\n",
    "        </ul>        \n",
    "    <li> <b>(Optional pruning criterion)</b> How to prune the tree?\n",
    "        <ul>\n",
    "            <li> cost-complexity criterion\n",
    "            <li> cross-validation\n",
    "        </ul>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Model\n",
    "Consider a sample of $m$ examples $(x_i, y_i), \\; i = 1, 2, \\dots, N$.\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=m}^M c_m I(x \\in R_m)\n",
    "$$\n",
    "where $c_m$ is the predicted output in region $R_m$.\n",
    "\n",
    "### Cost function\n",
    "- sum of squares for regression trees\n",
    "- misclassification rate, Gini index, cross-entropy for classification trees\n",
    "\n",
    "### Modelling approach\n",
    "- Finding the best binary partition is generally computationally infeasible. Hence proceed with a **greedy algorithm**.\n",
    "- Strategy:\n",
    "    - First, grow a large tree $T_0$, stopping the splitting process only when some minimum node size is reached.\n",
    "    - Then, prune the tree using **cost-complexity pruning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4545d0-2a9f-4981-988e-f34c22076eb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name = \"RT\"></a>\n",
    "## Regression trees\n",
    "\n",
    "### Notation\n",
    "| Notation | Meaning |\n",
    "| --- | --- |\n",
    "| $T$ | a subtree obtained by pruning $T_0$|\n",
    "|$\\vert T \\vert$| the number of terminal nodes of $T$|\n",
    "|$\\alpha$| the **tunning parameter**| \n",
    "\n",
    "### The cost complexity criterion\n",
    "$$\n",
    "\\begin{align}\n",
    "N_m =& \\#\\{x_i \\in R_m \\} \\\\\n",
    "\\hat c_m =& \\frac{1}{N_m} \\sum_{x_i \\in R_m} y_i \\\\\n",
    "Q_m(T) =& \\frac{1}{N_m} \\sum_{x_i \\in R_m} (Y_i - \\hat c_m)^2 \\\\\n",
    "C_\\alpha =& \\sum_{m=1}^{\\vert T \\vert} N_m Q_m(T) + \\alpha \\vert T\\vert\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8668c-1c65-43ab-9d25-839b727ad051",
   "metadata": {},
   "source": [
    "<a name = \"CT\"></a>\n",
    "## Classification trees\n",
    "\n",
    "### Notation\n",
    "| Notation | Meaning |\n",
    "| --- | --- |\n",
    "| $m$ | a terminal node|\n",
    "|$k$| a class|\n",
    "\n",
    "Proportion of observations that belong to class $k$ in region/node $m$:\n",
    "$$\n",
    "\\hat p_{mk} =\\frac{1}{N_m} \\sum_{x_i \\in R_m} I(y_i = k)\n",
    "$$\n",
    "\n",
    "Output class for region $m$:\n",
    "$$\n",
    "k(m) = \\arg\\max_k \\hat p_{mk}\n",
    "$$\n",
    "### Node impurity measures for a terminal node/region\n",
    "#### Mislcassification error\n",
    "$$\n",
    "\\frac{1}{N_m} \\sum_{i\\in R_m} I(y_i \\neq k(m)) = 1 - \\hat p_{mk(m)}\n",
    "$$\n",
    "\n",
    "#### Gini index\n",
    "$$\n",
    "\\sum_{k = 1}^K \\hat p_{mk} (1 - \\hat p_{mk})\n",
    "$$\n",
    "\n",
    "#### Cross-entropy/deviance\n",
    "$$\n",
    "- \\sum_{k = 1}^K \\hat p_{mk} \\log\\hat p_{mk}\n",
    "$$\n",
    "\n",
    "\n",
    "### Information gain\n",
    "Denote the cross-entropy at a specific node $m$ as $H(m)$. Consider the information gain of a split at $m$:\n",
    "\n",
    "$$\n",
    "\\text{Information gain} = H(m^{\\text{root}}) - [w^{\\text{left}}* H(m^{\\text{left}}) + w^{\\text{right}}* H(m^{\\text{right}})]\n",
    "$$\n",
    "\n",
    "<a name = \"RA\"></a>\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Recursive Algorithm</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        <ol>\n",
    "            <li>Start with all examples at the root node. \n",
    "            <li>Calculate the information gains for all possible features, and pick one with the highest information gain. \n",
    "            <li>Split dataset according to the selected feature, creating a left subtree and a right subtree. \n",
    "            <li>Keep <b>repeating the above splitting process</b> on the subtrees until a stopping criteria is met. (exhaust the left subtree first and then right subtree.)\n",
    "        </ol>\n",
    "        <div style = \"text-align: center;\">\n",
    "            <img src=\"./images/DT_Recursive Algorithm.pdf\" style=\"width:80%;\" >\n",
    "        </div>\n",
    "    </div>\n",
    "</section>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
