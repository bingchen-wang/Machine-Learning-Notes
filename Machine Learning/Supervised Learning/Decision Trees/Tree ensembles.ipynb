{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b794cf-7517-4826-9010-b87970fee144",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "## Decision Trees - Tree ensembles\n",
    "\n",
    "Author: Bingchen Wang\n",
    "\n",
    "Last Updated: 31 Oct, 2022\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning.ipynb\">Supervised Learning</a> |\n",
    "    <a href=\"../Decision Trees.ipynb\">Decision Trees</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52d096e0-5416-41d6-b807-0b3e497a7cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel='stylesheet' type='text/css' media='screen' href='../styles/custom.css'>\n",
       "<!--For a better format, run this code.-->\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel='stylesheet' type='text/css' media='screen' href='../styles/custom.css'>\n",
    "<!--For a better format, run this code.-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9f039e-3417-4c62-86dd-bf2ba8a3dfd6",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- Popular ensemble methods\n",
    "    - [Bootstrap aggregation/Bagging](#BA)\n",
    "        - [Bagging](#BA1)\n",
    "        - [On average, each bagged tree makes use of 2/3 of the observations.](#BA2)\n",
    "        - [Out-of-Bag Error Estimation](#BA3)\n",
    "        - [Variable Importance Measures](#BA4)\n",
    "    - [Random Forests](#RF)\n",
    "        - [Problem with bagging (high correlation)](#RF1)\n",
    "        - [The random forest process](#RF2)\n",
    "    - [Boosting](#Boosting)\n",
    "        - [Central idea: fit tree to residuals](#Boosting1)\n",
    "        - [Tunning parameters of boosting: $B$, $\\lambda$ and $d$](#Boosting2)\n",
    "        - [Boosting Algorithm for Regression Trees](#Boosting3)\n",
    "        - [XGBoost](https://xgboost.readthedocs.io/en/stable/tutorials/model.html)\n",
    "    - [Bayesian additive regression trees](#BART)\n",
    "        - [Bayesian additive regression trees algorithm](#BART1)\n",
    "- Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae2aad-4949-4326-9668-12172b07216c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name = \"BA\"></a>\n",
    "## Bootstrap aggregation/Bagging\n",
    "\n",
    "<blockquote>\n",
    "    Bootstrap aggregation, or <b>bagging</b>, is a general-purpose procedure for <b>reducing the variance</b> of a statistical learning method. -- Introduction to Statistical Learning, p.340. \n",
    "</blockquote>\n",
    "\n",
    "<div class = \"alert alert-block alert-info\">\n",
    "    <b>Central idea: Averaging a set of imperfectly correlated observations reduces variance.</b><br>\n",
    "    Consider a set of $n$ <em>independent</em> observations $Z_1, \\dots, Z_n$, each with variance $\\sigma^2$. Then the variance of the mean $\\bar Z$ of the observations is given by $\\sigma^2/n$.\n",
    "</div>\n",
    "\n",
    "<a name = \"BA1\"></a>\n",
    "### Bagging\n",
    "Given a training data set, create $B$ bootstrapped training data sets by taking repeated samples (with replacement) from the original data set. For each of the bootstrapped data set ($b = 1, \\dots, B$), train the method to get prediction rule $\\hat f^{*b}(x)$. Finally, average all the predictions to get:\n",
    "$$\n",
    "\\hat f_{\\text{bag}}(x) = \\frac{1}{B}\\sum_{b=1}^B \\hat{f}^{*b}(x).\n",
    "$$\n",
    "\n",
    "In the context of decision trees, each decision tree trained on a boostrapped data set is grown deep and *not pruned*, thereby having low bias and high variance.\n",
    "\n",
    "<a name = \"BA2\"></a>\n",
    "### On average, each bagged tree makes use of 2/3 of the observations.\n",
    "\n",
    "Suppose that the size of training data set is $n$ (which is also the size of the bootstrapped data set), the chance of an observation not being selected by a bootstrapped data set is:\n",
    "$$\n",
    "(1-\\frac{1}{n})^n\n",
    "$$\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./convergence.png\" style=\"width:50%;\" > <br>\n",
    "    Note how quickly it converges, with $n = 5$ very close to $1/3$. \n",
    "</div>\n",
    "<br>\n",
    "\n",
    "**Claim**: $\\lim_{n\\rightarrow \\infty}(1-\\frac{1}{n})^n = 1/e$ <br>\n",
    "**Proof**:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lim_{n\\rightarrow \\infty}(1-\\frac{1}{n})^n = & \\lim_{n\\rightarrow \\infty}(1-\\frac{1}{n})^n \\\\\n",
    "= & \\lim_{n\\rightarrow \\infty} \\frac{1}{(\\frac{n}{n-1})^n} \\\\\n",
    "= & \\lim_{n\\rightarrow \\infty} \\frac{1}{(1 + \\frac{1}{n-1})(1 + \\frac{1}{n-1})^{(n-1)}} \\\\\n",
    "= & \\frac{1}{\\underbrace{\\lim_{n\\rightarrow \\infty} (1 + \\frac{1}{n-1})}_{1}\\underbrace{\\lim_{n\\rightarrow \\infty} (1 + \\frac{1}{n-1})^{(n-1)}}_{e}} \\\\\n",
    "= & \\frac{1}{e}\n",
    "\\end{align}\n",
    "$$\n",
    "<details>\n",
    "    <summary><font size=\"3\"><b>Additional proof ($\\lim_{n\\rightarrow \\infty}(1+\\frac{1}{n})^n = e$) <br></b></font></summary>\n",
    "Consider $x$ in the interval $[1, 1 + \\frac{1}{n}]$,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{1}{1 + \\frac{1}{n}} \\leq \\frac{1}{x} \\leq 1 \\\\\n",
    "\\int_{1}^{1 + \\frac{1}{n}} \\frac{1}{1 + \\frac{1}{n}} dx \\leq  \\int_{1}^{1 + \\frac{1}{n}}\\frac{1}{x} dx \\leq \\int_{1}^{1 + \\frac{1}{n}} 1 dx \\\\\n",
    "\\frac{1}{n+1} \\leq \\ln \\left(1 + \\frac{1}{n}\\right) \\leq \\frac{1}{n} \\\\\n",
    "e^{\\frac{1}{n+1}} \\leq 1 + \\frac{1}{n} \\leq e^{\\frac{1}{n}}\n",
    "\\end{align}\n",
    "$$\n",
    "Then, focus on the left inequality:\n",
    "$$\n",
    "\\begin{align}\n",
    "e \\leq & {\\left(1 + \\frac{1}{n}\\right)}^{n+1} \\\\\n",
    "\\frac{e}{1 + \\frac{1}{n}} \\leq & {\\left(1 + \\frac{1}{n}\\right)}^{n}\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, $\\lim_{n\\rightarrow \\infty}{\\left(1 + \\frac{1}{n}\\right)}^{n} \\geq e$. <br>\n",
    "Next, focus on the right inequality:\n",
    "$$\n",
    "{\\left(1 + \\frac{1}{n}\\right)}^{n} \\leq e\n",
    "$$\n",
    "Thus, $\\lim_{n\\rightarrow \\infty}{\\left(1 + \\frac{1}{n}\\right)}^{n} \\leq e$. <br>\n",
    "Therefore, $\\lim_{n\\rightarrow \\infty}{\\left(1 + \\frac{1}{n}\\right)}^{n} = e$. <br>    \n",
    "Detailed proof: <a href = \"http://aleph0.clarku.edu/~djoyce/ma122/elimit.pdf\">here</a>\n",
    "</details>\n",
    "\n",
    "<a name = \"BA3\"></a>\n",
    "### Out-of-Bag (OOB) Error Estimation\n",
    "<blockquote>\n",
    "    One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree can are referred to as the <b>out-of-bag</b> (OOB) observations. -- Introduction to Statistical Learning, p.342. <br> <br>\n",
    "    The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous. -- Introduction to Statistical Learning, p.343.\n",
    "</blockquote>\n",
    "This provides a way to estimate the test error without the need to perform cross-validation or the validation set approach.\n",
    "\n",
    "#### The Process:\n",
    "1. For each observation $x_i$, there will be around $B/3$ predictions (using trees in which $x_i$ is OOB).\n",
    "2. Obtain a single prediction (call it the OOB prediction) through averaging (regression) or majority vote (classification).\n",
    "3. Calculate the overall OOB MSE (regression) or classification error (classification).\n",
    "\n",
    "<div class =\"alert alert-block alert-warning\"> With $B$ sufficiently large, it can be shown that the OOB error is virtually equivalent to the leave-one-out cross-validation error.</div>\n",
    "\n",
    "<a name = \"BA4\"></a>\n",
    "### Variable Importance Measures\n",
    "- decrease in the Residual Sum of Squares (RSS) (for bagging regression trees)\n",
    "    - add up the **total** amount of decrease in the RSS due to splits over a given predictor\n",
    "    - **average** over all $B$ trees\n",
    "- decrease in the Gini index (for bagging classification trees)\n",
    "    - add up the **total** amount of decrease in the Gini index/cross-entropy due to splits over a given predictor\n",
    "    - **average** over all $B$ trees\n",
    "    \n",
    "<a name = \"RF\"></a>    \n",
    "## Random Forests\n",
    "\n",
    "<a name = \"RF1\"></a> \n",
    "### Problem with bagging\n",
    "**Trees can look fairly similar** in the presence of strong dominant predictors, resulting in highly correlated predictions. Averaging highly correlated quantities does not lead to a large reduction in variance.\n",
    "\n",
    "Random forests improves upon bagged trees by **decorrelating** the trees through incorporating additional randomness in the splitting process.\n",
    "\n",
    "<a name = \"RF2\"></a> \n",
    "### The random forest process\n",
    "1. Get $B$ bootstrapped training data sets.\n",
    "2. Train a decision tree on each bootstrapped data set. Each time a split in a tree is considered, a random sample of $m \\approx \\sqrt{p}$ predictors is considered rather than the full predictors set. A fresh sample of $m$ predictors is taken at **each split** within a decision tree.\n",
    "\n",
    "<div class =\"alert alert-block alert-info\"> When $m = p$, a random forest amounts simply to bagging. Using a small value of $m$ will typically be helpful in the presence of a large number of highly correlated predictors (e.g., genomics and biology).</div>\n",
    "\n",
    "<a name = \"Boosting\"></a>\n",
    "## Boosting\n",
    "\n",
    "<blockquote>\n",
    "    Like bagging, <b>boosting</b>, is a general approach that can be applied to many statistical learning methods for regression and classification. -- Introduction to Statistical Learning, p.345. \n",
    "</blockquote>\n",
    "\n",
    "In boosting, trees are grown **sequentially**, with information from previously grown trees feeding into subsequent trees. Boosting does not use bootstrapping but **modified versions** of the original data set.\n",
    "\n",
    "<a name = \"Boosting1\"></a>\n",
    "### Central idea\n",
    "Fit a decision tree to the **residuals** from the previous model.\n",
    "\n",
    "<a name = \"Boosting2\"></a>\n",
    "### Tunning parameters of boosting:\n",
    "1. **The number of trees $B$.** Unlike bagging and random forests, boosting can *overfit* if $B$ is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select $B$.\n",
    "2. **The shrinkage parameter $\\lambda$ (a small positive number).** This controls *the rate at which boosting learns*. Typical values are 0.01 or 0.001, and the right choice can depend on the problem. To achieve good performance, a small $\\lambda$ may require a large $B$.\n",
    "3. **The number $d$ of splits in each tree,** which governs the *complexity* of the boosted ensemble. $d$ splits lead to $d+1$ terminal nodes. Often $d=1$ works well, in which ease each tree is a *stump*, consisting of a single split.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"../images/slumps vs depth-2 decision tree.pdf\" style=\"width:80%;\" > <br>\n",
    "    Illustration: a single decision tree with 2 splits vs average of two slumps\n",
    "</div>\n",
    "\n",
    "<a name = \"Boosting3\"></a>\n",
    "### Boosting Algorithm for Regression Trees\n",
    "\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Boosting for Regression Trees</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        Set $\\hat f(x) = 0$ and $r_i = y_i$ for all $i$ in the training set. <br>\n",
    "        For $b = 1, \\dots, B$, repeat:\n",
    "        <blockquote>\n",
    "            <ol>\n",
    "                <li> Fit a tree $\\hat f^b$ with $d$ splits ($d+1$ terminal nodes) to the training data $(X, r)$.\n",
    "                <li> Update $\\hat f$ by adding in a shrunken version of the new tree:\n",
    "                    $$\n",
    "                    \\hat f(x) \\leftarrow \\hat f(x) + \\lambda \\hat f^b(x)\n",
    "                    $$\n",
    "                <li> Update the residuals,\n",
    "                    $$\n",
    "                    r_i \\leftarrow r_i - \\lambda \\hat f^b(x_i)\n",
    "                    $$\n",
    "            </ol>\n",
    "        </blockquote>\n",
    "        Output the boosted model,\n",
    "        $$\n",
    "        \\hat f(x) = \\sum_{b=1}^B \\lambda \\hat f^b(x).\n",
    "        $$\n",
    "    </div>\n",
    "</section>\n",
    "\n",
    "### XGBoost\n",
    "A nice tutorial: <a href = \"https://xgboost.readthedocs.io/en/stable/tutorials/model.html\">here</a>.\n",
    "\n",
    "<a name = \"BART\"></a>\n",
    "## Bayesian additive regression trees (BART)\n",
    "<blockquote>\n",
    "    BART is related to both approaches (bagging/RF and boosting): each tree is constructed in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting. -- Introduction to Statistical Learning, p.348. <br><br>\n",
    "    It turns out that the BART method can be viewed as a <em>Bayesian</em> approach to fitting an ensemble of tree: each time we randomly perturb a tree in order to fit the residuals, we are in fact drawing a new tree from the posterior distribution. -- Introduction to Statistical Learning, p.350.\n",
    "</blockquote>\n",
    "\n",
    "#### New notation\n",
    "- $K$: the number of regression trees\n",
    "- $B$: the number of iterations for which the BART algrithm will be run\n",
    "- $L$: the number of burn-in iterations\n",
    "- $\\hat f_k^b(x)$: the prediction at $x$ for the $k$th regression tree used in the $b$th iteration.\n",
    "- $\\hat f^b(x) = \\sum_{k=1}^K \\hat f_k^b(x)$: the prediction at $x$ in the $b$th iteration.\n",
    "\n",
    "#### Partial residual\n",
    "To update the $k$th tree in the $b$th iteration, construct a **partial residual**:\n",
    "$$\n",
    "r_i = y_i -\\sum_{k^\\prime < k} \\hat f_{k^\\prime}^b(x_i) - \\sum_{k^\\prime > k} \\hat f_{k^\\prime}^{b-1}(x_i)\n",
    "$$\n",
    "for $i = 1, \\dots, n$.\n",
    "#### Random perturbation\n",
    "Rather than fitting a fresh tree based on the partial residual, BART **randomly chooses a perturbation** to $f_k^{b-1}$ from a set of possible perturbations, favouring ones that *improve the fit to the partial residual*:\n",
    "1. Change the structure of the tree by adding or prunning branches.\n",
    "2. Change the prediction in each terminal node of the tree.\n",
    "\n",
    "<div class = \"alert alert-block alert-info\">Roughly speaking, this <em>guards against overfitting</em> as it limits how \"hard\" we fit the data in each iteration.</div>\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"../images/Random perturbations.jpeg\" style=\"width:70%;\" > <br>\n",
    "    Source: Introduction to Statistical Learning, p.349.\n",
    "</div>\n",
    "\n",
    "#### Burn-in period\n",
    "Models obtained in the earlier iterations–known as the burn-in period–tend not to provide very good results. Thus, we usually discard the first few prediction models $b = 1, \\dots, L$.\n",
    "\n",
    "#### Model prediction and uncertainty\n",
    "$$\n",
    "\\hat f(x) = \\frac{1}{B-L} \\sum_{b = B-L}^B \\hat f^b(x)\n",
    "$$\n",
    "The percentiles of $\\hat f^{L+1}(x), \\dots, \\hat f^B(x)$ provide a measure of uncertainty in the final prediction.\n",
    "\n",
    "<a name = \"BART1\"></a>\n",
    "### Bayesian Additive Regression Trees\n",
    "\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Bayesian Additive Regression Trees </div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        Let $\\hat f^1_1(x) = \\hat f^1_2(x) = \\cdots = \\hat f^1_K(x) = \\frac{1}{nK} \\sum_{i=1}^n y_i$ and thus \n",
    "        $$\n",
    "        \\hat f^1(x) = \\sum_{k=1}^K \\hat f^1_k(x) = \\frac{1}{n}\\sum_{i=1}^n y_i \\; \\textbf{(sample mean)}\n",
    "        $$\n",
    "        For $b = 2, \\dots, B$:\n",
    "        <blockquote>\n",
    "            <ol>\n",
    "                <li> For $k = 1, \\dots, K$:\n",
    "                    <ol>\n",
    "                        <li> Compute the current partial residual:\n",
    "                            $$\n",
    "                            r_i = y_i -\\sum_{k^\\prime < k} \\hat f_{k^\\prime}^b(x_i) - \\sum_{k^\\prime > k} \\hat f_{k^\\prime}^{b-1}(x_i)\n",
    "                            $$\n",
    "                            for $i = 1, \\dots, n$.\n",
    "                        <li> Fit a new tree, $\\hat f^b_k(x)$ to $\\{r_i\\}$ by randomly perturbaing the $k$th tree from the previous iteration $f_k^{b-1}$, favuring the ones that improve the fit.\n",
    "                    </ol>\n",
    "                <li> Compute $\\hat f^b(x) = \\sum_{k=1}^K \\hat f_k^b(x)$.\n",
    "            </ol>\n",
    "        </blockquote>\n",
    "        Output the BART model,\n",
    "        $$\n",
    "        \\hat f(x) = \\frac{1}{B-L} \\sum_{b=B-L}^B \\hat f^b(x).\n",
    "        $$\n",
    "    </div>\n",
    "</section>\n",
    "<blockquote>\n",
    "    BART has been shown to have very impressive out-of-box performance–that is, it performs very well with minimal tunning. -- Introduction to Statistical Learning, p.351.\n",
    "</blockquote>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
