{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c8e7bc-a46d-47ba-917f-e76ce2239355",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "Author: Bingchen Wang\n",
    "\n",
    "Last Updated: 24 Sep, 2022\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e6f1e0-ec43-4fd8-a89d-79521ec8d067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel='stylesheet' type='text/css' media='screen' href='../styles/custom.css'>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel='stylesheet' type='text/css' media='screen' href='../styles/custom.css'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384be3b-c0fd-4bcc-8a58-32f768d6ed33",
   "metadata": {},
   "source": [
    "<section class = \"section--outline\">\n",
    "    <div class = \"outline--header\">Outline </div>\n",
    "    <div class = \"outline--content\">\n",
    "        <b>Concepts:</b>\n",
    "        <ul>\n",
    "            <li> <a href = \"#KMC\">K-means Clustering</a>\n",
    "            <li> <a href = \"#MoG\">Mixture of Gaussians</a>\n",
    "                <ul>\n",
    "                    <li> <a href = \"#AD\">Anomaly Detection</a>\n",
    "                    <li> <a href = \"#MoGM\">Mixture of Gaussians Model</a>\n",
    "                    <li> <a href = \"#EMA\">Expectation Maximization Algorithm</ul>\n",
    "            <li> <a href = \"#FA\">Factor Analysis</a>\n",
    "            <li> Principal Component Analysis\n",
    "            <li> Independent Component Analysis\n",
    "        </ul>\n",
    "        <b>Implementation:</b>\n",
    "        <ul>\n",
    "            <li> K-means Clustering\n",
    "                <ul>\n",
    "                    <li> <a href = \"./K-means Clustering/Numpy Implementation.ipynb\">Numpy Implementation</a>\n",
    "                    <li> <a href = \"./K-means Clustering/Sklearn Implementation.ipynb\">Sklearn Implementation</a>\n",
    "                </ul>\n",
    "            <li> Mixture of Gaussians\n",
    "                <ul>\n",
    "                    <li> <a href = \"./Mixture of Gaussians/Numpy Implementation.ipynb\">Numpy Implementation</a>\n",
    "                    <li> <a href = \"./Mixture of Gaussians/Sklearn Implementation.ipynb\">Sklearn Implementation</a>\n",
    "                </ul>\n",
    "        </ul>    \n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd5463-ca85-4044-9fb3-abdfdc233824",
   "metadata": {},
   "source": [
    "<a name = \"KMC\"></a>\n",
    "## K-means Clustering\n",
    "### Cost function\n",
    "$$\n",
    "J(\\mathbf{c},\\mathbf{\\mu}) = \\sum^m_{i=1}\\left\\Vert x^{(i)} - \\mu_{c^{(i)}}\\right\\Vert^2\n",
    "$$\n",
    "\n",
    "### Algorithm\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> K-means Clustering Algorithm</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        Data $ \\{x^{(1)}, x^{(2)}, \\dots, x^{(m)}\\}$.\n",
    "        <blockquote>\n",
    "            Initialize cluster centroids $\\mu_1, \\mu_2, \\dots, \\mu_k \\in \\mathbb{R}^n$ <br>\n",
    "            <div class = \"alert alert-block alert-success\"><b>Note:</b> Usually randomly pick $k$ examples from the dataset to be the initial cluster centroids. </div>\n",
    "            Repeat until convergence:\n",
    "            <blockquote>\n",
    "                (a) <b>(colour the points)</b> Set $c^{(i)} := \\arg\\min_j \\left\\Vert x^{(i)} - \\mu_j \\right\\Vert_2$ <br>\n",
    "                (b) <b>(move the cluster centroids)</b> For $j = 1, 2, \\dots, k$, \n",
    "                $$\n",
    "                \\mu_j := \\frac{\\sum^m_{i=1} \\mathbb{1}\\{c^{(i)} = j \\} x^{(i)}}{\\sum^m_{i=1} \\mathbb{1}\\{c^{(i)} = j \\}}\n",
    "                $$\n",
    "            </blockquote>\n",
    "        </blockquote>\n",
    "    </div>\n",
    "</section>\n",
    "\n",
    "### Local minima\n",
    "**Q: Worry about local minima?** <br>\n",
    "A: Run the algorithm several times, say 10, 100, 1000 times, with different random initializations of cluster centroids. Pick the one that results in the lowest value for the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c81e9-03d8-4bd7-8a08-cd431e86b56b",
   "metadata": {},
   "source": [
    "<a name = \"MoG\"></a>\n",
    "## Mixture of Gaussians\n",
    "\n",
    "<a name = \"AD\"></a>\n",
    "### Anomaly Detection\n",
    "<img align=\"right\" src=\"./images/Anomaly Detection.jpeg\" style=\"width:300px;\" >\n",
    "\n",
    "#### Supervised Learning vs Unsupervised Learning\n",
    "Supervised Learning when:\n",
    "- Lots of labelled data of both classes (normal and anomalous)\n",
    "- Future anomalies similar to the ones seen in the training set\n",
    "\n",
    "Unsupervised Learning when:\n",
    "- Unlabelled data or labelled data with few to none anomalies\n",
    "- Various types of anomalies, with unseen future anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a303409-6fd6-4632-acd3-270568f41b5f",
   "metadata": {},
   "source": [
    "<a name = \"MoGM\"></a>\n",
    "### Mixture of Gaussians Model\n",
    "Suppose there is a latent (hidden/unobserved) random variable $z$, and $x^{(i)}, z^{(i)}$ are distributed\n",
    "$$\n",
    "P(x^{(i)}, z^{(i)}) = P(x^{(i)}\\vert z^{(i)})P(z^{(i)})\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align}\n",
    "z^{(i)} \\sim & \\; \\text{Multinomial}(\\phi), z^{(i)} \\in \\{1, \\dots, k\\} \\\\\n",
    "x^{(i)}\\vert z^{(i)} \\sim & \\; \\mathcal{N}(\\mu_j, \\Sigma_j)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a15bf6-dec0-4d30-88b5-f3bfc0502741",
   "metadata": {},
   "source": [
    "<a name = \"EMA\"></a>\n",
    "### Expectation Maximization Algorithm\n",
    "<div class = \"alert alert-block alert-info\"><b>Intuition:</b> Like K-means but with soft assignments.</div>\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Expectation maximization.jpeg\" style=\"width:50%;\" >\n",
    "</div>\n",
    "        \n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Expectation Maximization Algorithm (for mixture of gaussians)</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        <b>E-step</b> (Guess the value of $z^{(i)}$'s)\n",
    "        <blockquote>\n",
    "            Set $$ \n",
    "            \\begin{align}\n",
    "            w_j^{(i)} =& Q_i(z^{(i)}=j) = P(z^{(i)} = j | x^{(i)}; \\mathbf{\\phi}, \\mathbf{\\mu}, \\mathbf{\\Sigma}) \\\\\n",
    "            =& \\frac{P(x^{(i)}\\vert z^{(i)} = j)P(z^{(i)} = j)}{\\sum^k_{l=1}P(x^{(i)}\\vert z^{(i)} = l)P(z^{(i)} = l)}\n",
    "            \\end{align}\n",
    "            $$\n",
    "            where\n",
    "            $$\n",
    "            \\begin{align}\n",
    "            P(z^{(i)} = j) = & \\phi_j  \\\\\n",
    "            P(x^{(i)}\\vert z^{(i)} = j) = & \\frac{1}{{(2\\pi)}^{n/2}\\vert\\Sigma_j\\vert^{1/2}} \\exp{\\left(-\\frac{1}{2}(x^{(i)}-\\mu_j)^T \\Sigma_j^{-1}(x^{(i)}-\\mu_j)\\right)}\n",
    "            \\end{align}\n",
    "            $$\n",
    "        </blockquote>\n",
    "        <b>M-step</b> (Update the gaussians)\n",
    "        <blockquote>\n",
    "        <div class= \"alert alert-block alert-success\">\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        \\max_{\\phi, \\mu, \\Sigma}& \\Sigma_i \\Sigma_{z^{(i)}} Q_i(z^{(i)}) \\log\\left[\\frac{P(x^{(i)}, z^{(i)}; \\phi, \\mu, \\Sigma)}{Q_i(z^{(i)})}\\right] \\\\\n",
    "        &= \\Sigma_i \\Sigma_{j} w^{(i)}_j \\log\\left[\\frac{\\frac{1}{{(2\\pi)}^{n/2}\\vert\\Sigma_j\\vert^{1/2}} \\exp{\\left(-\\frac{1}{2}(x^{(i)}-\\mu_j)^T \\Sigma_j^{-1}(x^{(i)}-\\mu_j)\\right)}\\phi_j}{w^{(i)}_j}\\right]\n",
    "        \\end{align}\n",
    "        $$\n",
    "        </div> <br>\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        \\phi_j :=& \\frac{1}{m} \\sum^m_{i=1} w_j^{(i)} \\\\\n",
    "        \\mu_j :=& \\frac{\\sum^m_{i=1}w_j^{(i)}x^{(i)}}{\\sum^m_{i=1}w_j^{(i)}} \\\\\n",
    "        \\Sigma_j :=& \\frac{\\sum^m_{i=1}w_j^{(i)}(x^{(i)} -\\mu_j)(x^{(i)} -\\mu_j)^T}{\\sum^m_{i=1}w_j^{(i)}}&\n",
    "        \\end{align}\n",
    "        $$\n",
    "        </blockquote>\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3a417-a9c8-4876-8a6b-16af5fa39851",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "$$ P(x) = \\Sigma_k p(x,z = k) $$\n",
    "\n",
    "Criteria:\n",
    "\n",
    "$$\n",
    "\\left\\{ \\begin{array}{c c}\n",
    "P(x) \\geq \\epsilon & \\text{(ok)} \\\\\n",
    "P(x) < \\epsilon & \\text{(anomaly)}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Choose $\\epsilon$ using CV with a labelled CV dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0363045-4500-4f78-903c-348f3ed6c8a4",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><font size=\"3\"><b>Why EM works</b></font></summary>\n",
    "    <br>\n",
    "    <section class = \"section--concept\">\n",
    "        <div class = \"concept--header\"> Jensen's Inequality</div>\n",
    "        <div class = \"concept--content\">\n",
    "        Let $f$ be a convex function. (e.g. $f^{\\prime\\prime}(x)\\geq 0$). Let $X$ be a random variable. Then,\n",
    "        $$\n",
    "        f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X))]\n",
    "        $$\n",
    "        Further, if $f^{\\prime\\prime}(x) > 0$ ($f$ is strictly convex), then \n",
    "            $$E[f(X)] = f(E[X]) \\iff X \\text{ is constant.}$$ <br>\n",
    "        <div style = \"text-align: center;\">\n",
    "        <img src=\"./images/Jensen's inequality.jpeg\" style=\"width:300px;\" >\n",
    "        </div>\n",
    "        </div>\n",
    "    </section>\n",
    "    <br>\n",
    "    <b>Idea:</b> construct lower bounds and optimize lower bounds. <br>\n",
    "    <b>Construct lower bounds that are tight at the current $\\theta$:</b>\n",
    "    MLE: $$\n",
    "    \\begin{align}\n",
    "    \\max_\\theta  \\log (\\Pi_i P(x^{(i)};\\theta)) = &\n",
    "    \\Sigma_i \\log P(x^{(i)};\\theta) \\\\\n",
    "    = & \\Sigma_i \\log \\Sigma_{z^{(i)}}  P(x^{(i)}, z^{(i)};\\theta) \\\\\n",
    "    = & \\Sigma_i \\log \\Sigma_{z^{(i)}} Q_i(z^{(i)})  \\left[ \\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})} \\right] \\; \\text{where $Q_i(z^{(i)})$ is a probability distribution (i.e., $\\Sigma_{z^{(i)}}Q_i(z^{(i)}) = 1$)} \\\\\n",
    "    = & \\Sigma_i \\log \\mathbb{E}_{z^{(i)} \\sim Q_i} \\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right] \\\\\n",
    "    \\geq & \\Sigma_i \\mathbb{E}_{z^{(i)} \\sim Q_i} \\log \\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right] \\; \\text{(Jensen's Inequality)} \\\\\n",
    "    = & \\Sigma_i \\Sigma_{z^{(i)}} Q_i(z^{(i)}) \\log \\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right]\n",
    "    \\end{align}\n",
    "    $$\n",
    "    Want lower bounds to be tight at the current $\\theta$:\n",
    "    $$\n",
    "    \\log \\mathbb{E} \\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right] = \\mathbb{E} \\log \\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right]\n",
    "    $$\n",
    "    Since $\\log(\\cdot)$ is strictly concave, then it follows that:\n",
    "    $$\n",
    "    \\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})} \\; \\text{is constant} \\implies Q_i(z^{(i)}) \\propto P(x^{(i)}, z^{(i)};\\theta)\n",
    "    $$\n",
    "    Since $Q_i$ is a pdf, it follows that $\\Sigma_{z^{(i)}}Q_i(z^{(i)}) = 1$. Then,\n",
    "    $$\n",
    "    Q_i(z^{(i)}) = \\frac{P(x^{(i)}, z^{(i)};\\theta)}{\\Sigma_{z^{(i)}}P(x^{(i)}, z^{(i)};\\theta)} = \\frac{P(x^{(i)}, z^{(i)};\\theta)}{P(x^{(i)};\\theta)} = P(z^{(i)} \\vert x^{(i)};\\theta)\n",
    "    $$\n",
    "    <b>Expectation Maximization:</b><br>\n",
    "    E-step: Set $Q_i(z^{(i)}) = P(z^{(i)}|x^{(i)};\\theta)$ <br>\n",
    "    M-step: $\\theta = \\arg\\max_\\theta \\Sigma_i\\Sigma_{z^{(i)}}Q_i(z^{(i)})\\log\\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right]$ <br>\n",
    "    This shows that the EM algorithm is a maximum likelihood estimation algorithm, with optimization solved by constructing lower bounds and optimizing lower bounds.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a9c98-0fe7-4e65-a2ba-1aab688cd13f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name = \"FA\"></a>\n",
    "## Factor Analysis\n",
    "\n",
    "<blockquote>\n",
    "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.  -- Wikipedia\n",
    "</blockquote>\n",
    "\n",
    "<blockquote>\n",
    "    <b>Factor Pricing Models in Asset Pricing Theory (APT)</b> (Financial Economics) <br>\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    x^i =& \\alpha_i + \\Sigma_{j=1}^M \\beta_{ij}f_j + \\epsilon^i \\\\\n",
    "    =& \\alpha_i + \\mathbf{\\beta}_i^\\prime \\mathbf{f} + \\epsilon^i \\\\\n",
    "    =& \\mathbb{E}[x^i] + \\Sigma_{j=1}^M \\beta_{ij}\\tilde{f_j} + \\epsilon^i \\; \\text{(by convention, $\\mathbb{E}[\\tilde{f}] = 0$)}\n",
    "    \\end{align}\n",
    "    $$\n",
    "    where $x^i$ is the return to asset $i$ and $f_j$'s are common factors, such as the market portfolio (\"the market\"), industry portfolios, or size and book to market portfolios, etc.\n",
    "    -- For more, see <a href =\"https://press.princeton.edu/books/hardcover/9780691121376/asset-pricing\">Cochrane (2005)<a>.\n",
    "</blockquote>    \n",
    "    \n",
    "### Factor Analysis Model\n",
    "    \n",
    "$\\mathbf{X}$ are **observed** variables of shape $(m,n)$ and $\\mathbf{Z}$ are **unobserved/latent** factors of shape $(m,d)$, where $d < n$. Denote a single vector of factors as $z \\in \\mathbb{R}^d$ and a single vector of variables as $x \\in \\mathbb{R}^n$.\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    z \\sim & \\mathcal{N}(0, I) \\\\\n",
    "    x = & \\mathbf{\\mu} +  \\Lambda z + \\epsilon, \\; \\epsilon \\sim \\mathcal{N}(0, \\Psi)\n",
    "    \\end{align}\n",
    "    $$\n",
    "    where $\\mu \\in \\mathbb{R}^n, \\;\\Lambda \\in \\mathbb{R}^{n\\times d},\\; \\Psi \\in \\mathbb{R}^{n \\times n}$ diagonal.\n",
    "    $$\n",
    "    \\left[\\begin{array}{c}\n",
    "    z \\\\\n",
    "    x\n",
    "    \\end{array}\\right] \\sim\n",
    "    \\mathcal{N}\\left(\\left[\\begin{array}{c}\n",
    "    0 \\\\\n",
    "    \\mu\n",
    "    \\end{array}\\right],\n",
    "    \\left[\\begin{array}{cc}\n",
    "    I & \\Lambda^T \\\\\n",
    "    \\Lambda & \\Psi + \\Lambda\\Lambda^T\n",
    "    \\end{array}\\right]\n",
    "    \\right)\n",
    "    $$\n",
    "    \n",
    "Conventionally, it is easier to work with the demeaned variables $\\tilde x = x - \\mathbb{E}[x]$ such that:<br>\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    z \\sim & \\mathcal{N}(0, I) \\\\\n",
    "    \\tilde x = &  \\Lambda z + \\epsilon, \\; \\epsilon \\sim \\mathcal{N}(0, \\Psi)\n",
    "    \\end{align}\n",
    "    $$<br>\n",
    "    $$\n",
    "    \\left[\\begin{array}{c}\n",
    "    z \\\\\n",
    "    \\tilde x\n",
    "    \\end{array}\\right] \\sim\n",
    "    \\mathcal{N}\\left(\\left[\\begin{array}{c}\n",
    "    0 \\\\\n",
    "    0\n",
    "    \\end{array}\\right],\n",
    "    \\left[\\begin{array}{cc}\n",
    "    I & \\Lambda^T \\\\\n",
    "    \\Lambda & \\Psi + \\Lambda\\Lambda^T\n",
    "    \\end{array}\\right]\n",
    "    \\right)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fe4f53-e821-4855-b736-4efbdfaa3481",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><font size=\"3\"><b>Conditional Normal Distribution</b></font></summary>\n",
    "    <br>\n",
    "    <section class = \"section--concept\">\n",
    "        <div class = \"concept--header\"> Conditional Normal Distribution</div>\n",
    "        <div class = \"concept--content\">\n",
    "            Given a multivariate normal distribution\n",
    "            $$\n",
    "                \\left[\\begin{array}{c}\n",
    "                X_1 \\\\\n",
    "                X_2\n",
    "                \\end{array}\\right] \\sim\n",
    "                \\mathcal{N}\\left(\\left[\\begin{array}{c}\n",
    "                \\mu_1\\\\\n",
    "                \\mu_2\n",
    "                \\end{array}\\right],\n",
    "                \\left[\\begin{array}{cc}\n",
    "                \\Sigma_{11} & \\Sigma_{12} \\\\\n",
    "                \\Sigma_{21} & \\Sigma_{22}\n",
    "                \\end{array}\\right]\n",
    "                \\right),\n",
    "            $$\n",
    "            the conditional distribution of $X_1 \\vert X_2$ is normal with:\n",
    "            $$\n",
    "            \\begin{align}\n",
    "            \\mu_{1\\vert2} = \\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(X_2 - \\mu_2) \\\\\n",
    "            \\Sigma_{1\\vert2} = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n",
    "            \\end{align}\n",
    "            $$\n",
    "            (Derivation : <a href = \"https://stats.stackexchange.com/questions/30588/deriving-the-conditional-distributions-of-a-multivariate-normal-distribution\">here</a>)\n",
    "        </div>\n",
    "    </section>\n",
    "    <br>\n",
    "    It follows that: $z \\vert x$ is normal with:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\mu_{z\\vert x} = \\Lambda^T {(\\Psi + \\Lambda\\Lambda^T)}^{-1}(x - \\mu) \\\\\n",
    "    \\Sigma_{z\\vert x} = I - \\Lambda^T{(\\Psi + \\Lambda\\Lambda^T)}^{-1}\\Lambda\n",
    "    \\end{align}\n",
    "    $$\n",
    "    If we see $z \\vert x$ through the lens of <b>linear projection</b>, then\n",
    "    $$\n",
    "    \\mathbb{E}[z\\vert x] = \\beta (x - \\mu)\n",
    "    $$\n",
    "    where $\\beta \\equiv \\Lambda^T {(\\Psi + \\Lambda\\Lambda^T)}^{-1}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1c843-bf0f-4a2d-a098-03e3b13fe005",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Expectation Maximization for Factor Analysis\n",
    "Recall **EM**: (construct and optimise lower bounds)\n",
    "<blockquote>\n",
    "    <b>E-step</b>: compute $w_j^{(i)}= Q_i(z^{(i)}=j) = P(z^{(i)} = j\\vert x^{(i)})$. <br>\n",
    "    <b>M-step</b>: solve for $\\theta$ that maximises the log-likehood\n",
    "    $$\n",
    "    \\arg\\max_\\theta \\log(\\Pi_i^m P(x^{(i)}; \\theta))\n",
    "    = \\arg\\max_\\theta \\sum_i^m \\mathbb{E}_{z^{(i)}\\sim Q_i}[\\log(P(x^{(i)}; \\theta))]\n",
    "    $$\n",
    "</blockquote>\n",
    "Here, since $z$ follows a normal distribution, we can make use of the sufficient statistics (i.e. mean and variance). <br>\n",
    "<br>\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Expectation Maximization Algorithm (for factor analysis)</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        <b>E-step</b> (Estimate the first and second moments of the conditional distribution, i.e. $E[z^{(i)}\\vert  x^{(i)}]$ and $E[z^{(i)}z^{(i)\\prime} \\vert  x^{(i)}]$ using $\\Lambda$ and $\\Psi$)\n",
    "        <blockquote>\n",
    "        The variance is given by\n",
    "            $$\n",
    "            \\mathrm{Var}[z^{(i)} \\vert x^{(i)}] = \\Sigma_{z\\vert x^{(i)}} = I - \\Lambda^T{(\\Psi + \\Lambda\\Lambda^T)}^{-1}\\Lambda.\n",
    "            $$\n",
    "            <br>\n",
    "        <div class = \"alert alert-block alert-info\">\n",
    "            Recall that $\\mathrm{Var}[z] = \\mathbb{E}[zz^\\prime] - \\mathbb{E}[z]\\mathbb{E}[z^\\prime]$, which implies that:\n",
    "            $$\n",
    "            E[z^{(i)}z^{(i)\\prime} \\vert x^{(i)}] = \\mathrm{Var}[z^{(i)} \\vert x^{(i)}] + \\mathbb{E}[z^{(i)}\\vert  x^{(i)}]\\mathbb{E}[z^{(i)\\prime}\\vert  x^{(i)}]\n",
    "            $$\n",
    "        </div>\n",
    "        Thus, only need to estimate the conditional mean $\\mathbb{E}[z^{(i)}\\vert  x^{(i)}]$:\n",
    "            $$\n",
    "            \\hat \\mu_{z\\vert x^{(i)}} = \\Lambda^T {(\\Psi + \\Lambda\\Lambda^T)}^{-1}(x^{(i)} - \\hat \\mu).\n",
    "            $$\n",
    "        Estimate the mean $\\mu$ once before the EM as the sample mean:\n",
    "            $$\n",
    "            \\hat \\mu = \\frac{1}{m}\\sum_{i=1}^m x^{(i)}.\n",
    "            $$\n",
    "        </blockquote>\n",
    "        <b>M-step</b> (Update $\\Lambda$ and $\\Psi$)\n",
    "        <blockquote>\n",
    "        <div class= \"alert alert-block alert-success\">\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        \\max_{\\Lambda, \\Psi}& \\sum_i \\mathbb{E}_{z^{(i)}\\vert x^{(i)}} \\log\\left[P(x^{(i)}; \\Lambda, \\Psi)\\right] \\\\\n",
    "        &= \\sum_i \\mathbb{E}_{z^{(i)}\\vert x^{(i)}} \\log\\left[ (2\\pi)^{-n/2} {\\vert\\Psi\\vert}^{-1/2} \\exp\\left\\{-\\frac{1}{2}{(x^{(i)} - \\Lambda z^{(i)})}^T\\Psi^{-1}(x^{(i)} - \\Lambda z^{(i)}) \\right\\}\\right] \\\\\n",
    "        &= -\\frac{mn}{2}\\log 2\\pi - \\frac{n}{2}\\log \\vert \\Psi \\vert - \\frac{1}{2}\\sum_i x^{(i)T}\\Psi^{-1}x^{(i)} + \\sum_i x^{(i)T}\\Psi^{-1}\\Lambda \\mathbb{E}[z^{(i)}\\vert x^{(i)}] - \\frac{1}{2}\\sum_i \\mathrm{tr}\\left[\\Lambda^T\\Psi^{-1}\\Lambda\\mathbb{E}[z^{(i)}z^{(i)T}\\vert x^{(i)}]\\right]\n",
    "        \\end{align}\n",
    "        $$\n",
    "        Solve the first order conditions (FOCs) w.r.t. $\\Lambda$ and $\\Psi$.\n",
    "        </div> <br>\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        \\Lambda :=& \\left(\\sum^m_{i=1}x^{(i)}\\mathbb{E}[z^{(i)T}\\vert x^{(i)}]\\right){\\left(\\sum^m_{i=1}\\mathbb{E}[z^{(i)}z^{(i)T}\\vert x^{(i)}]\\right)}^{-1}  \\\\\n",
    "        \\Psi  :=& \\mathrm{diag}\\left(\\frac{1}{m}\\sum_i^m x^{(i)}x^{(i)T} - \\frac{1}{m}\\underbrace{\\left(\\sum_i^m x^{(i)}\\mathbb{E}[z^{(i)T}\\vert x^{(i)}]\\right){\\left(\\sum^m_{i=1}\\mathbb{E}[z^{(i)}z^{(i)T}\\vert x^{(i)}]\\right)}^{-1}}_{\\Lambda^{\\mathrm{new}}}\\left(\\sum_i^m \\mathbb{E}[z^{(i)}\\vert x^{(i)}]x^{(i)T}\\right)\\right)\n",
    "        \\end{align}\n",
    "        $$\n",
    "        where the $\\mathrm{diag}$ operator sets all of the off-diagonal elements of a matrix to $0$.\n",
    "        </blockquote>\n",
    "        (Original paper: <a href = \"https://mlg.eng.cam.ac.uk/zoubin/papers/tr-96-1.pdf\">here</a>)\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ec9f6-9c1d-439f-8161-f20f80fe66d2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
